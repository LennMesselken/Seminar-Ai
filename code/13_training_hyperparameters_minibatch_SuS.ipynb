{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a01baafc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a2fc82bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a NumPy array of bytes:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the pixels into a matrix where each line is an image:\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "\n",
    "# 60000 images, each 784 elements (28 * 28 pixels)\n",
    "X_train = load_images(\"../data/mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 784 elements, with the same structure as X_train\n",
    "X_test = load_images(\"../data/mnist/t10k-images-idx3-ubyte.gz\")\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    n_labels = Y.shape[0]\n",
    "    n_classes = 10\n",
    "    encoded_Y = np.zeros((n_labels, n_classes))\n",
    "    for i in range(n_labels):\n",
    "        label = Y[i]\n",
    "        encoded_Y[i][label] = 1\n",
    "    return encoded_Y\n",
    "\n",
    "\n",
    "# !!! EDIT PATHS TO WHERE YOUR MNIST DATA IS !!!\n",
    "\n",
    "# 60K labels, each a single digit from 0 to 9\n",
    "Y_train_unencoded = load_labels(\"../data/mnist/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# 60K labels, each consisting of 10 one-hot encoded elements\n",
    "Y_train = one_hot_encode(Y_train_unencoded)\n",
    "\n",
    "# 10000 labels, each a single digit from 0 to 9\n",
    "Y_test = load_labels(\"../data/mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e876c08",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dd3a89ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network implementation (almost the same as backpropagation.py,\n",
    "# except for a tiny refactoring in the back() function).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exponentials = np.exp(logits)\n",
    "    return exponentials / np.sum(exponentials, axis=1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def sigmoid_gradient(sigmoid):\n",
    "    return np.multiply(sigmoid, (1 - sigmoid))\n",
    "\n",
    "\n",
    "def loss(Y, y_hat):\n",
    "    return -np.sum(Y * np.log(y_hat)) / Y.shape[0]\n",
    "\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "def forward(X, w1, w2):\n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))\n",
    "    return (y_hat, h)\n",
    "\n",
    "\n",
    "def back(X, Y, y_hat, w2, h):\n",
    "    w2_gradient = np.matmul(prepend_bias(h).T, (y_hat - Y)) / X.shape[0]\n",
    "    w1_gradient = np.matmul(prepend_bias(X).T, np.matmul(y_hat - Y, w2[1:].T)\n",
    "                            * sigmoid_gradient(h)) / X.shape[0]\n",
    "    return (w1_gradient, w2_gradient)\n",
    "\n",
    "\n",
    "def classify(X, w1, w2):\n",
    "    y_hat, _ = forward(X, w1, w2)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def initialize_weights(n_input_variables, n_hidden_nodes, n_classes):\n",
    "    w1_rows = n_input_variables + 1\n",
    "    w1 = np.random.randn(w1_rows, n_hidden_nodes) * np.sqrt(1 / w1_rows)\n",
    "\n",
    "    w2_rows = n_hidden_nodes + 1\n",
    "    w2 = np.random.randn(w2_rows, n_classes) * np.sqrt(1 / w2_rows)\n",
    "\n",
    "    return (w1, w2)\n",
    "\n",
    "\n",
    "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat, _ = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(\"Iteration: %5d, Loss: %.8f, Accuracy: %.2f%%\" %\n",
    "          (iteration, training_loss, accuracy))\n",
    "    return accuracy, training_loss\n",
    "    \n",
    "\n",
    "def train(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr):\n",
    "    start_time = time.time()\n",
    "    success_rates = []\n",
    "    losses = []\n",
    "    times = []\n",
    "    n_input_variables = X_train.shape[1]\n",
    "    n_classes = Y_train.shape[1]\n",
    "    w1, w2 = initialize_weights(n_input_variables, n_hidden_nodes, n_classes)\n",
    "    for iteration in range(iterations):\n",
    "        y_hat, h = forward(X_train, w1, w2)\n",
    "        w1_gradient, w2_gradient = back(X_train, Y_train, y_hat, w2, h)\n",
    "        w1 = w1 - (w1_gradient * lr)\n",
    "        w2 = w2 - (w2_gradient * lr)\n",
    "        accuracy, training_loss = report(iteration, X_train, Y_train, X_test, Y_test, w1, w2)\n",
    "        success_rates.append(accuracy)\n",
    "        losses.append(training_loss)\n",
    "        times.append(time.time()-start_time)\n",
    "    return (w1, w2, success_rates, losses, times)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fed4dbd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# NEU: minibatch-Verfahren\n",
    "\n",
    "def prepare_batches(X_train, Y_train, batchsize):\n",
    "    x_batches = []\n",
    "    y_batches = []\n",
    "    n_examples = X_train.shape[0]\n",
    "    for b in range(0, n_examples, batchsize):\n",
    "        e = b + batchsize\n",
    "        x_batches.append(X_train[b:e])\n",
    "        y_batches.append(Y_train[b:e])\n",
    "    return x_batches, y_batches\n",
    "\n",
    "\n",
    "def train_minibatch(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr, batchsize):\n",
    "    start_time = time.time()\n",
    "    success_rates = []\n",
    "    losses = []\n",
    "    times = []\n",
    "    success_rates_batches = []\n",
    "    losses_batches = []    \n",
    "    times_batches = []\n",
    "    n_input_variables = X_train.shape[1]\n",
    "    n_classes = Y_train.shape[1]\n",
    "    w1, w2 = initialize_weights(n_input_variables, n_hidden_nodes, n_classes)\n",
    "    x_batches, y_batches = prepare_batches(X_train, Y_train, batchsize)\n",
    "    print('prepared %d batches'%len(x_batches))\n",
    "    for iteration in range(iterations):\n",
    "        if iteration >= 20:\n",
    "            lr = 0.1\n",
    "        if iteration >= 30:\n",
    "            lr = 0.01\n",
    "        if iteration >= 40:\n",
    "            lr = 0.03\n",
    "        for batch in range(len(x_batches)):\n",
    "            y_hat, h = forward(x_batches[batch], w1, w2)\n",
    "            w1_gradient, w2_gradient = back(x_batches[batch], y_batches[batch], y_hat, w2, h)\n",
    "            w1 = w1 - (w1_gradient * lr)\n",
    "            w2 = w2 - (w2_gradient * lr)\n",
    "            accuracy, training_loss = report_minibatch(iteration, batch, X_train, Y_train, X_test, Y_test, w1, w2)\n",
    "            success_rates_batches.append(accuracy)\n",
    "            losses_batches.append(training_loss)\n",
    "            times_batches.append(time.time()-start_time)\n",
    "        success_rates.append(accuracy)\n",
    "        losses.append(training_loss)\n",
    "        times.append(time.time()-start_time)\n",
    "    return (w1, w2, success_rates, losses, times, success_rates_batches, losses_batches, times_batches)\n",
    "\n",
    "\n",
    "def report_minibatch(iteration, batch, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat, _ = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(\"Iteration: %5d, batch: %3d, Loss: %.8f, Accuracy: %.2f%%\" %\n",
    "          (iteration, batch, training_los\n",
    "           s, accuracy))\n",
    "    return accuracy, training_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e445f71f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d6da70d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4789bf1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store your success rates\n",
    "# !!! UNCOMMENT, EXECUTE, AND COMMENT AGAIN !!!\n",
    "# !!! DON'T EXECUTE THE LINES AGAIN AFTER STARTING TO COLLECT RESULTS !!!\n",
    "\n",
    "#success_rates = {}\n",
    "#losses = {}\n",
    "#times = {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "cc402ea9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "prepared 10 batches\n",
      "Iteration:     0, batch:   0, Loss: 5.53953041, Accuracy: 22.56%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/r7d1zvcs4g5_lpdwx6rf5tgr0000gn/T/ipykernel_93577/150598522.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:     0, batch:   1, Loss: 7.31623678, Accuracy: 48.61%\n",
      "Iteration:     0, batch:   2, Loss: 5.13209651, Accuracy: 36.58%\n",
      "Iteration:     0, batch:   3, Loss: 2.59506544, Accuracy: 32.15%\n",
      "Iteration:     0, batch:   4, Loss: 1.73479192, Accuracy: 40.61%\n",
      "Iteration:     0, batch:   5, Loss: 1.34065095, Accuracy: 67.47%\n",
      "Iteration:     0, batch:   6, Loss: 1.08545840, Accuracy: 78.55%\n",
      "Iteration:     0, batch:   7, Loss: 0.91774324, Accuracy: 80.97%\n",
      "Iteration:     0, batch:   8, Loss: 0.81646242, Accuracy: 82.49%\n",
      "Iteration:     0, batch:   9, Loss: 0.75049926, Accuracy: 82.15%\n",
      "Iteration:     1, batch:   0, Loss: 0.73008355, Accuracy: 81.79%\n",
      "Iteration:     1, batch:   1, Loss: 0.72046618, Accuracy: 79.12%\n",
      "Iteration:     1, batch:   2, Loss: 0.71074493, Accuracy: 80.20%\n",
      "Iteration:     1, batch:   3, Loss: 0.62350101, Accuracy: 81.78%\n",
      "Iteration:     1, batch:   4, Loss: 0.60515018, Accuracy: 82.79%\n",
      "Iteration:     1, batch:   5, Loss: 0.62615487, Accuracy: 78.62%\n",
      "Iteration:     1, batch:   6, Loss: 0.61322801, Accuracy: 80.07%\n",
      "Iteration:     1, batch:   7, Loss: 0.63330149, Accuracy: 77.67%\n",
      "Iteration:     1, batch:   8, Loss: 0.56128125, Accuracy: 82.35%\n",
      "Iteration:     1, batch:   9, Loss: 0.52057153, Accuracy: 83.15%\n",
      "Iteration:     2, batch:   0, Loss: 0.48296824, Accuracy: 85.90%\n",
      "Iteration:     2, batch:   1, Loss: 0.46934824, Accuracy: 85.66%\n",
      "Iteration:     2, batch:   2, Loss: 0.45373714, Accuracy: 86.56%\n",
      "Iteration:     2, batch:   3, Loss: 0.44963955, Accuracy: 86.14%\n",
      "Iteration:     2, batch:   4, Loss: 0.43695660, Accuracy: 86.70%\n",
      "Iteration:     2, batch:   5, Loss: 0.41444387, Accuracy: 87.96%\n",
      "Iteration:     2, batch:   6, Loss: 0.39365454, Accuracy: 88.90%\n",
      "Iteration:     2, batch:   7, Loss: 0.39533223, Accuracy: 89.33%\n",
      "Iteration:     2, batch:   8, Loss: 0.38203814, Accuracy: 88.94%\n",
      "Iteration:     2, batch:   9, Loss: 0.36638374, Accuracy: 90.08%\n",
      "Iteration:     3, batch:   0, Loss: 0.35527633, Accuracy: 90.43%\n",
      "Iteration:     3, batch:   1, Loss: 0.34944397, Accuracy: 90.38%\n",
      "Iteration:     3, batch:   2, Loss: 0.35497763, Accuracy: 89.84%\n",
      "Iteration:     3, batch:   3, Loss: 0.34452157, Accuracy: 90.42%\n",
      "Iteration:     3, batch:   4, Loss: 0.32605754, Accuracy: 90.95%\n",
      "Iteration:     3, batch:   5, Loss: 0.31831811, Accuracy: 91.38%\n",
      "Iteration:     3, batch:   6, Loss: 0.31192343, Accuracy: 91.47%\n",
      "Iteration:     3, batch:   7, Loss: 0.30781651, Accuracy: 91.43%\n",
      "Iteration:     3, batch:   8, Loss: 0.30082928, Accuracy: 91.71%\n",
      "Iteration:     3, batch:   9, Loss: 0.29699602, Accuracy: 91.83%\n",
      "Iteration:     4, batch:   0, Loss: 0.29281964, Accuracy: 91.93%\n",
      "Iteration:     4, batch:   1, Loss: 0.29106484, Accuracy: 91.83%\n",
      "Iteration:     4, batch:   2, Loss: 0.28910078, Accuracy: 91.82%\n",
      "Iteration:     4, batch:   3, Loss: 0.28616249, Accuracy: 91.90%\n",
      "Iteration:     4, batch:   4, Loss: 0.28132259, Accuracy: 92.09%\n",
      "Iteration:     4, batch:   5, Loss: 0.27932818, Accuracy: 92.08%\n",
      "Iteration:     4, batch:   6, Loss: 0.27653511, Accuracy: 92.21%\n",
      "Iteration:     4, batch:   7, Loss: 0.27409128, Accuracy: 92.25%\n",
      "Iteration:     4, batch:   8, Loss: 0.27037011, Accuracy: 92.37%\n",
      "Iteration:     4, batch:   9, Loss: 0.26659152, Accuracy: 92.53%\n",
      "Iteration:     5, batch:   0, Loss: 0.26368948, Accuracy: 92.67%\n",
      "Iteration:     5, batch:   1, Loss: 0.26280108, Accuracy: 92.37%\n",
      "Iteration:     5, batch:   2, Loss: 0.25937756, Accuracy: 92.73%\n",
      "Iteration:     5, batch:   3, Loss: 0.25778022, Accuracy: 92.57%\n",
      "Iteration:     5, batch:   4, Loss: 0.25744651, Accuracy: 92.64%\n",
      "Iteration:     5, batch:   5, Loss: 0.25736458, Accuracy: 92.40%\n",
      "Iteration:     5, batch:   6, Loss: 0.25413846, Accuracy: 92.79%\n",
      "Iteration:     5, batch:   7, Loss: 0.25292134, Accuracy: 92.63%\n",
      "Iteration:     5, batch:   8, Loss: 0.24919048, Accuracy: 92.71%\n",
      "Iteration:     5, batch:   9, Loss: 0.24374709, Accuracy: 92.90%\n",
      "Iteration:     6, batch:   0, Loss: 0.24143882, Accuracy: 92.80%\n",
      "Iteration:     6, batch:   1, Loss: 0.24344163, Accuracy: 92.84%\n",
      "Iteration:     6, batch:   2, Loss: 0.23988241, Accuracy: 93.03%\n",
      "Iteration:     6, batch:   3, Loss: 0.23836628, Accuracy: 93.09%\n",
      "Iteration:     6, batch:   4, Loss: 0.23950760, Accuracy: 92.92%\n",
      "Iteration:     6, batch:   5, Loss: 0.23793711, Accuracy: 92.69%\n",
      "Iteration:     6, batch:   6, Loss: 0.23278921, Accuracy: 93.21%\n",
      "Iteration:     6, batch:   7, Loss: 0.23057830, Accuracy: 93.15%\n",
      "Iteration:     6, batch:   8, Loss: 0.22739962, Accuracy: 93.36%\n",
      "Iteration:     6, batch:   9, Loss: 0.22389104, Accuracy: 93.30%\n",
      "Iteration:     7, batch:   0, Loss: 0.22202643, Accuracy: 93.43%\n",
      "Iteration:     7, batch:   1, Loss: 0.22180521, Accuracy: 93.35%\n",
      "Iteration:     7, batch:   2, Loss: 0.22098193, Accuracy: 93.45%\n",
      "Iteration:     7, batch:   3, Loss: 0.21980217, Accuracy: 93.51%\n",
      "Iteration:     7, batch:   4, Loss: 0.21748265, Accuracy: 93.36%\n",
      "Iteration:     7, batch:   5, Loss: 0.21666004, Accuracy: 93.39%\n",
      "Iteration:     7, batch:   6, Loss: 0.21436897, Accuracy: 93.58%\n",
      "Iteration:     7, batch:   7, Loss: 0.21584901, Accuracy: 93.46%\n",
      "Iteration:     7, batch:   8, Loss: 0.21384478, Accuracy: 93.52%\n",
      "Iteration:     7, batch:   9, Loss: 0.21091521, Accuracy: 93.56%\n",
      "Iteration:     8, batch:   0, Loss: 0.20791849, Accuracy: 93.66%\n",
      "Iteration:     8, batch:   1, Loss: 0.20755318, Accuracy: 93.59%\n",
      "Iteration:     8, batch:   2, Loss: 0.20773502, Accuracy: 93.69%\n",
      "Iteration:     8, batch:   3, Loss: 0.20750617, Accuracy: 93.68%\n",
      "Iteration:     8, batch:   4, Loss: 0.20383293, Accuracy: 93.69%\n",
      "Iteration:     8, batch:   5, Loss: 0.20147216, Accuracy: 93.67%\n",
      "Iteration:     8, batch:   6, Loss: 0.20199159, Accuracy: 93.73%\n",
      "Iteration:     8, batch:   7, Loss: 0.20264828, Accuracy: 93.67%\n",
      "Iteration:     8, batch:   8, Loss: 0.19908834, Accuracy: 93.72%\n",
      "Iteration:     8, batch:   9, Loss: 0.19835547, Accuracy: 93.86%\n",
      "Iteration:     9, batch:   0, Loss: 0.19614432, Accuracy: 93.87%\n",
      "Iteration:     9, batch:   1, Loss: 0.19753601, Accuracy: 93.83%\n",
      "Iteration:     9, batch:   2, Loss: 0.19628533, Accuracy: 93.86%\n",
      "Iteration:     9, batch:   3, Loss: 0.19572477, Accuracy: 93.78%\n",
      "Iteration:     9, batch:   4, Loss: 0.19326208, Accuracy: 93.92%\n",
      "Iteration:     9, batch:   5, Loss: 0.19114716, Accuracy: 93.79%\n",
      "Iteration:     9, batch:   6, Loss: 0.19056416, Accuracy: 94.11%\n",
      "Iteration:     9, batch:   7, Loss: 0.18978827, Accuracy: 94.00%\n",
      "Iteration:     9, batch:   8, Loss: 0.18801984, Accuracy: 94.07%\n",
      "Iteration:     9, batch:   9, Loss: 0.18678704, Accuracy: 94.09%\n",
      "Iteration:    10, batch:   0, Loss: 0.18520973, Accuracy: 93.99%\n",
      "Iteration:    10, batch:   1, Loss: 0.18689037, Accuracy: 94.18%\n",
      "Iteration:    10, batch:   2, Loss: 0.18630133, Accuracy: 93.97%\n",
      "Iteration:    10, batch:   3, Loss: 0.18458835, Accuracy: 94.10%\n",
      "Iteration:    10, batch:   4, Loss: 0.18344062, Accuracy: 94.20%\n",
      "Iteration:    10, batch:   5, Loss: 0.18295119, Accuracy: 94.05%\n",
      "Iteration:    10, batch:   6, Loss: 0.18027809, Accuracy: 94.10%\n",
      "Iteration:    10, batch:   7, Loss: 0.17934328, Accuracy: 94.25%\n",
      "Iteration:    10, batch:   8, Loss: 0.17923614, Accuracy: 94.25%\n",
      "Iteration:    10, batch:   9, Loss: 0.17685392, Accuracy: 94.30%\n",
      "Iteration:    11, batch:   0, Loss: 0.17564738, Accuracy: 94.31%\n",
      "Iteration:    11, batch:   1, Loss: 0.17602508, Accuracy: 94.12%\n",
      "Iteration:    11, batch:   2, Loss: 0.17461208, Accuracy: 94.33%\n",
      "Iteration:    11, batch:   3, Loss: 0.17351039, Accuracy: 94.45%\n",
      "Iteration:    11, batch:   4, Loss: 0.17331720, Accuracy: 94.33%\n",
      "Iteration:    11, batch:   5, Loss: 0.17219389, Accuracy: 94.24%\n",
      "Iteration:    11, batch:   6, Loss: 0.17133563, Accuracy: 94.37%\n",
      "Iteration:    11, batch:   7, Loss: 0.17150487, Accuracy: 94.38%\n",
      "Iteration:    11, batch:   8, Loss: 0.17019412, Accuracy: 94.31%\n",
      "Iteration:    11, batch:   9, Loss: 0.16862207, Accuracy: 94.49%\n",
      "Iteration:    12, batch:   0, Loss: 0.16707846, Accuracy: 94.43%\n",
      "Iteration:    12, batch:   1, Loss: 0.16861721, Accuracy: 94.32%\n",
      "Iteration:    12, batch:   2, Loss: 0.16906893, Accuracy: 94.31%\n",
      "Iteration:    12, batch:   3, Loss: 0.16689256, Accuracy: 94.54%\n",
      "Iteration:    12, batch:   4, Loss: 0.16567398, Accuracy: 94.56%\n",
      "Iteration:    12, batch:   5, Loss: 0.16500505, Accuracy: 94.44%\n",
      "Iteration:    12, batch:   6, Loss: 0.16265036, Accuracy: 94.41%\n",
      "Iteration:    12, batch:   7, Loss: 0.16354020, Accuracy: 94.52%\n",
      "Iteration:    12, batch:   8, Loss: 0.16104630, Accuracy: 94.56%\n",
      "Iteration:    12, batch:   9, Loss: 0.16087542, Accuracy: 94.58%\n",
      "Iteration:    13, batch:   0, Loss: 0.15967295, Accuracy: 94.51%\n",
      "Iteration:    13, batch:   1, Loss: 0.16122318, Accuracy: 94.59%\n",
      "Iteration:    13, batch:   2, Loss: 0.16204295, Accuracy: 94.36%\n",
      "Iteration:    13, batch:   3, Loss: 0.16178940, Accuracy: 94.26%\n",
      "Iteration:    13, batch:   4, Loss: 0.15944839, Accuracy: 94.41%\n",
      "Iteration:    13, batch:   5, Loss: 0.15639582, Accuracy: 94.60%\n",
      "Iteration:    13, batch:   6, Loss: 0.15572366, Accuracy: 94.77%\n",
      "Iteration:    13, batch:   7, Loss: 0.15584963, Accuracy: 94.67%\n",
      "Iteration:    13, batch:   8, Loss: 0.15426899, Accuracy: 94.56%\n",
      "Iteration:    13, batch:   9, Loss: 0.15235083, Accuracy: 94.65%\n",
      "Iteration:    14, batch:   0, Loss: 0.15215093, Accuracy: 94.63%\n",
      "Iteration:    14, batch:   1, Loss: 0.15300773, Accuracy: 94.55%\n",
      "Iteration:    14, batch:   2, Loss: 0.15331276, Accuracy: 94.58%\n",
      "Iteration:    14, batch:   3, Loss: 0.15176735, Accuracy: 94.67%\n",
      "Iteration:    14, batch:   4, Loss: 0.14966889, Accuracy: 94.79%\n",
      "Iteration:    14, batch:   5, Loss: 0.14850121, Accuracy: 94.68%\n",
      "Iteration:    14, batch:   6, Loss: 0.14655884, Accuracy: 94.73%\n",
      "Iteration:    14, batch:   7, Loss: 0.14764474, Accuracy: 94.90%\n",
      "Iteration:    14, batch:   8, Loss: 0.14798967, Accuracy: 94.78%\n",
      "Iteration:    14, batch:   9, Loss: 0.14589986, Accuracy: 94.84%\n",
      "Iteration:    15, batch:   0, Loss: 0.14432832, Accuracy: 94.87%\n",
      "Iteration:    15, batch:   1, Loss: 0.14632651, Accuracy: 94.73%\n",
      "Iteration:    15, batch:   2, Loss: 0.14477369, Accuracy: 94.67%\n",
      "Iteration:    15, batch:   3, Loss: 0.14436142, Accuracy: 94.85%\n",
      "Iteration:    15, batch:   4, Loss: 0.14428966, Accuracy: 94.74%\n",
      "Iteration:    15, batch:   5, Loss: 0.14343024, Accuracy: 94.79%\n",
      "Iteration:    15, batch:   6, Loss: 0.14316844, Accuracy: 94.79%\n",
      "Iteration:    15, batch:   7, Loss: 0.14170194, Accuracy: 94.86%\n",
      "Iteration:    15, batch:   8, Loss: 0.14073674, Accuracy: 94.82%\n",
      "Iteration:    15, batch:   9, Loss: 0.13998731, Accuracy: 94.87%\n",
      "Iteration:    16, batch:   0, Loss: 0.13890371, Accuracy: 94.93%\n",
      "Iteration:    16, batch:   1, Loss: 0.13986205, Accuracy: 94.80%\n",
      "Iteration:    16, batch:   2, Loss: 0.13960240, Accuracy: 94.88%\n",
      "Iteration:    16, batch:   3, Loss: 0.13868996, Accuracy: 94.83%\n",
      "Iteration:    16, batch:   4, Loss: 0.13824976, Accuracy: 94.91%\n",
      "Iteration:    16, batch:   5, Loss: 0.13616238, Accuracy: 94.91%\n",
      "Iteration:    16, batch:   6, Loss: 0.13519764, Accuracy: 94.94%\n",
      "Iteration:    16, batch:   7, Loss: 0.13607619, Accuracy: 95.00%\n",
      "Iteration:    16, batch:   8, Loss: 0.13590718, Accuracy: 95.01%\n",
      "Iteration:    16, batch:   9, Loss: 0.13370732, Accuracy: 94.93%\n",
      "Iteration:    17, batch:   0, Loss: 0.13344733, Accuracy: 95.03%\n",
      "Iteration:    17, batch:   1, Loss: 0.13568725, Accuracy: 94.95%\n",
      "Iteration:    17, batch:   2, Loss: 0.13428112, Accuracy: 95.03%\n",
      "Iteration:    17, batch:   3, Loss: 0.13265003, Accuracy: 95.01%\n",
      "Iteration:    17, batch:   4, Loss: 0.13280056, Accuracy: 94.95%\n",
      "Iteration:    17, batch:   5, Loss: 0.13158931, Accuracy: 95.02%\n",
      "Iteration:    17, batch:   6, Loss: 0.13153046, Accuracy: 95.02%\n",
      "Iteration:    17, batch:   7, Loss: 0.13142192, Accuracy: 95.18%\n",
      "Iteration:    17, batch:   8, Loss: 0.13079691, Accuracy: 95.04%\n",
      "Iteration:    17, batch:   9, Loss: 0.12975238, Accuracy: 95.09%\n",
      "Iteration:    18, batch:   0, Loss: 0.12862271, Accuracy: 95.16%\n",
      "Iteration:    18, batch:   1, Loss: 0.12990479, Accuracy: 95.04%\n",
      "Iteration:    18, batch:   2, Loss: 0.12983353, Accuracy: 95.09%\n",
      "Iteration:    18, batch:   3, Loss: 0.12955620, Accuracy: 95.05%\n",
      "Iteration:    18, batch:   4, Loss: 0.12787390, Accuracy: 94.91%\n",
      "Iteration:    18, batch:   5, Loss: 0.12670770, Accuracy: 95.04%\n",
      "Iteration:    18, batch:   6, Loss: 0.12562513, Accuracy: 95.12%\n",
      "Iteration:    18, batch:   7, Loss: 0.12660805, Accuracy: 95.16%\n",
      "Iteration:    18, batch:   8, Loss: 0.12689594, Accuracy: 95.13%\n",
      "Iteration:    18, batch:   9, Loss: 0.12640934, Accuracy: 95.16%\n",
      "Iteration:    19, batch:   0, Loss: 0.12421885, Accuracy: 95.13%\n",
      "Iteration:    19, batch:   1, Loss: 0.12563875, Accuracy: 95.04%\n",
      "Iteration:    19, batch:   2, Loss: 0.12451825, Accuracy: 95.14%\n",
      "Iteration:    19, batch:   3, Loss: 0.12410859, Accuracy: 95.18%\n",
      "Iteration:    19, batch:   4, Loss: 0.12408100, Accuracy: 95.09%\n",
      "Iteration:    19, batch:   5, Loss: 0.12307275, Accuracy: 95.24%\n",
      "Iteration:    19, batch:   6, Loss: 0.12313031, Accuracy: 95.22%\n",
      "Iteration:    19, batch:   7, Loss: 0.12354259, Accuracy: 95.15%\n",
      "Iteration:    19, batch:   8, Loss: 0.12277226, Accuracy: 95.17%\n",
      "Iteration:    19, batch:   9, Loss: 0.12078856, Accuracy: 95.28%\n",
      "Iteration:    20, batch:   0, Loss: 0.11987031, Accuracy: 95.35%\n",
      "Iteration:    20, batch:   1, Loss: 0.12110930, Accuracy: 95.34%\n",
      "Iteration:    20, batch:   2, Loss: 0.12061259, Accuracy: 95.34%\n",
      "Iteration:    20, batch:   3, Loss: 0.12159232, Accuracy: 95.27%\n",
      "Iteration:    20, batch:   4, Loss: 0.11951682, Accuracy: 95.22%\n",
      "Iteration:    20, batch:   5, Loss: 0.11900955, Accuracy: 95.25%\n",
      "Iteration:    20, batch:   6, Loss: 0.11881494, Accuracy: 95.24%\n",
      "Iteration:    20, batch:   7, Loss: 0.11911148, Accuracy: 95.24%\n",
      "Iteration:    20, batch:   8, Loss: 0.11734200, Accuracy: 95.33%\n",
      "Iteration:    20, batch:   9, Loss: 0.11680849, Accuracy: 95.32%\n",
      "Iteration:    21, batch:   0, Loss: 0.11620509, Accuracy: 95.39%\n",
      "Iteration:    21, batch:   1, Loss: 0.11798752, Accuracy: 95.35%\n",
      "Iteration:    21, batch:   2, Loss: 0.11638473, Accuracy: 95.29%\n",
      "Iteration:    21, batch:   3, Loss: 0.11522137, Accuracy: 95.38%\n",
      "Iteration:    21, batch:   4, Loss: 0.11569723, Accuracy: 95.29%\n",
      "Iteration:    21, batch:   5, Loss: 0.11368926, Accuracy: 95.25%\n",
      "Iteration:    21, batch:   6, Loss: 0.11354147, Accuracy: 95.40%\n",
      "Iteration:    21, batch:   7, Loss: 0.11367746, Accuracy: 95.34%\n",
      "Iteration:    21, batch:   8, Loss: 0.11401735, Accuracy: 95.39%\n",
      "Iteration:    21, batch:   9, Loss: 0.11277307, Accuracy: 95.35%\n",
      "Iteration:    22, batch:   0, Loss: 0.11231282, Accuracy: 95.30%\n",
      "Iteration:    22, batch:   1, Loss: 0.11300581, Accuracy: 95.41%\n",
      "Iteration:    22, batch:   2, Loss: 0.11282553, Accuracy: 95.29%\n",
      "Iteration:    22, batch:   3, Loss: 0.11318196, Accuracy: 95.36%\n",
      "Iteration:    22, batch:   4, Loss: 0.11245726, Accuracy: 95.20%\n",
      "Iteration:    22, batch:   5, Loss: 0.11027557, Accuracy: 95.29%\n",
      "Iteration:    22, batch:   6, Loss: 0.11035989, Accuracy: 95.43%\n",
      "Iteration:    22, batch:   7, Loss: 0.11034948, Accuracy: 95.32%\n",
      "Iteration:    22, batch:   8, Loss: 0.10958887, Accuracy: 95.44%\n",
      "Iteration:    22, batch:   9, Loss: 0.10864513, Accuracy: 95.30%\n",
      "Iteration:    23, batch:   0, Loss: 0.10747813, Accuracy: 95.35%\n",
      "Iteration:    23, batch:   1, Loss: 0.10908897, Accuracy: 95.44%\n",
      "Iteration:    23, batch:   2, Loss: 0.10963421, Accuracy: 95.39%\n",
      "Iteration:    23, batch:   3, Loss: 0.10856486, Accuracy: 95.39%\n",
      "Iteration:    23, batch:   4, Loss: 0.10823963, Accuracy: 95.43%\n",
      "Iteration:    23, batch:   5, Loss: 0.10706833, Accuracy: 95.38%\n",
      "Iteration:    23, batch:   6, Loss: 0.10643643, Accuracy: 95.47%\n",
      "Iteration:    23, batch:   7, Loss: 0.10706809, Accuracy: 95.54%\n",
      "Iteration:    23, batch:   8, Loss: 0.10591089, Accuracy: 95.40%\n",
      "Iteration:    23, batch:   9, Loss: 0.10566330, Accuracy: 95.43%\n",
      "Iteration:    24, batch:   0, Loss: 0.10426185, Accuracy: 95.52%\n",
      "Iteration:    24, batch:   1, Loss: 0.10567495, Accuracy: 95.61%\n",
      "Iteration:    24, batch:   2, Loss: 0.10427725, Accuracy: 95.47%\n",
      "Iteration:    24, batch:   3, Loss: 0.10453527, Accuracy: 95.52%\n",
      "Iteration:    24, batch:   4, Loss: 0.10530882, Accuracy: 95.48%\n",
      "Iteration:    24, batch:   5, Loss: 0.10505168, Accuracy: 95.44%\n",
      "Iteration:    24, batch:   6, Loss: 0.10395289, Accuracy: 95.40%\n",
      "Iteration:    24, batch:   7, Loss: 0.10393874, Accuracy: 95.38%\n",
      "Iteration:    24, batch:   8, Loss: 0.10271837, Accuracy: 95.58%\n",
      "Iteration:    24, batch:   9, Loss: 0.10247476, Accuracy: 95.53%\n",
      "Iteration:    25, batch:   0, Loss: 0.10109586, Accuracy: 95.60%\n",
      "Iteration:    25, batch:   1, Loss: 0.10340988, Accuracy: 95.53%\n",
      "Iteration:    25, batch:   2, Loss: 0.10217129, Accuracy: 95.56%\n",
      "Iteration:    25, batch:   3, Loss: 0.10117523, Accuracy: 95.62%\n",
      "Iteration:    25, batch:   4, Loss: 0.10174066, Accuracy: 95.65%\n",
      "Iteration:    25, batch:   5, Loss: 0.09955109, Accuracy: 95.68%\n",
      "Iteration:    25, batch:   6, Loss: 0.10031974, Accuracy: 95.74%\n",
      "Iteration:    25, batch:   7, Loss: 0.10048037, Accuracy: 95.58%\n",
      "Iteration:    25, batch:   8, Loss: 0.09977675, Accuracy: 95.55%\n",
      "Iteration:    25, batch:   9, Loss: 0.09857494, Accuracy: 95.52%\n",
      "Iteration:    26, batch:   0, Loss: 0.09785394, Accuracy: 95.56%\n",
      "Iteration:    26, batch:   1, Loss: 0.09893982, Accuracy: 95.49%\n",
      "Iteration:    26, batch:   2, Loss: 0.09863852, Accuracy: 95.63%\n",
      "Iteration:    26, batch:   3, Loss: 0.09890021, Accuracy: 95.64%\n",
      "Iteration:    26, batch:   4, Loss: 0.09790661, Accuracy: 95.66%\n",
      "Iteration:    26, batch:   5, Loss: 0.09663592, Accuracy: 95.71%\n",
      "Iteration:    26, batch:   6, Loss: 0.09688678, Accuracy: 95.76%\n",
      "Iteration:    26, batch:   7, Loss: 0.09599217, Accuracy: 95.65%\n",
      "Iteration:    26, batch:   8, Loss: 0.09560486, Accuracy: 95.73%\n",
      "Iteration:    26, batch:   9, Loss: 0.09424448, Accuracy: 95.64%\n",
      "Iteration:    27, batch:   0, Loss: 0.09384691, Accuracy: 95.69%\n",
      "Iteration:    27, batch:   1, Loss: 0.09505862, Accuracy: 95.53%\n",
      "Iteration:    27, batch:   2, Loss: 0.09504997, Accuracy: 95.66%\n",
      "Iteration:    27, batch:   3, Loss: 0.09527352, Accuracy: 95.76%\n",
      "Iteration:    27, batch:   4, Loss: 0.09512795, Accuracy: 95.61%\n",
      "Iteration:    27, batch:   5, Loss: 0.09471824, Accuracy: 95.57%\n",
      "Iteration:    27, batch:   6, Loss: 0.09432338, Accuracy: 95.81%\n",
      "Iteration:    27, batch:   7, Loss: 0.09362193, Accuracy: 95.65%\n",
      "Iteration:    27, batch:   8, Loss: 0.09301979, Accuracy: 95.78%\n",
      "Iteration:    27, batch:   9, Loss: 0.09210621, Accuracy: 95.77%\n",
      "Iteration:    28, batch:   0, Loss: 0.09173631, Accuracy: 95.75%\n",
      "Iteration:    28, batch:   1, Loss: 0.09279449, Accuracy: 95.73%\n",
      "Iteration:    28, batch:   2, Loss: 0.09283600, Accuracy: 95.79%\n",
      "Iteration:    28, batch:   3, Loss: 0.09363714, Accuracy: 95.77%\n",
      "Iteration:    28, batch:   4, Loss: 0.09293545, Accuracy: 95.77%\n",
      "Iteration:    28, batch:   5, Loss: 0.09178620, Accuracy: 95.77%\n",
      "Iteration:    28, batch:   6, Loss: 0.09184722, Accuracy: 95.73%\n",
      "Iteration:    28, batch:   7, Loss: 0.09260096, Accuracy: 95.65%\n",
      "Iteration:    28, batch:   8, Loss: 0.09114959, Accuracy: 95.90%\n",
      "Iteration:    28, batch:   9, Loss: 0.09009037, Accuracy: 95.82%\n",
      "Iteration:    29, batch:   0, Loss: 0.08981237, Accuracy: 95.89%\n",
      "Iteration:    29, batch:   1, Loss: 0.09343232, Accuracy: 95.83%\n",
      "Iteration:    29, batch:   2, Loss: 0.09112767, Accuracy: 95.82%\n",
      "Iteration:    29, batch:   3, Loss: 0.09037510, Accuracy: 95.76%\n",
      "Iteration:    29, batch:   4, Loss: 0.09031794, Accuracy: 95.81%\n",
      "Iteration:    29, batch:   5, Loss: 0.08905464, Accuracy: 95.90%\n",
      "Iteration:    29, batch:   6, Loss: 0.08950292, Accuracy: 95.80%\n",
      "Iteration:    29, batch:   7, Loss: 0.09039026, Accuracy: 95.76%\n",
      "Iteration:    29, batch:   8, Loss: 0.08860357, Accuracy: 95.84%\n",
      "Iteration:    29, batch:   9, Loss: 0.08734047, Accuracy: 95.89%\n",
      "Iteration:    30, batch:   0, Loss: 0.08555523, Accuracy: 95.93%\n",
      "Iteration:    30, batch:   1, Loss: 0.08417966, Accuracy: 95.91%\n",
      "Iteration:    30, batch:   2, Loss: 0.08323920, Accuracy: 95.94%\n",
      "Iteration:    30, batch:   3, Loss: 0.08248582, Accuracy: 95.93%\n",
      "Iteration:    30, batch:   4, Loss: 0.08200130, Accuracy: 95.96%\n",
      "Iteration:    30, batch:   5, Loss: 0.08148990, Accuracy: 95.91%\n",
      "Iteration:    30, batch:   6, Loss: 0.08118969, Accuracy: 95.94%\n",
      "Iteration:    30, batch:   7, Loss: 0.08097899, Accuracy: 95.91%\n",
      "Iteration:    30, batch:   8, Loss: 0.08081574, Accuracy: 95.92%\n",
      "Iteration:    30, batch:   9, Loss: 0.08057623, Accuracy: 95.96%\n",
      "Iteration:    31, batch:   0, Loss: 0.08023063, Accuracy: 95.96%\n",
      "Iteration:    31, batch:   1, Loss: 0.08001570, Accuracy: 95.97%\n",
      "Iteration:    31, batch:   2, Loss: 0.07977165, Accuracy: 95.97%\n",
      "Iteration:    31, batch:   3, Loss: 0.07953051, Accuracy: 95.93%\n",
      "Iteration:    31, batch:   4, Loss: 0.07929033, Accuracy: 95.93%\n",
      "Iteration:    31, batch:   5, Loss: 0.07907738, Accuracy: 95.89%\n",
      "Iteration:    31, batch:   6, Loss: 0.07887702, Accuracy: 95.91%\n",
      "Iteration:    31, batch:   7, Loss: 0.07879841, Accuracy: 95.93%\n",
      "Iteration:    31, batch:   8, Loss: 0.07871797, Accuracy: 96.01%\n",
      "Iteration:    31, batch:   9, Loss: 0.07857550, Accuracy: 95.99%\n",
      "Iteration:    32, batch:   0, Loss: 0.07829769, Accuracy: 95.98%\n",
      "Iteration:    32, batch:   1, Loss: 0.07817631, Accuracy: 95.98%\n",
      "Iteration:    32, batch:   2, Loss: 0.07806902, Accuracy: 95.93%\n",
      "Iteration:    32, batch:   3, Loss: 0.07797104, Accuracy: 95.90%\n",
      "Iteration:    32, batch:   4, Loss: 0.07770331, Accuracy: 95.92%\n",
      "Iteration:    32, batch:   5, Loss: 0.07756917, Accuracy: 95.91%\n",
      "Iteration:    32, batch:   6, Loss: 0.07740956, Accuracy: 95.95%\n",
      "Iteration:    32, batch:   7, Loss: 0.07728821, Accuracy: 95.91%\n",
      "Iteration:    32, batch:   8, Loss: 0.07723458, Accuracy: 95.92%\n",
      "Iteration:    32, batch:   9, Loss: 0.07706156, Accuracy: 95.91%\n",
      "Iteration:    33, batch:   0, Loss: 0.07680345, Accuracy: 95.99%\n",
      "Iteration:    33, batch:   1, Loss: 0.07675657, Accuracy: 95.90%\n",
      "Iteration:    33, batch:   2, Loss: 0.07665673, Accuracy: 95.93%\n",
      "Iteration:    33, batch:   3, Loss: 0.07660039, Accuracy: 95.92%\n",
      "Iteration:    33, batch:   4, Loss: 0.07640530, Accuracy: 95.85%\n",
      "Iteration:    33, batch:   5, Loss: 0.07628731, Accuracy: 95.95%\n",
      "Iteration:    33, batch:   6, Loss: 0.07615199, Accuracy: 95.92%\n",
      "Iteration:    33, batch:   7, Loss: 0.07603831, Accuracy: 95.93%\n",
      "Iteration:    33, batch:   8, Loss: 0.07596281, Accuracy: 95.93%\n",
      "Iteration:    33, batch:   9, Loss: 0.07584150, Accuracy: 95.94%\n",
      "Iteration:    34, batch:   0, Loss: 0.07564898, Accuracy: 95.95%\n",
      "Iteration:    34, batch:   1, Loss: 0.07557186, Accuracy: 95.95%\n",
      "Iteration:    34, batch:   2, Loss: 0.07547334, Accuracy: 95.94%\n",
      "Iteration:    34, batch:   3, Loss: 0.07548856, Accuracy: 95.92%\n",
      "Iteration:    34, batch:   4, Loss: 0.07534320, Accuracy: 95.88%\n",
      "Iteration:    34, batch:   5, Loss: 0.07524021, Accuracy: 95.94%\n",
      "Iteration:    34, batch:   6, Loss: 0.07514039, Accuracy: 95.93%\n",
      "Iteration:    34, batch:   7, Loss: 0.07499867, Accuracy: 95.93%\n",
      "Iteration:    34, batch:   8, Loss: 0.07487862, Accuracy: 95.93%\n",
      "Iteration:    34, batch:   9, Loss: 0.07477817, Accuracy: 95.91%\n",
      "Iteration:    35, batch:   0, Loss: 0.07466513, Accuracy: 95.92%\n",
      "Iteration:    35, batch:   1, Loss: 0.07454236, Accuracy: 95.95%\n",
      "Iteration:    35, batch:   2, Loss: 0.07447268, Accuracy: 95.94%\n",
      "Iteration:    35, batch:   3, Loss: 0.07445390, Accuracy: 95.89%\n",
      "Iteration:    35, batch:   4, Loss: 0.07431712, Accuracy: 95.91%\n",
      "Iteration:    35, batch:   5, Loss: 0.07421719, Accuracy: 95.92%\n",
      "Iteration:    35, batch:   6, Loss: 0.07415403, Accuracy: 95.93%\n",
      "Iteration:    35, batch:   7, Loss: 0.07406252, Accuracy: 95.92%\n",
      "Iteration:    35, batch:   8, Loss: 0.07393713, Accuracy: 95.95%\n",
      "Iteration:    35, batch:   9, Loss: 0.07379370, Accuracy: 95.89%\n",
      "Iteration:    36, batch:   0, Loss: 0.07379241, Accuracy: 95.96%\n",
      "Iteration:    36, batch:   1, Loss: 0.07367513, Accuracy: 95.92%\n",
      "Iteration:    36, batch:   2, Loss: 0.07355092, Accuracy: 95.93%\n",
      "Iteration:    36, batch:   3, Loss: 0.07361611, Accuracy: 95.85%\n",
      "Iteration:    36, batch:   4, Loss: 0.07342599, Accuracy: 95.91%\n",
      "Iteration:    36, batch:   5, Loss: 0.07337428, Accuracy: 95.94%\n",
      "Iteration:    36, batch:   6, Loss: 0.07328073, Accuracy: 95.91%\n",
      "Iteration:    36, batch:   7, Loss: 0.07323302, Accuracy: 95.93%\n",
      "Iteration:    36, batch:   8, Loss: 0.07308382, Accuracy: 95.95%\n",
      "Iteration:    36, batch:   9, Loss: 0.07295901, Accuracy: 95.89%\n",
      "Iteration:    37, batch:   0, Loss: 0.07297282, Accuracy: 95.95%\n",
      "Iteration:    37, batch:   1, Loss: 0.07288825, Accuracy: 95.94%\n",
      "Iteration:    37, batch:   2, Loss: 0.07272860, Accuracy: 95.95%\n",
      "Iteration:    37, batch:   3, Loss: 0.07273882, Accuracy: 95.85%\n",
      "Iteration:    37, batch:   4, Loss: 0.07262785, Accuracy: 95.89%\n",
      "Iteration:    37, batch:   5, Loss: 0.07252814, Accuracy: 95.92%\n",
      "Iteration:    37, batch:   6, Loss: 0.07244518, Accuracy: 95.93%\n",
      "Iteration:    37, batch:   7, Loss: 0.07241218, Accuracy: 95.95%\n",
      "Iteration:    37, batch:   8, Loss: 0.07228576, Accuracy: 95.96%\n",
      "Iteration:    37, batch:   9, Loss: 0.07217115, Accuracy: 95.92%\n",
      "Iteration:    38, batch:   0, Loss: 0.07220350, Accuracy: 95.95%\n",
      "Iteration:    38, batch:   1, Loss: 0.07211843, Accuracy: 95.90%\n",
      "Iteration:    38, batch:   2, Loss: 0.07198715, Accuracy: 95.94%\n",
      "Iteration:    38, batch:   3, Loss: 0.07195775, Accuracy: 95.86%\n",
      "Iteration:    38, batch:   4, Loss: 0.07191397, Accuracy: 95.90%\n",
      "Iteration:    38, batch:   5, Loss: 0.07181693, Accuracy: 95.94%\n",
      "Iteration:    38, batch:   6, Loss: 0.07170702, Accuracy: 95.94%\n",
      "Iteration:    38, batch:   7, Loss: 0.07163081, Accuracy: 95.93%\n",
      "Iteration:    38, batch:   8, Loss: 0.07160437, Accuracy: 95.98%\n",
      "Iteration:    38, batch:   9, Loss: 0.07146225, Accuracy: 95.93%\n",
      "Iteration:    39, batch:   0, Loss: 0.07145778, Accuracy: 95.97%\n",
      "Iteration:    39, batch:   1, Loss: 0.07140230, Accuracy: 95.95%\n",
      "Iteration:    39, batch:   2, Loss: 0.07129946, Accuracy: 95.94%\n",
      "Iteration:    39, batch:   3, Loss: 0.07121216, Accuracy: 95.86%\n",
      "Iteration:    39, batch:   4, Loss: 0.07118367, Accuracy: 95.91%\n",
      "Iteration:    39, batch:   5, Loss: 0.07102017, Accuracy: 95.94%\n",
      "Iteration:    39, batch:   6, Loss: 0.07095703, Accuracy: 95.92%\n",
      "Iteration:    39, batch:   7, Loss: 0.07090203, Accuracy: 95.93%\n",
      "Iteration:    39, batch:   8, Loss: 0.07087500, Accuracy: 95.97%\n",
      "Iteration:    39, batch:   9, Loss: 0.07074685, Accuracy: 95.93%\n",
      "Iteration:    40, batch:   0, Loss: 0.07075280, Accuracy: 95.94%\n",
      "Iteration:    40, batch:   1, Loss: 0.07070091, Accuracy: 95.98%\n",
      "Iteration:    40, batch:   2, Loss: 0.07057956, Accuracy: 95.95%\n",
      "Iteration:    40, batch:   3, Loss: 0.07055681, Accuracy: 95.85%\n",
      "Iteration:    40, batch:   4, Loss: 0.07047885, Accuracy: 95.89%\n",
      "Iteration:    40, batch:   5, Loss: 0.07039157, Accuracy: 95.94%\n",
      "Iteration:    40, batch:   6, Loss: 0.07034197, Accuracy: 95.90%\n",
      "Iteration:    40, batch:   7, Loss: 0.07021771, Accuracy: 95.95%\n",
      "Iteration:    40, batch:   8, Loss: 0.07023211, Accuracy: 95.94%\n",
      "Iteration:    40, batch:   9, Loss: 0.07011149, Accuracy: 95.92%\n",
      "Iteration:    41, batch:   0, Loss: 0.07011924, Accuracy: 95.94%\n",
      "Iteration:    41, batch:   1, Loss: 0.07004382, Accuracy: 95.96%\n",
      "Iteration:    41, batch:   2, Loss: 0.06998412, Accuracy: 95.95%\n",
      "Iteration:    41, batch:   3, Loss: 0.06991270, Accuracy: 95.86%\n",
      "Iteration:    41, batch:   4, Loss: 0.06985111, Accuracy: 95.89%\n",
      "Iteration:    41, batch:   5, Loss: 0.06976643, Accuracy: 95.94%\n",
      "Iteration:    41, batch:   6, Loss: 0.06972842, Accuracy: 95.91%\n",
      "Iteration:    41, batch:   7, Loss: 0.06960576, Accuracy: 95.98%\n",
      "Iteration:    41, batch:   8, Loss: 0.06961219, Accuracy: 95.95%\n",
      "Iteration:    41, batch:   9, Loss: 0.06951005, Accuracy: 95.95%\n",
      "Iteration:    42, batch:   0, Loss: 0.06952904, Accuracy: 95.93%\n",
      "Iteration:    42, batch:   1, Loss: 0.06946701, Accuracy: 95.99%\n",
      "Iteration:    42, batch:   2, Loss: 0.06941010, Accuracy: 95.97%\n",
      "Iteration:    42, batch:   3, Loss: 0.06930567, Accuracy: 95.87%\n",
      "Iteration:    42, batch:   4, Loss: 0.06923751, Accuracy: 95.91%\n",
      "Iteration:    42, batch:   5, Loss: 0.06916590, Accuracy: 95.94%\n",
      "Iteration:    42, batch:   6, Loss: 0.06911457, Accuracy: 95.95%\n",
      "Iteration:    42, batch:   7, Loss: 0.06903854, Accuracy: 95.97%\n",
      "Iteration:    42, batch:   8, Loss: 0.06900905, Accuracy: 95.94%\n",
      "Iteration:    42, batch:   9, Loss: 0.06892035, Accuracy: 95.96%\n",
      "Iteration:    43, batch:   0, Loss: 0.06895637, Accuracy: 95.96%\n",
      "Iteration:    43, batch:   1, Loss: 0.06888456, Accuracy: 95.99%\n",
      "Iteration:    43, batch:   2, Loss: 0.06884197, Accuracy: 95.96%\n",
      "Iteration:    43, batch:   3, Loss: 0.06872155, Accuracy: 95.88%\n",
      "Iteration:    43, batch:   4, Loss: 0.06866657, Accuracy: 95.90%\n",
      "Iteration:    43, batch:   5, Loss: 0.06858686, Accuracy: 95.93%\n",
      "Iteration:    43, batch:   6, Loss: 0.06852825, Accuracy: 95.96%\n",
      "Iteration:    43, batch:   7, Loss: 0.06849767, Accuracy: 95.96%\n",
      "Iteration:    43, batch:   8, Loss: 0.06845366, Accuracy: 95.96%\n",
      "Iteration:    43, batch:   9, Loss: 0.06836339, Accuracy: 95.95%\n",
      "Iteration:    44, batch:   0, Loss: 0.06837794, Accuracy: 95.97%\n",
      "Iteration:    44, batch:   1, Loss: 0.06833385, Accuracy: 95.99%\n",
      "Iteration:    44, batch:   2, Loss: 0.06828358, Accuracy: 95.94%\n",
      "Iteration:    44, batch:   3, Loss: 0.06820358, Accuracy: 95.91%\n",
      "Iteration:    44, batch:   4, Loss: 0.06812857, Accuracy: 95.91%\n",
      "Iteration:    44, batch:   5, Loss: 0.06805456, Accuracy: 95.93%\n",
      "Iteration:    44, batch:   6, Loss: 0.06798653, Accuracy: 95.94%\n",
      "Iteration:    44, batch:   7, Loss: 0.06793582, Accuracy: 95.95%\n",
      "Iteration:    44, batch:   8, Loss: 0.06795584, Accuracy: 95.99%\n",
      "Iteration:    44, batch:   9, Loss: 0.06784750, Accuracy: 95.97%\n",
      "Iteration:    45, batch:   0, Loss: 0.06783500, Accuracy: 95.96%\n",
      "Iteration:    45, batch:   1, Loss: 0.06779284, Accuracy: 95.98%\n",
      "Iteration:    45, batch:   2, Loss: 0.06778802, Accuracy: 95.97%\n",
      "Iteration:    45, batch:   3, Loss: 0.06771470, Accuracy: 95.93%\n",
      "Iteration:    45, batch:   4, Loss: 0.06759644, Accuracy: 95.91%\n",
      "Iteration:    45, batch:   5, Loss: 0.06752072, Accuracy: 95.92%\n",
      "Iteration:    45, batch:   6, Loss: 0.06749651, Accuracy: 95.94%\n",
      "Iteration:    45, batch:   7, Loss: 0.06742206, Accuracy: 95.97%\n",
      "Iteration:    45, batch:   8, Loss: 0.06743859, Accuracy: 95.97%\n",
      "Iteration:    45, batch:   9, Loss: 0.06734995, Accuracy: 95.99%\n",
      "Iteration:    46, batch:   0, Loss: 0.06731393, Accuracy: 95.94%\n",
      "Iteration:    46, batch:   1, Loss: 0.06727999, Accuracy: 95.97%\n",
      "Iteration:    46, batch:   2, Loss: 0.06729415, Accuracy: 95.99%\n",
      "Iteration:    46, batch:   3, Loss: 0.06721176, Accuracy: 95.93%\n",
      "Iteration:    46, batch:   4, Loss: 0.06710335, Accuracy: 95.91%\n",
      "Iteration:    46, batch:   5, Loss: 0.06705493, Accuracy: 95.90%\n",
      "Iteration:    46, batch:   6, Loss: 0.06697694, Accuracy: 95.94%\n",
      "Iteration:    46, batch:   7, Loss: 0.06694398, Accuracy: 95.98%\n",
      "Iteration:    46, batch:   8, Loss: 0.06696142, Accuracy: 95.97%\n",
      "Iteration:    46, batch:   9, Loss: 0.06683389, Accuracy: 95.98%\n",
      "Iteration:    47, batch:   0, Loss: 0.06685617, Accuracy: 95.96%\n",
      "Iteration:    47, batch:   1, Loss: 0.06682372, Accuracy: 95.99%\n",
      "Iteration:    47, batch:   2, Loss: 0.06677068, Accuracy: 96.01%\n",
      "Iteration:    47, batch:   3, Loss: 0.06668575, Accuracy: 95.95%\n",
      "Iteration:    47, batch:   4, Loss: 0.06659049, Accuracy: 95.91%\n",
      "Iteration:    47, batch:   5, Loss: 0.06658020, Accuracy: 95.91%\n",
      "Iteration:    47, batch:   6, Loss: 0.06652686, Accuracy: 95.92%\n",
      "Iteration:    47, batch:   7, Loss: 0.06650390, Accuracy: 95.97%\n",
      "Iteration:    47, batch:   8, Loss: 0.06645568, Accuracy: 95.96%\n",
      "Iteration:    47, batch:   9, Loss: 0.06636615, Accuracy: 96.00%\n",
      "Iteration:    48, batch:   0, Loss: 0.06634076, Accuracy: 95.97%\n",
      "Iteration:    48, batch:   1, Loss: 0.06630072, Accuracy: 96.00%\n",
      "Iteration:    48, batch:   2, Loss: 0.06629509, Accuracy: 96.00%\n",
      "Iteration:    48, batch:   3, Loss: 0.06626288, Accuracy: 95.95%\n",
      "Iteration:    48, batch:   4, Loss: 0.06614862, Accuracy: 95.92%\n",
      "Iteration:    48, batch:   5, Loss: 0.06611102, Accuracy: 95.91%\n",
      "Iteration:    48, batch:   6, Loss: 0.06602753, Accuracy: 95.93%\n",
      "Iteration:    48, batch:   7, Loss: 0.06601021, Accuracy: 95.98%\n",
      "Iteration:    48, batch:   8, Loss: 0.06606149, Accuracy: 95.99%\n",
      "Iteration:    48, batch:   9, Loss: 0.06588725, Accuracy: 96.01%\n",
      "Iteration:    49, batch:   0, Loss: 0.06586441, Accuracy: 95.97%\n",
      "Iteration:    49, batch:   1, Loss: 0.06585157, Accuracy: 96.00%\n",
      "Iteration:    49, batch:   2, Loss: 0.06581180, Accuracy: 95.99%\n",
      "Iteration:    49, batch:   3, Loss: 0.06575340, Accuracy: 95.99%\n",
      "Iteration:    49, batch:   4, Loss: 0.06564356, Accuracy: 95.94%\n",
      "Iteration:    49, batch:   5, Loss: 0.06569350, Accuracy: 95.92%\n",
      "Iteration:    49, batch:   6, Loss: 0.06560438, Accuracy: 95.93%\n",
      "Iteration:    49, batch:   7, Loss: 0.06559276, Accuracy: 96.00%\n",
      "Iteration:    49, batch:   8, Loss: 0.06554474, Accuracy: 96.00%\n",
      "Iteration:    49, batch:   9, Loss: 0.06545618, Accuracy: 96.03%\n",
      "Iteration:    50, batch:   0, Loss: 0.06542740, Accuracy: 96.00%\n",
      "Iteration:    50, batch:   1, Loss: 0.06540647, Accuracy: 96.03%\n",
      "Iteration:    50, batch:   2, Loss: 0.06539030, Accuracy: 95.99%\n",
      "Iteration:    50, batch:   3, Loss: 0.06532029, Accuracy: 95.98%\n",
      "Iteration:    50, batch:   4, Loss: 0.06523724, Accuracy: 95.93%\n",
      "Iteration:    50, batch:   5, Loss: 0.06527476, Accuracy: 95.93%\n",
      "Iteration:    50, batch:   6, Loss: 0.06524320, Accuracy: 95.95%\n",
      "Iteration:    50, batch:   7, Loss: 0.06513062, Accuracy: 95.97%\n",
      "Iteration:    50, batch:   8, Loss: 0.06512087, Accuracy: 95.99%\n",
      "Iteration:    50, batch:   9, Loss: 0.06498489, Accuracy: 96.03%\n",
      "Iteration:    51, batch:   0, Loss: 0.06502013, Accuracy: 96.00%\n",
      "Iteration:    51, batch:   1, Loss: 0.06499096, Accuracy: 96.01%\n",
      "Iteration:    51, batch:   2, Loss: 0.06493998, Accuracy: 96.00%\n",
      "Iteration:    51, batch:   3, Loss: 0.06490318, Accuracy: 95.97%\n",
      "Iteration:    51, batch:   4, Loss: 0.06477719, Accuracy: 95.96%\n",
      "Iteration:    51, batch:   5, Loss: 0.06485906, Accuracy: 95.96%\n",
      "Iteration:    51, batch:   6, Loss: 0.06478854, Accuracy: 95.95%\n",
      "Iteration:    51, batch:   7, Loss: 0.06477062, Accuracy: 95.97%\n",
      "Iteration:    51, batch:   8, Loss: 0.06477111, Accuracy: 95.98%\n",
      "Iteration:    51, batch:   9, Loss: 0.06461812, Accuracy: 96.03%\n",
      "Iteration:    52, batch:   0, Loss: 0.06462890, Accuracy: 96.01%\n",
      "Iteration:    52, batch:   1, Loss: 0.06459720, Accuracy: 96.00%\n",
      "Iteration:    52, batch:   2, Loss: 0.06455153, Accuracy: 96.01%\n",
      "Iteration:    52, batch:   3, Loss: 0.06445356, Accuracy: 95.97%\n",
      "Iteration:    52, batch:   4, Loss: 0.06437115, Accuracy: 95.95%\n",
      "Iteration:    52, batch:   5, Loss: 0.06437710, Accuracy: 95.98%\n",
      "Iteration:    52, batch:   6, Loss: 0.06442289, Accuracy: 95.96%\n",
      "Iteration:    52, batch:   7, Loss: 0.06432652, Accuracy: 95.96%\n",
      "Iteration:    52, batch:   8, Loss: 0.06434874, Accuracy: 96.00%\n",
      "Iteration:    52, batch:   9, Loss: 0.06421117, Accuracy: 96.05%\n",
      "Iteration:    53, batch:   0, Loss: 0.06425083, Accuracy: 95.99%\n",
      "Iteration:    53, batch:   1, Loss: 0.06421760, Accuracy: 96.02%\n",
      "Iteration:    53, batch:   2, Loss: 0.06414859, Accuracy: 95.98%\n",
      "Iteration:    53, batch:   3, Loss: 0.06406230, Accuracy: 95.98%\n",
      "Iteration:    53, batch:   4, Loss: 0.06401013, Accuracy: 95.98%\n",
      "Iteration:    53, batch:   5, Loss: 0.06393437, Accuracy: 95.99%\n",
      "Iteration:    53, batch:   6, Loss: 0.06400801, Accuracy: 95.95%\n",
      "Iteration:    53, batch:   7, Loss: 0.06393301, Accuracy: 95.97%\n",
      "Iteration:    53, batch:   8, Loss: 0.06397083, Accuracy: 96.03%\n",
      "Iteration:    53, batch:   9, Loss: 0.06382460, Accuracy: 96.07%\n",
      "Iteration:    54, batch:   0, Loss: 0.06383910, Accuracy: 96.00%\n",
      "Iteration:    54, batch:   1, Loss: 0.06379491, Accuracy: 96.03%\n",
      "Iteration:    54, batch:   2, Loss: 0.06374820, Accuracy: 96.00%\n",
      "Iteration:    54, batch:   3, Loss: 0.06364323, Accuracy: 95.98%\n",
      "Iteration:    54, batch:   4, Loss: 0.06358537, Accuracy: 95.97%\n",
      "Iteration:    54, batch:   5, Loss: 0.06354342, Accuracy: 95.95%\n",
      "Iteration:    54, batch:   6, Loss: 0.06362351, Accuracy: 95.96%\n",
      "Iteration:    54, batch:   7, Loss: 0.06353220, Accuracy: 95.96%\n",
      "Iteration:    54, batch:   8, Loss: 0.06356157, Accuracy: 95.99%\n",
      "Iteration:    54, batch:   9, Loss: 0.06346242, Accuracy: 96.06%\n",
      "Iteration:    55, batch:   0, Loss: 0.06346329, Accuracy: 95.98%\n",
      "Iteration:    55, batch:   1, Loss: 0.06340716, Accuracy: 96.02%\n",
      "Iteration:    55, batch:   2, Loss: 0.06336689, Accuracy: 96.01%\n",
      "Iteration:    55, batch:   3, Loss: 0.06326674, Accuracy: 96.02%\n",
      "Iteration:    55, batch:   4, Loss: 0.06319440, Accuracy: 95.99%\n",
      "Iteration:    55, batch:   5, Loss: 0.06315756, Accuracy: 95.95%\n",
      "Iteration:    55, batch:   6, Loss: 0.06323428, Accuracy: 95.94%\n",
      "Iteration:    55, batch:   7, Loss: 0.06314220, Accuracy: 95.97%\n",
      "Iteration:    55, batch:   8, Loss: 0.06318706, Accuracy: 95.99%\n",
      "Iteration:    55, batch:   9, Loss: 0.06309134, Accuracy: 96.07%\n",
      "Iteration:    56, batch:   0, Loss: 0.06306358, Accuracy: 96.00%\n",
      "Iteration:    56, batch:   1, Loss: 0.06305038, Accuracy: 96.03%\n",
      "Iteration:    56, batch:   2, Loss: 0.06298985, Accuracy: 96.03%\n",
      "Iteration:    56, batch:   3, Loss: 0.06290347, Accuracy: 96.01%\n",
      "Iteration:    56, batch:   4, Loss: 0.06282192, Accuracy: 95.98%\n",
      "Iteration:    56, batch:   5, Loss: 0.06279720, Accuracy: 95.96%\n",
      "Iteration:    56, batch:   6, Loss: 0.06285159, Accuracy: 95.95%\n",
      "Iteration:    56, batch:   7, Loss: 0.06276390, Accuracy: 95.98%\n",
      "Iteration:    56, batch:   8, Loss: 0.06277987, Accuracy: 96.00%\n",
      "Iteration:    56, batch:   9, Loss: 0.06272256, Accuracy: 96.07%\n",
      "Iteration:    57, batch:   0, Loss: 0.06268641, Accuracy: 96.01%\n",
      "Iteration:    57, batch:   1, Loss: 0.06270484, Accuracy: 96.01%\n",
      "Iteration:    57, batch:   2, Loss: 0.06263058, Accuracy: 96.03%\n",
      "Iteration:    57, batch:   3, Loss: 0.06255836, Accuracy: 96.03%\n",
      "Iteration:    57, batch:   4, Loss: 0.06244214, Accuracy: 96.01%\n",
      "Iteration:    57, batch:   5, Loss: 0.06243044, Accuracy: 95.97%\n",
      "Iteration:    57, batch:   6, Loss: 0.06247754, Accuracy: 95.95%\n",
      "Iteration:    57, batch:   7, Loss: 0.06239308, Accuracy: 95.99%\n",
      "Iteration:    57, batch:   8, Loss: 0.06242124, Accuracy: 96.00%\n",
      "Iteration:    57, batch:   9, Loss: 0.06234958, Accuracy: 96.03%\n",
      "Iteration:    58, batch:   0, Loss: 0.06231803, Accuracy: 96.02%\n",
      "Iteration:    58, batch:   1, Loss: 0.06235536, Accuracy: 96.01%\n",
      "Iteration:    58, batch:   2, Loss: 0.06227729, Accuracy: 96.05%\n",
      "Iteration:    58, batch:   3, Loss: 0.06219328, Accuracy: 96.00%\n",
      "Iteration:    58, batch:   4, Loss: 0.06212158, Accuracy: 95.99%\n",
      "Iteration:    58, batch:   5, Loss: 0.06208085, Accuracy: 95.98%\n",
      "Iteration:    58, batch:   6, Loss: 0.06212563, Accuracy: 95.95%\n",
      "Iteration:    58, batch:   7, Loss: 0.06203135, Accuracy: 95.99%\n",
      "Iteration:    58, batch:   8, Loss: 0.06202054, Accuracy: 96.00%\n",
      "Iteration:    58, batch:   9, Loss: 0.06200491, Accuracy: 96.02%\n",
      "Iteration:    59, batch:   0, Loss: 0.06196697, Accuracy: 96.02%\n",
      "Iteration:    59, batch:   1, Loss: 0.06199906, Accuracy: 96.03%\n",
      "Iteration:    59, batch:   2, Loss: 0.06191667, Accuracy: 96.03%\n",
      "Iteration:    59, batch:   3, Loss: 0.06187045, Accuracy: 96.01%\n",
      "Iteration:    59, batch:   4, Loss: 0.06174874, Accuracy: 95.99%\n",
      "Iteration:    59, batch:   5, Loss: 0.06172074, Accuracy: 95.96%\n",
      "Iteration:    59, batch:   6, Loss: 0.06175957, Accuracy: 95.96%\n",
      "Iteration:    59, batch:   7, Loss: 0.06169022, Accuracy: 96.01%\n",
      "Iteration:    59, batch:   8, Loss: 0.06166330, Accuracy: 96.01%\n",
      "Iteration:    59, batch:   9, Loss: 0.06165519, Accuracy: 96.02%\n"
     ]
    }
   ],
   "source": [
    "n_train = 60000 # number of examples in training data (max is 60,000)\n",
    "n_test  = 10000 # number of examples in training data (max is 10,000)\n",
    "n_nodes = 800 # number of hidden nodes\n",
    "n_iters = 60 # number of iterations\n",
    "lr      = 0.4\n",
    "batchsize = 6000\n",
    "\n",
    "# create a string with the parameter values\n",
    "lr_str = str(lr).replace('.', 'p')\n",
    "if 0: # no minibatches\n",
    "    name = 'success_rates_train{}_test{}_nodes{}_iters{}_lr{}'.format(n_train, n_test, n_nodes, n_iters, lr_str) \n",
    "    (w1, w2, sr, ls, ts) = train(X_train[:n_train], Y_train[:n_train], \n",
    "                   X_test[:n_test], Y_test[:n_test], \n",
    "                   n_hidden_nodes=n_nodes, iterations=n_iters, lr=lr)\n",
    "    success_rates[name] = sr # store results\n",
    "    times[name] = ts\n",
    "\n",
    "if 1: # with minibatches\n",
    "    name_minibatch = 'success_rates_train{}_test{}_nodes{}_iters{}_lr{}_batchsize{}'.format(n_train, n_test, n_nodes, n_iters, lr_str, batchsize) \n",
    "    (w1, w2, sr, ls, ts, sr_batches, ls_batches, ts_batches) = train_minibatch(X_train[:n_train], Y_train[:n_train], \n",
    "                   X_test[:n_test], Y_test[:n_test], \n",
    "                   n_hidden_nodes=n_nodes, iterations=n_iters, lr=lr, batchsize = batchsize)\n",
    "    success_rates[name_minibatch] = sr # store results\n",
    "    times[name_minibatch] = ts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66758399",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "c296e070",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'success_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 7\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Plot results as a function of ITERATIONS\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;66;03m# here you can plot several training curves alongside each other \u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# uncomment, delete, edit, copy-paste lines correspondingly\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# edit the NAME of the results to be plotted AND edit the LABEL, such that the legend is correct\u001b[39;00m\n\u001b[1;32m      6\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 7\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43msuccess_rates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuccess_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \n\u001b[1;32m      8\u001b[0m                label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rates_train60000_test10000_nodes800_iters40_lr0.4_batchsize6000\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124miterations\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'success_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot results as a function of ITERATIONS\n",
    "#\n",
    "# here you can plot several training curves alongside each other \n",
    "# uncomment, delete, edit, copy-paste lines correspondingly\n",
    "# edit the NAME of the results to be plotted AND edit the LABEL, such that the legend is correct\n",
    "plt.figure(figsize = (7,5))\n",
    "plt.plot(success_rates['success_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000'], \n",
    "               label = 'success_rates_train60000_test10000_nodes800_iters40_lr0.4_batchsize6000')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('success rate in %')\n",
    "plt.grid()\n",
    "#plt.ylim([60,95]) # uncomment line to restrict the y-range for more detailed view on the late training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "0166fa66",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'success_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[47], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# plot results as a function of TIME\u001b[39;00m\n\u001b[1;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[0;32m----> 4\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(        \u001b[43mtimes\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuccess_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, \n\u001b[1;32m      5\u001b[0m          success_rates[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000\u001b[39m\u001b[38;5;124m'\u001b[39m], \n\u001b[1;32m      6\u001b[0m                label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rates_train60000_test10000_nodes800_iters40_lr0.4_batchsize6000\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n\u001b[1;32m      8\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtime in seconds\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'success_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results as a function of TIME\n",
    "\n",
    "plt.figure(figsize = (7,5))\n",
    "plt.plot(        times['success_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000'], \n",
    "         success_rates['success_rates_train60000_test10000_nodes800_iters40_lr0p4_batchsize6000'], \n",
    "               label = 'success_rates_train60000_test10000_nodes800_iters40_lr0.4_batchsize6000')\n",
    "plt.legend()\n",
    "plt.xlabel('time in seconds')\n",
    "plt.ylabel('success rate in %')\n",
    "plt.grid()\n",
    "#plt.ylim([80,95]) # uncomment line to restrict the y-range for more detailed view on the late training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f15a76c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9e3fa7b3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8df4e06",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2da615f9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e687bd82",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
