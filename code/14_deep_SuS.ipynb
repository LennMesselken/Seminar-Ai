{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "19b72192-9c89-4a3f-bb97-45f583ae5f2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6f72ad3f-81bb-4aba-ba88-105ec8c23102",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a NumPy array of bytes:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the pixels into a matrix where each line is an image:\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "\n",
    "# 60000 images, each 784 elements (28 * 28 pixels)\n",
    "X_train = load_images(\"../data/mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 784 elements, with the same structure as X_train\n",
    "X_test = load_images(\"../data/mnist/t10k-images-idx3-ubyte.gz\")\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    n_labels = Y.shape[0]\n",
    "    n_classes = 10\n",
    "    encoded_Y = np.zeros((n_labels, n_classes))\n",
    "    for i in range(n_labels):\n",
    "        label = Y[i]\n",
    "        encoded_Y[i][label] = 1\n",
    "    return encoded_Y\n",
    "\n",
    "\n",
    "# !!! EDIT PATHS TO WHERE YOUR MNIST DATA IS !!!\n",
    "\n",
    "# 60K labels, each a single digit from 0 to 9\n",
    "Y_train_unencoded = load_labels(\"../data/mnist/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# 60K labels, each consisting of 10 one-hot encoded elements\n",
    "Y_train = one_hot_encode(Y_train_unencoded)\n",
    "\n",
    "# 10000 labels, each a single digit from 0 to 9\n",
    "Y_test = load_labels(\"../data/mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0daf7e2e-32f3-4779-92d6-dc309bd86d79",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network implementation (almost the same as backpropagation.py,\n",
    "# except for a tiny refactoring in the back() function).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exponentials = np.exp(logits)\n",
    "    return exponentials / np.sum(exponentials, axis=1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def sigmoid_gradient(sigmoid):\n",
    "    return np.multiply(sigmoid, (1 - sigmoid))\n",
    "\n",
    "\n",
    "def loss(Y, y_hat):\n",
    "    return -np.sum(Y * np.log(y_hat)) / Y.shape[0]\n",
    "\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "def forward(X, w1, w2):\n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))\n",
    "    return (y_hat, h)\n",
    "\n",
    "\n",
    "def back(X, Y, y_hat, w2, h):\n",
    "    w2_gradient = np.matmul(prepend_bias(h).T, (y_hat - Y)) / X.shape[0]\n",
    "    w1_gradient = np.matmul(prepend_bias(X).T, np.matmul(y_hat - Y, w2[1:].T) \n",
    "                            * sigmoid_gradient(h)) / X.shape[0]\n",
    "    return (w1_gradient, w2_gradient)\n",
    "\n",
    "\n",
    "def classify(X, w1, w2):\n",
    "    y_hat, _ = forward(X, w1, w2)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def initialize_weights(n_input_variables, n_hidden_nodes, n_classes):\n",
    "    w1_rows = n_input_variables + 1\n",
    "    w1 = np.random.randn(w1_rows, n_hidden_nodes) * np.sqrt(1 / w1_rows)\n",
    "\n",
    "    w2_rows = n_hidden_nodes + 1\n",
    "    w2 = np.random.randn(w2_rows, n_classes) * np.sqrt(1 / w2_rows)\n",
    "\n",
    "    return (w1, w2)\n",
    "\n",
    "\n",
    "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat, _ = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(\"Iteration: %5d, Loss: %.8f, Accuracy: %.2f%%\" %\n",
    "          (iteration, training_loss, accuracy))\n",
    "    return accuracy, training_loss\n",
    "    \n",
    "\n",
    "def train(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr):\n",
    "    start_time = time.time()\n",
    "    success_rates = []\n",
    "    losses = []\n",
    "    times = []\n",
    "    n_input_variables = X_train.shape[1]\n",
    "    n_classes = Y_train.shape[1]\n",
    "    w1, w2 = initialize_weights(n_input_variables, n_hidden_nodes, n_classes)\n",
    "    for iteration in range(iterations):\n",
    "        y_hat, h = forward(X_train, w1, w2)\n",
    "        w1_gradient, w2_gradient = back(X_train, Y_train, y_hat, w2, h)\n",
    "        w1 = w1 - (w1_gradient * lr)\n",
    "        w2 = w2 - (w2_gradient * lr)\n",
    "        accuracy, training_loss = report(iteration, X_train, Y_train, X_test, Y_test, w1, w2)\n",
    "        success_rates.append(accuracy)\n",
    "        losses.append(training_loss)\n",
    "        times.append(time.time()-start_time)\n",
    "    return (w1, w2, success_rates, losses, times)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd30600b-06c5-46f1-b5df-16c4cd6aca3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# three layers\n",
    "\n",
    "def forward3(X, W1, W2, W3):\n",
    "    H1 = sigmoid(np.matmul(prepend_bias(X), W1))\n",
    "    H2 = sigmoid(np.matmul(prepend_bias(H1), W2))\n",
    "    Y_hat = softmax(np.matmul(prepend_bias(H2), W3))\n",
    "    return (Y_hat, H2, H1)\n",
    "\n",
    "def back3(X, Y, Y_hat, W2, W3, H1, H2):\n",
    "    W3_gradient = np.matmul(prepend_bias(H2).T, (Y_hat - Y)) / X.shape[0]\n",
    "    W2_gradient = np.matmul(prepend_bias(H1).T, np.matmul( Y_hat - Y, W3[1:].T) \n",
    "                            * sigmoid_gradient(H2))  / X.shape[0]\n",
    "    W1_gradient = np.matmul(prepend_bias( X).T, np.matmul( np.matmul( Y_hat - Y, W3[1:].T) \n",
    "                            * sigmoid_gradient(H2) , W2[1:].T) * sigmoid_gradient(H1)) / X.shape[0]\n",
    "    return (W1_gradient, W2_gradient, W3_gradient)\n",
    "\n",
    "def back4(X,Y, Y_hat, W2, W3, W4, H1, H2, H3):\n",
    "    W4_gradient = np.matmul(prepend_bias(H3).T, (Y_hat - Y)) / X.shape[0]\n",
    "    W3_gradient = np.matmul(prepend_bias(H2).T, np.matmul( Y_hat - Y, W4[1:].T) * sigmoid_gradient(H3)) / X.shape[0]\n",
    "    W2_gradient = np.matmul(prepend_bias(H1).T, np.matmul( np.matmul(Y_hat - Y, W4[1:].T) * sigmoid_gradient(H3), W3[1:].T) * sigmoid_gradient(H2)) / X.shape[0]\n",
    "    W1_gradient = np.matmul(prepend_bias(X ).T, np.matmul(np.matmul( np.matmul(Y_hat - Y, W4[1:].T) * sigmoid_gradient(H3), W3[1:].T) * sigmoid_gradient(H2), W2[1:].T) * sigmoid_gradient(H1)) / X.shape[0]\n",
    "\n",
    "    return (W1_gradient, W2_gradient, W3_gradient, W4_gradient)\n",
    "\n",
    "def classify3(X, W1, W2, W3):\n",
    "    y_hat,_,_ = forward3(X, W1, W2, W3)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "def initialize_weights3(n_input_variables, h1, h2, n_classes):\n",
    "    w1_rows = n_input_variables + 1\n",
    "    w1 = np.random.randn(w1_rows, h1) * np.sqrt(1 / w1_rows)\n",
    "\n",
    "    w2_rows = h1 + 1\n",
    "    w2 = np.random.randn(w2_rows, h2) * np.sqrt(1 / w2_rows)\n",
    "\n",
    "    w3_rows = h2 + 1\n",
    "    w3 = np.random.randn(w3_rows, n_classes) * np.sqrt(1 / w2_rows)\n",
    "    return (w1, w2, w3)\n",
    "\n",
    "def report3(iteration, X_train, Y_train, X_test, Y_test, w1, w2, w3):\n",
    "    y_hat,_,_ = forward3(X_train, w1, w2, w3)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify3(X_test, w1, w2, w3)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(\"Iteration: %5d, Loss: %.8f, Accuracy: %.2f%%\" %\n",
    "          (iteration, training_loss, accuracy))\n",
    "    return accuracy, training_loss\n",
    "\n",
    "def train3(X_train, Y_train, X_test, Y_test, h1, h2, iterations, lr):\n",
    "    start_time = time.time()\n",
    "    success_rates = []\n",
    "    losses = []\n",
    "    times = []\n",
    "    n_input_variables = X_train.shape[1]\n",
    "    n_classes = Y_train.shape[1]\n",
    "    w1, w2, w3 = initialize_weights3(n_input_variables, h1, h2, n_classes)\n",
    "    for iteration in range(iterations):\n",
    "        if iteration > 100:\n",
    "            lr = 0.1\n",
    "        if iteration > 350:\n",
    "            lr = 0.05\n",
    "        if iteration > 450:\n",
    "            lr = 0.01\n",
    "        if iteration > 600:\n",
    "            lr = 0.005\n",
    "        y_hat, H2, H1 = forward3(X_train, w1, w2, w3)\n",
    "        w1_gradient, w2_gradient, w3_gradient = back3(X_train, Y_train, y_hat, w2, w3, H1, H2)\n",
    "        w1 = w1 - (w1_gradient * lr)\n",
    "        w2 = w2 - (w2_gradient * lr)\n",
    "        w3 = w3 - (w3_gradient * lr)\n",
    "        accuracy, training_loss = report3(iteration, X_train, Y_train, X_test, Y_test, w1, w2, w3)\n",
    "        success_rates.append(accuracy)\n",
    "        losses.append(training_loss)\n",
    "        times.append(time.time()-start_time)\n",
    "    return (w1, w2, w3, success_rates, losses, times)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2dcf59f2-0e3c-422a-b0a9-1db918f94cc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store your success rates\n",
    "# !!! UNCOMMENT, EXECUTE, AND COMMENT AGAIN !!!\n",
    "# !!! DON'T EXECUTE THE LINES AGAIN AFTER STARTING TO COLLECT RESULTS !!!\n",
    "\n",
    "#success_rates = {}\n",
    "#losses = {}\n",
    "#times = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73fa9340-a7e0-4b2c-a152-8188004a0864",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "235288ef-756a-41a4-9d49-228d041f5c7a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:     0, Loss: 2.78170302, Accuracy: 21.77%\n",
      "Iteration:     1, Loss: 3.31751605, Accuracy: 20.57%\n",
      "Iteration:     2, Loss: 2.68783274, Accuracy: 24.85%\n",
      "Iteration:     3, Loss: 2.32030455, Accuracy: 25.96%\n",
      "Iteration:     4, Loss: 2.08383853, Accuracy: 44.88%\n",
      "Iteration:     5, Loss: 1.97834737, Accuracy: 62.53%\n",
      "Iteration:     6, Loss: 1.87332374, Accuracy: 62.67%\n",
      "Iteration:     7, Loss: 1.75819867, Accuracy: 69.49%\n",
      "Iteration:     8, Loss: 1.63797930, Accuracy: 68.52%\n",
      "Iteration:     9, Loss: 1.52727260, Accuracy: 73.65%\n",
      "Iteration:    10, Loss: 1.46452695, Accuracy: 63.35%\n",
      "Iteration:    11, Loss: 1.67828505, Accuracy: 47.99%\n",
      "Iteration:    12, Loss: 2.07434565, Accuracy: 29.28%\n",
      "Iteration:    13, Loss: 2.40835329, Accuracy: 37.85%\n",
      "Iteration:    14, Loss: 1.74480955, Accuracy: 35.17%\n",
      "Iteration:    15, Loss: 1.65252501, Accuracy: 47.06%\n",
      "Iteration:    16, Loss: 1.52282739, Accuracy: 51.26%\n",
      "Iteration:    17, Loss: 1.33977248, Accuracy: 60.49%\n",
      "Iteration:    18, Loss: 1.10087424, Accuracy: 72.37%\n",
      "Iteration:    19, Loss: 1.00986023, Accuracy: 78.62%\n",
      "Iteration:    20, Loss: 1.00367793, Accuracy: 69.63%\n",
      "Iteration:    21, Loss: 1.16696397, Accuracy: 64.58%\n",
      "Iteration:    22, Loss: 1.36333625, Accuracy: 56.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/r7d1zvcs4g5_lpdwx6rf5tgr0000gn/T/ipykernel_37381/2475075527.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:    23, Loss: 1.35661146, Accuracy: 59.29%\n",
      "Iteration:    24, Loss: 0.92285149, Accuracy: 70.96%\n",
      "Iteration:    25, Loss: 0.82407980, Accuracy: 74.34%\n",
      "Iteration:    26, Loss: 0.82597593, Accuracy: 74.29%\n",
      "Iteration:    27, Loss: 1.00720571, Accuracy: 62.67%\n",
      "Iteration:    28, Loss: 1.34173794, Accuracy: 53.79%\n",
      "Iteration:    29, Loss: 1.19201207, Accuracy: 53.07%\n",
      "Iteration:    30, Loss: 1.00452457, Accuracy: 63.00%\n",
      "Iteration:    31, Loss: 0.75535074, Accuracy: 78.51%\n",
      "Iteration:    32, Loss: 0.64852971, Accuracy: 83.56%\n",
      "Iteration:    33, Loss: 0.60187544, Accuracy: 84.98%\n",
      "Iteration:    34, Loss: 0.58339010, Accuracy: 84.68%\n",
      "Iteration:    35, Loss: 0.57729171, Accuracy: 84.68%\n",
      "Iteration:    36, Loss: 0.58239090, Accuracy: 83.08%\n",
      "Iteration:    37, Loss: 0.62376248, Accuracy: 81.05%\n",
      "Iteration:    38, Loss: 0.63142874, Accuracy: 77.87%\n",
      "Iteration:    39, Loss: 0.67974933, Accuracy: 77.53%\n",
      "Iteration:    40, Loss: 0.64678212, Accuracy: 76.60%\n",
      "Iteration:    41, Loss: 0.60707116, Accuracy: 78.96%\n",
      "Iteration:    42, Loss: 0.58121487, Accuracy: 79.33%\n",
      "Iteration:    43, Loss: 0.57135958, Accuracy: 79.81%\n",
      "Iteration:    44, Loss: 0.55425727, Accuracy: 80.78%\n",
      "Iteration:    45, Loss: 0.56341233, Accuracy: 80.89%\n",
      "Iteration:    46, Loss: 0.52908359, Accuracy: 81.80%\n",
      "Iteration:    47, Loss: 0.53823531, Accuracy: 81.87%\n",
      "Iteration:    48, Loss: 0.50426620, Accuracy: 82.50%\n",
      "Iteration:    49, Loss: 0.51261703, Accuracy: 82.75%\n",
      "Iteration:    50, Loss: 0.49372185, Accuracy: 83.13%\n",
      "Iteration:    51, Loss: 0.49287572, Accuracy: 84.03%\n",
      "Iteration:    52, Loss: 0.48203965, Accuracy: 84.10%\n",
      "Iteration:    53, Loss: 0.46200218, Accuracy: 85.91%\n",
      "Iteration:    54, Loss: 0.45219986, Accuracy: 86.21%\n",
      "Iteration:    55, Loss: 0.42990166, Accuracy: 87.84%\n",
      "Iteration:    56, Loss: 0.43128916, Accuracy: 87.25%\n",
      "Iteration:    57, Loss: 0.43729002, Accuracy: 87.74%\n",
      "Iteration:    58, Loss: 0.46780676, Accuracy: 86.03%\n",
      "Iteration:    59, Loss: 0.52786984, Accuracy: 84.74%\n",
      "Iteration:    60, Loss: 0.58539418, Accuracy: 82.47%\n",
      "Iteration:    61, Loss: 0.57375868, Accuracy: 82.75%\n",
      "Iteration:    62, Loss: 0.50934523, Accuracy: 84.33%\n",
      "Iteration:    63, Loss: 0.44054777, Accuracy: 86.68%\n",
      "Iteration:    64, Loss: 0.39615748, Accuracy: 87.94%\n",
      "Iteration:    65, Loss: 0.37108062, Accuracy: 88.81%\n",
      "Iteration:    66, Loss: 0.35324225, Accuracy: 89.69%\n",
      "Iteration:    67, Loss: 0.34100999, Accuracy: 90.30%\n",
      "Iteration:    68, Loss: 0.33062887, Accuracy: 90.69%\n",
      "Iteration:    69, Loss: 0.32502210, Accuracy: 90.68%\n",
      "Iteration:    70, Loss: 0.32069232, Accuracy: 91.01%\n",
      "Iteration:    71, Loss: 0.31783212, Accuracy: 90.91%\n",
      "Iteration:    72, Loss: 0.32012568, Accuracy: 91.00%\n",
      "Iteration:    73, Loss: 0.32259899, Accuracy: 90.69%\n",
      "Iteration:    74, Loss: 0.32980791, Accuracy: 90.50%\n",
      "Iteration:    75, Loss: 0.33645724, Accuracy: 89.94%\n",
      "Iteration:    76, Loss: 0.35381007, Accuracy: 89.44%\n",
      "Iteration:    77, Loss: 0.37035554, Accuracy: 88.64%\n",
      "Iteration:    78, Loss: 0.39931491, Accuracy: 87.36%\n",
      "Iteration:    79, Loss: 0.40839811, Accuracy: 87.15%\n",
      "Iteration:    80, Loss: 0.41005459, Accuracy: 86.98%\n",
      "Iteration:    81, Loss: 0.38774271, Accuracy: 87.91%\n",
      "Iteration:    82, Loss: 0.35265124, Accuracy: 89.24%\n",
      "Iteration:    83, Loss: 0.31974347, Accuracy: 90.57%\n",
      "Iteration:    84, Loss: 0.29182982, Accuracy: 91.67%\n",
      "Iteration:    85, Loss: 0.27996758, Accuracy: 91.87%\n",
      "Iteration:    86, Loss: 0.27190796, Accuracy: 92.06%\n",
      "Iteration:    87, Loss: 0.26766754, Accuracy: 92.21%\n",
      "Iteration:    88, Loss: 0.26434718, Accuracy: 92.19%\n",
      "Iteration:    89, Loss: 0.26258056, Accuracy: 92.30%\n",
      "Iteration:    90, Loss: 0.26144882, Accuracy: 92.32%\n",
      "Iteration:    91, Loss: 0.26037636, Accuracy: 92.28%\n",
      "Iteration:    92, Loss: 0.25901839, Accuracy: 92.29%\n",
      "Iteration:    93, Loss: 0.25916095, Accuracy: 92.32%\n",
      "Iteration:    94, Loss: 0.25808531, Accuracy: 92.29%\n",
      "Iteration:    95, Loss: 0.25775102, Accuracy: 92.30%\n",
      "Iteration:    96, Loss: 0.25754243, Accuracy: 92.19%\n",
      "Iteration:    97, Loss: 0.25892048, Accuracy: 92.16%\n",
      "Iteration:    98, Loss: 0.25720979, Accuracy: 92.38%\n",
      "Iteration:    99, Loss: 0.25973236, Accuracy: 92.12%\n",
      "Iteration:   100, Loss: 0.25872130, Accuracy: 92.26%\n",
      "Iteration:   101, Loss: 0.26057863, Accuracy: 92.10%\n",
      "Iteration:   102, Loss: 0.26064239, Accuracy: 92.05%\n",
      "Iteration:   103, Loss: 0.25937948, Accuracy: 92.12%\n",
      "Iteration:   104, Loss: 0.25896898, Accuracy: 92.04%\n",
      "Iteration:   105, Loss: 0.25805125, Accuracy: 92.26%\n",
      "Iteration:   106, Loss: 0.25573699, Accuracy: 92.13%\n",
      "Iteration:   107, Loss: 0.25191896, Accuracy: 92.20%\n",
      "Iteration:   108, Loss: 0.25026878, Accuracy: 92.16%\n",
      "Iteration:   109, Loss: 0.24697276, Accuracy: 92.41%\n",
      "Iteration:   110, Loss: 0.24421637, Accuracy: 92.38%\n",
      "Iteration:   111, Loss: 0.24218803, Accuracy: 92.51%\n",
      "Iteration:   112, Loss: 0.24243612, Accuracy: 92.50%\n",
      "Iteration:   113, Loss: 0.24145595, Accuracy: 92.37%\n",
      "Iteration:   114, Loss: 0.24238166, Accuracy: 92.40%\n",
      "Iteration:   115, Loss: 0.24264037, Accuracy: 92.34%\n",
      "Iteration:   116, Loss: 0.24606114, Accuracy: 92.21%\n",
      "Iteration:   117, Loss: 0.24608370, Accuracy: 92.13%\n",
      "Iteration:   118, Loss: 0.24941025, Accuracy: 91.94%\n",
      "Iteration:   119, Loss: 0.24794416, Accuracy: 92.07%\n",
      "Iteration:   120, Loss: 0.25091868, Accuracy: 91.95%\n",
      "Iteration:   121, Loss: 0.24533496, Accuracy: 92.22%\n",
      "Iteration:   122, Loss: 0.24622358, Accuracy: 92.00%\n",
      "Iteration:   123, Loss: 0.23498106, Accuracy: 92.45%\n",
      "Iteration:   124, Loss: 0.23040816, Accuracy: 92.67%\n",
      "Iteration:   125, Loss: 0.22213085, Accuracy: 93.03%\n",
      "Iteration:   126, Loss: 0.21703436, Accuracy: 93.06%\n",
      "Iteration:   127, Loss: 0.21245539, Accuracy: 93.32%\n",
      "Iteration:   128, Loss: 0.20929909, Accuracy: 93.31%\n",
      "Iteration:   129, Loss: 0.20782072, Accuracy: 93.51%\n",
      "Iteration:   130, Loss: 0.20699252, Accuracy: 93.41%\n",
      "Iteration:   131, Loss: 0.20555016, Accuracy: 93.52%\n",
      "Iteration:   132, Loss: 0.20407191, Accuracy: 93.48%\n",
      "Iteration:   133, Loss: 0.20377942, Accuracy: 93.57%\n",
      "Iteration:   134, Loss: 0.20368509, Accuracy: 93.47%\n",
      "Iteration:   135, Loss: 0.20437392, Accuracy: 93.45%\n",
      "Iteration:   136, Loss: 0.20414405, Accuracy: 93.50%\n",
      "Iteration:   137, Loss: 0.20650096, Accuracy: 93.21%\n",
      "Iteration:   138, Loss: 0.20795035, Accuracy: 93.22%\n",
      "Iteration:   139, Loss: 0.20928151, Accuracy: 92.99%\n",
      "Iteration:   140, Loss: 0.21104132, Accuracy: 93.35%\n",
      "Iteration:   141, Loss: 0.21349317, Accuracy: 92.89%\n",
      "Iteration:   142, Loss: 0.21437382, Accuracy: 93.17%\n",
      "Iteration:   143, Loss: 0.21425672, Accuracy: 92.94%\n",
      "Iteration:   144, Loss: 0.21470486, Accuracy: 93.04%\n",
      "Iteration:   145, Loss: 0.21228154, Accuracy: 93.00%\n",
      "Iteration:   146, Loss: 0.20929309, Accuracy: 93.17%\n",
      "Iteration:   147, Loss: 0.20797676, Accuracy: 93.13%\n",
      "Iteration:   148, Loss: 0.20518418, Accuracy: 93.27%\n",
      "Iteration:   149, Loss: 0.20133032, Accuracy: 93.47%\n",
      "Iteration:   150, Loss: 0.19770987, Accuracy: 93.51%\n",
      "Iteration:   151, Loss: 0.19438965, Accuracy: 93.73%\n",
      "Iteration:   152, Loss: 0.19206308, Accuracy: 93.66%\n",
      "Iteration:   153, Loss: 0.18906119, Accuracy: 93.90%\n",
      "Iteration:   154, Loss: 0.18681921, Accuracy: 93.82%\n",
      "Iteration:   155, Loss: 0.18448021, Accuracy: 93.99%\n",
      "Iteration:   156, Loss: 0.18309657, Accuracy: 93.91%\n",
      "Iteration:   157, Loss: 0.18318323, Accuracy: 93.94%\n",
      "Iteration:   158, Loss: 0.18434906, Accuracy: 93.85%\n",
      "Iteration:   159, Loss: 0.18587963, Accuracy: 93.91%\n",
      "Iteration:   160, Loss: 0.18603376, Accuracy: 93.79%\n",
      "Iteration:   161, Loss: 0.18672865, Accuracy: 93.73%\n",
      "Iteration:   162, Loss: 0.18822012, Accuracy: 93.78%\n",
      "Iteration:   163, Loss: 0.19261797, Accuracy: 93.64%\n",
      "Iteration:   164, Loss: 0.19771345, Accuracy: 93.47%\n",
      "Iteration:   165, Loss: 0.20286058, Accuracy: 93.14%\n",
      "Iteration:   166, Loss: 0.21083027, Accuracy: 93.11%\n",
      "Iteration:   167, Loss: 0.22253815, Accuracy: 92.64%\n",
      "Iteration:   168, Loss: 0.24173781, Accuracy: 92.04%\n",
      "Iteration:   169, Loss: 0.26036027, Accuracy: 91.52%\n",
      "Iteration:   170, Loss: 0.31959213, Accuracy: 89.76%\n",
      "Iteration:   171, Loss: 0.37993930, Accuracy: 87.09%\n",
      "Iteration:   172, Loss: 0.61812592, Accuracy: 83.91%\n",
      "Iteration:   173, Loss: 0.63549395, Accuracy: 81.77%\n",
      "Iteration:   174, Loss: 0.63891083, Accuracy: 82.25%\n",
      "Iteration:   175, Loss: 0.33135985, Accuracy: 89.54%\n",
      "Iteration:   176, Loss: 0.28681801, Accuracy: 90.79%\n",
      "Iteration:   177, Loss: 0.29700880, Accuracy: 90.74%\n",
      "Iteration:   178, Loss: 0.27199388, Accuracy: 91.50%\n",
      "Iteration:   179, Loss: 0.23464970, Accuracy: 92.58%\n",
      "Iteration:   180, Loss: 0.20261372, Accuracy: 93.56%\n",
      "Iteration:   181, Loss: 0.18755190, Accuracy: 93.96%\n",
      "Iteration:   182, Loss: 0.18029745, Accuracy: 94.29%\n",
      "Iteration:   183, Loss: 0.17497529, Accuracy: 94.26%\n",
      "Iteration:   184, Loss: 0.17241454, Accuracy: 94.37%\n",
      "Iteration:   185, Loss: 0.16981145, Accuracy: 94.43%\n",
      "Iteration:   186, Loss: 0.16778311, Accuracy: 94.37%\n",
      "Iteration:   187, Loss: 0.16664790, Accuracy: 94.43%\n",
      "Iteration:   188, Loss: 0.16617520, Accuracy: 94.37%\n",
      "Iteration:   189, Loss: 0.16633880, Accuracy: 94.46%\n",
      "Iteration:   190, Loss: 0.16591083, Accuracy: 94.46%\n",
      "Iteration:   191, Loss: 0.16559422, Accuracy: 94.41%\n",
      "Iteration:   192, Loss: 0.16518673, Accuracy: 94.36%\n",
      "Iteration:   193, Loss: 0.16510581, Accuracy: 94.38%\n",
      "Iteration:   194, Loss: 0.16556087, Accuracy: 94.40%\n",
      "Iteration:   195, Loss: 0.16605452, Accuracy: 94.34%\n",
      "Iteration:   196, Loss: 0.16728987, Accuracy: 94.32%\n",
      "Iteration:   197, Loss: 0.16720821, Accuracy: 94.30%\n",
      "Iteration:   198, Loss: 0.16859282, Accuracy: 94.21%\n",
      "Iteration:   199, Loss: 0.16869805, Accuracy: 94.18%\n",
      "Iteration:   200, Loss: 0.16979507, Accuracy: 94.07%\n",
      "Iteration:   201, Loss: 0.16885079, Accuracy: 94.25%\n",
      "Iteration:   202, Loss: 0.16774595, Accuracy: 94.18%\n",
      "Iteration:   203, Loss: 0.16584703, Accuracy: 94.24%\n",
      "Iteration:   204, Loss: 0.16280798, Accuracy: 94.28%\n",
      "Iteration:   205, Loss: 0.15870856, Accuracy: 94.52%\n",
      "Iteration:   206, Loss: 0.15534458, Accuracy: 94.42%\n",
      "Iteration:   207, Loss: 0.15199395, Accuracy: 94.67%\n",
      "Iteration:   208, Loss: 0.15042869, Accuracy: 94.73%\n",
      "Iteration:   209, Loss: 0.14863582, Accuracy: 94.78%\n",
      "Iteration:   210, Loss: 0.14734464, Accuracy: 94.63%\n",
      "Iteration:   211, Loss: 0.14661323, Accuracy: 94.85%\n",
      "Iteration:   212, Loss: 0.14649121, Accuracy: 94.79%\n",
      "Iteration:   213, Loss: 0.14585937, Accuracy: 94.83%\n",
      "Iteration:   214, Loss: 0.14643673, Accuracy: 94.65%\n",
      "Iteration:   215, Loss: 0.14701186, Accuracy: 94.74%\n",
      "Iteration:   216, Loss: 0.14776689, Accuracy: 94.70%\n",
      "Iteration:   217, Loss: 0.14845901, Accuracy: 94.65%\n",
      "Iteration:   218, Loss: 0.14822156, Accuracy: 94.65%\n",
      "Iteration:   219, Loss: 0.14830070, Accuracy: 94.64%\n",
      "Iteration:   220, Loss: 0.14810422, Accuracy: 94.70%\n",
      "Iteration:   221, Loss: 0.14848533, Accuracy: 94.66%\n",
      "Iteration:   222, Loss: 0.14899793, Accuracy: 94.59%\n",
      "Iteration:   223, Loss: 0.14966323, Accuracy: 94.48%\n",
      "Iteration:   224, Loss: 0.15021611, Accuracy: 94.50%\n",
      "Iteration:   225, Loss: 0.14943907, Accuracy: 94.66%\n",
      "Iteration:   226, Loss: 0.14893309, Accuracy: 94.55%\n",
      "Iteration:   227, Loss: 0.14721299, Accuracy: 94.62%\n",
      "Iteration:   228, Loss: 0.14720839, Accuracy: 94.49%\n",
      "Iteration:   229, Loss: 0.14581768, Accuracy: 94.64%\n",
      "Iteration:   230, Loss: 0.14435324, Accuracy: 94.65%\n",
      "Iteration:   231, Loss: 0.14252703, Accuracy: 94.71%\n",
      "Iteration:   232, Loss: 0.14045925, Accuracy: 94.84%\n",
      "Iteration:   233, Loss: 0.13880693, Accuracy: 94.91%\n",
      "Iteration:   234, Loss: 0.13671582, Accuracy: 94.91%\n",
      "Iteration:   235, Loss: 0.13512064, Accuracy: 94.98%\n",
      "Iteration:   236, Loss: 0.13368125, Accuracy: 95.01%\n",
      "Iteration:   237, Loss: 0.13318481, Accuracy: 95.06%\n",
      "Iteration:   238, Loss: 0.13233775, Accuracy: 94.92%\n",
      "Iteration:   239, Loss: 0.13257314, Accuracy: 95.03%\n",
      "Iteration:   240, Loss: 0.13278960, Accuracy: 95.03%\n",
      "Iteration:   241, Loss: 0.13377352, Accuracy: 95.04%\n",
      "Iteration:   242, Loss: 0.13304313, Accuracy: 94.80%\n",
      "Iteration:   243, Loss: 0.13464750, Accuracy: 94.96%\n",
      "Iteration:   244, Loss: 0.13597258, Accuracy: 94.77%\n",
      "Iteration:   245, Loss: 0.13584985, Accuracy: 95.03%\n",
      "Iteration:   246, Loss: 0.13611557, Accuracy: 94.72%\n",
      "Iteration:   247, Loss: 0.13739484, Accuracy: 94.96%\n",
      "Iteration:   248, Loss: 0.13926343, Accuracy: 94.68%\n",
      "Iteration:   249, Loss: 0.14168780, Accuracy: 94.77%\n",
      "Iteration:   250, Loss: 0.14369909, Accuracy: 94.56%\n",
      "Iteration:   251, Loss: 0.14483276, Accuracy: 94.67%\n",
      "Iteration:   252, Loss: 0.14753166, Accuracy: 94.35%\n",
      "Iteration:   253, Loss: 0.14722743, Accuracy: 94.62%\n",
      "Iteration:   254, Loss: 0.14849489, Accuracy: 94.33%\n",
      "Iteration:   255, Loss: 0.14900263, Accuracy: 94.51%\n",
      "Iteration:   256, Loss: 0.14952903, Accuracy: 94.36%\n",
      "Iteration:   257, Loss: 0.14720777, Accuracy: 94.46%\n",
      "Iteration:   258, Loss: 0.14755513, Accuracy: 94.31%\n",
      "Iteration:   259, Loss: 0.14567766, Accuracy: 94.58%\n",
      "Iteration:   260, Loss: 0.14395225, Accuracy: 94.48%\n",
      "Iteration:   261, Loss: 0.14157262, Accuracy: 94.61%\n",
      "Iteration:   262, Loss: 0.14072319, Accuracy: 94.61%\n",
      "Iteration:   263, Loss: 0.13634738, Accuracy: 94.84%\n",
      "Iteration:   264, Loss: 0.13432732, Accuracy: 94.84%\n",
      "Iteration:   265, Loss: 0.12914621, Accuracy: 94.99%\n",
      "Iteration:   266, Loss: 0.12670888, Accuracy: 95.11%\n",
      "Iteration:   267, Loss: 0.12390123, Accuracy: 95.09%\n",
      "Iteration:   268, Loss: 0.12179672, Accuracy: 95.22%\n",
      "Iteration:   269, Loss: 0.12100700, Accuracy: 95.28%\n",
      "Iteration:   270, Loss: 0.12139376, Accuracy: 95.18%\n",
      "Iteration:   271, Loss: 0.12185337, Accuracy: 95.20%\n",
      "Iteration:   272, Loss: 0.12274693, Accuracy: 95.08%\n",
      "Iteration:   273, Loss: 0.12364614, Accuracy: 95.19%\n",
      "Iteration:   274, Loss: 0.12532652, Accuracy: 95.02%\n",
      "Iteration:   275, Loss: 0.12807374, Accuracy: 94.96%\n",
      "Iteration:   276, Loss: 0.13049553, Accuracy: 94.80%\n",
      "Iteration:   277, Loss: 0.13370780, Accuracy: 94.69%\n",
      "Iteration:   278, Loss: 0.13494689, Accuracy: 94.70%\n",
      "Iteration:   279, Loss: 0.13683679, Accuracy: 94.57%\n",
      "Iteration:   280, Loss: 0.13626743, Accuracy: 94.69%\n",
      "Iteration:   281, Loss: 0.13613664, Accuracy: 94.62%\n",
      "Iteration:   282, Loss: 0.13560345, Accuracy: 94.73%\n",
      "Iteration:   283, Loss: 0.13354084, Accuracy: 94.70%\n",
      "Iteration:   284, Loss: 0.12784661, Accuracy: 95.00%\n",
      "Iteration:   285, Loss: 0.12359405, Accuracy: 95.14%\n",
      "Iteration:   286, Loss: 0.11907435, Accuracy: 95.07%\n",
      "Iteration:   287, Loss: 0.11536180, Accuracy: 95.39%\n",
      "Iteration:   288, Loss: 0.11382368, Accuracy: 95.37%\n",
      "Iteration:   289, Loss: 0.11280546, Accuracy: 95.37%\n",
      "Iteration:   290, Loss: 0.11152387, Accuracy: 95.36%\n",
      "Iteration:   291, Loss: 0.11104775, Accuracy: 95.46%\n",
      "Iteration:   292, Loss: 0.10965305, Accuracy: 95.39%\n",
      "Iteration:   293, Loss: 0.10902599, Accuracy: 95.46%\n",
      "Iteration:   294, Loss: 0.10876912, Accuracy: 95.45%\n",
      "Iteration:   295, Loss: 0.10861384, Accuracy: 95.54%\n",
      "Iteration:   296, Loss: 0.10765271, Accuracy: 95.50%\n",
      "Iteration:   297, Loss: 0.10748583, Accuracy: 95.45%\n",
      "Iteration:   298, Loss: 0.10745620, Accuracy: 95.43%\n",
      "Iteration:   299, Loss: 0.10756026, Accuracy: 95.41%\n",
      "Iteration:   300, Loss: 0.10804797, Accuracy: 95.37%\n",
      "Iteration:   301, Loss: 0.10887562, Accuracy: 95.44%\n",
      "Iteration:   302, Loss: 0.11195898, Accuracy: 95.24%\n",
      "Iteration:   303, Loss: 0.11523196, Accuracy: 95.31%\n",
      "Iteration:   304, Loss: 0.11672686, Accuracy: 95.16%\n",
      "Iteration:   305, Loss: 0.11977411, Accuracy: 95.19%\n",
      "Iteration:   306, Loss: 0.12275516, Accuracy: 95.06%\n",
      "Iteration:   307, Loss: 0.12726114, Accuracy: 94.91%\n",
      "Iteration:   308, Loss: 0.12880329, Accuracy: 94.84%\n",
      "Iteration:   309, Loss: 0.12935186, Accuracy: 94.91%\n",
      "Iteration:   310, Loss: 0.12854921, Accuracy: 94.83%\n",
      "Iteration:   311, Loss: 0.12850318, Accuracy: 94.99%\n",
      "Iteration:   312, Loss: 0.12673027, Accuracy: 94.91%\n",
      "Iteration:   313, Loss: 0.12612200, Accuracy: 94.87%\n",
      "Iteration:   314, Loss: 0.12329633, Accuracy: 95.06%\n",
      "Iteration:   315, Loss: 0.12254919, Accuracy: 95.09%\n",
      "Iteration:   316, Loss: 0.12138009, Accuracy: 95.10%\n",
      "Iteration:   317, Loss: 0.12026699, Accuracy: 95.15%\n",
      "Iteration:   318, Loss: 0.12158427, Accuracy: 94.99%\n",
      "Iteration:   319, Loss: 0.12180219, Accuracy: 94.97%\n",
      "Iteration:   320, Loss: 0.12247287, Accuracy: 95.01%\n",
      "Iteration:   321, Loss: 0.12201414, Accuracy: 94.92%\n",
      "Iteration:   322, Loss: 0.12383972, Accuracy: 94.97%\n",
      "Iteration:   323, Loss: 0.12245581, Accuracy: 95.09%\n",
      "Iteration:   324, Loss: 0.12336485, Accuracy: 95.02%\n",
      "Iteration:   325, Loss: 0.11975049, Accuracy: 95.13%\n",
      "Iteration:   326, Loss: 0.11822917, Accuracy: 95.14%\n",
      "Iteration:   327, Loss: 0.11127423, Accuracy: 95.26%\n",
      "Iteration:   328, Loss: 0.10742140, Accuracy: 95.35%\n",
      "Iteration:   329, Loss: 0.10259065, Accuracy: 95.46%\n",
      "Iteration:   330, Loss: 0.10069645, Accuracy: 95.62%\n",
      "Iteration:   331, Loss: 0.09855758, Accuracy: 95.58%\n",
      "Iteration:   332, Loss: 0.09745072, Accuracy: 95.59%\n",
      "Iteration:   333, Loss: 0.09660840, Accuracy: 95.61%\n",
      "Iteration:   334, Loss: 0.09634361, Accuracy: 95.73%\n",
      "Iteration:   335, Loss: 0.09543698, Accuracy: 95.60%\n",
      "Iteration:   336, Loss: 0.09546912, Accuracy: 95.55%\n",
      "Iteration:   337, Loss: 0.09538102, Accuracy: 95.58%\n",
      "Iteration:   338, Loss: 0.09508006, Accuracy: 95.62%\n",
      "Iteration:   339, Loss: 0.09528570, Accuracy: 95.61%\n",
      "Iteration:   340, Loss: 0.09499973, Accuracy: 95.53%\n",
      "Iteration:   341, Loss: 0.09548526, Accuracy: 95.58%\n",
      "Iteration:   342, Loss: 0.09606773, Accuracy: 95.45%\n",
      "Iteration:   343, Loss: 0.09786753, Accuracy: 95.50%\n",
      "Iteration:   344, Loss: 0.10012895, Accuracy: 95.29%\n",
      "Iteration:   345, Loss: 0.10212089, Accuracy: 95.54%\n",
      "Iteration:   346, Loss: 0.10571752, Accuracy: 95.24%\n",
      "Iteration:   347, Loss: 0.10939731, Accuracy: 95.28%\n",
      "Iteration:   348, Loss: 0.11481727, Accuracy: 94.91%\n",
      "Iteration:   349, Loss: 0.12172664, Accuracy: 95.02%\n",
      "Iteration:   350, Loss: 0.12894664, Accuracy: 94.50%\n",
      "Iteration:   351, Loss: 0.13039126, Accuracy: 94.73%\n",
      "Iteration:   352, Loss: 0.13966420, Accuracy: 94.18%\n",
      "Iteration:   353, Loss: 0.13867100, Accuracy: 94.56%\n",
      "Iteration:   354, Loss: 0.14894531, Accuracy: 93.83%\n",
      "Iteration:   355, Loss: 0.14472274, Accuracy: 94.24%\n",
      "Iteration:   356, Loss: 0.15103546, Accuracy: 93.88%\n",
      "Iteration:   357, Loss: 0.13590184, Accuracy: 94.72%\n",
      "Iteration:   358, Loss: 0.13024871, Accuracy: 94.58%\n",
      "Iteration:   359, Loss: 0.11160702, Accuracy: 95.17%\n",
      "Iteration:   360, Loss: 0.10096796, Accuracy: 95.50%\n",
      "Iteration:   361, Loss: 0.09315492, Accuracy: 95.60%\n",
      "Iteration:   362, Loss: 0.09000679, Accuracy: 95.80%\n",
      "Iteration:   363, Loss: 0.08877739, Accuracy: 95.73%\n",
      "Iteration:   364, Loss: 0.08816240, Accuracy: 95.74%\n",
      "Iteration:   365, Loss: 0.08790528, Accuracy: 95.72%\n",
      "Iteration:   366, Loss: 0.08754916, Accuracy: 95.77%\n",
      "Iteration:   367, Loss: 0.08719946, Accuracy: 95.73%\n",
      "Iteration:   368, Loss: 0.08745772, Accuracy: 95.73%\n",
      "Iteration:   369, Loss: 0.08749550, Accuracy: 95.69%\n",
      "Iteration:   370, Loss: 0.08737600, Accuracy: 95.73%\n",
      "Iteration:   371, Loss: 0.08813590, Accuracy: 95.78%\n",
      "Iteration:   372, Loss: 0.08953933, Accuracy: 95.71%\n",
      "Iteration:   373, Loss: 0.09183954, Accuracy: 95.60%\n",
      "Iteration:   374, Loss: 0.09430308, Accuracy: 95.62%\n",
      "Iteration:   375, Loss: 0.09681265, Accuracy: 95.50%\n",
      "Iteration:   376, Loss: 0.10000708, Accuracy: 95.56%\n",
      "Iteration:   377, Loss: 0.10533286, Accuracy: 95.33%\n",
      "Iteration:   378, Loss: 0.11268253, Accuracy: 95.12%\n",
      "Iteration:   379, Loss: 0.12192085, Accuracy: 94.82%\n",
      "Iteration:   380, Loss: 0.13535040, Accuracy: 94.49%\n",
      "Iteration:   381, Loss: 0.16758177, Accuracy: 93.37%\n",
      "Iteration:   382, Loss: 0.27431392, Accuracy: 90.37%\n",
      "Iteration:   383, Loss: 0.67441011, Accuracy: 85.31%\n",
      "Iteration:   384, Loss: 0.83745250, Accuracy: 82.74%\n",
      "Iteration:   385, Loss: 0.69572447, Accuracy: 82.19%\n",
      "Iteration:   386, Loss: 0.44709662, Accuracy: 84.56%\n",
      "Iteration:   387, Loss: 0.48179176, Accuracy: 83.76%\n",
      "Iteration:   388, Loss: 0.59820279, Accuracy: 81.11%\n",
      "Iteration:   389, Loss: 1.05474692, Accuracy: 73.92%\n",
      "Iteration:   390, Loss: 1.50838099, Accuracy: 70.22%\n",
      "Iteration:   391, Loss: 1.15351908, Accuracy: 74.84%\n",
      "Iteration:   392, Loss: 1.11568806, Accuracy: 70.26%\n",
      "Iteration:   393, Loss: 1.49602587, Accuracy: 72.80%\n",
      "Iteration:   394, Loss: 0.64917446, Accuracy: 78.93%\n",
      "Iteration:   395, Loss: 0.61873123, Accuracy: 81.11%\n",
      "Iteration:   396, Loss: 0.46382297, Accuracy: 85.96%\n",
      "Iteration:   397, Loss: 0.24863490, Accuracy: 92.74%\n",
      "Iteration:   398, Loss: 0.18505261, Accuracy: 94.14%\n",
      "Iteration:   399, Loss: 0.16830608, Accuracy: 94.69%\n",
      "Iteration:   400, Loss: 0.16114147, Accuracy: 94.88%\n",
      "Iteration:   401, Loss: 0.15622216, Accuracy: 94.91%\n",
      "Iteration:   402, Loss: 0.15211610, Accuracy: 94.92%\n",
      "Iteration:   403, Loss: 0.14919172, Accuracy: 94.90%\n",
      "Iteration:   404, Loss: 0.14682281, Accuracy: 94.97%\n",
      "Iteration:   405, Loss: 0.14408465, Accuracy: 94.96%\n",
      "Iteration:   406, Loss: 0.14164527, Accuracy: 95.04%\n",
      "Iteration:   407, Loss: 0.13971124, Accuracy: 95.14%\n",
      "Iteration:   408, Loss: 0.13797825, Accuracy: 95.04%\n",
      "Iteration:   409, Loss: 0.13658020, Accuracy: 95.11%\n",
      "Iteration:   410, Loss: 0.13515185, Accuracy: 95.11%\n",
      "Iteration:   411, Loss: 0.13327983, Accuracy: 95.17%\n",
      "Iteration:   412, Loss: 0.13169902, Accuracy: 95.18%\n",
      "Iteration:   413, Loss: 0.13047217, Accuracy: 95.22%\n",
      "Iteration:   414, Loss: 0.12937100, Accuracy: 95.24%\n",
      "Iteration:   415, Loss: 0.12833835, Accuracy: 95.34%\n",
      "Iteration:   416, Loss: 0.12671968, Accuracy: 95.26%\n",
      "Iteration:   417, Loss: 0.12559337, Accuracy: 95.28%\n",
      "Iteration:   418, Loss: 0.12453546, Accuracy: 95.24%\n",
      "Iteration:   419, Loss: 0.12425640, Accuracy: 95.29%\n",
      "Iteration:   420, Loss: 0.12294646, Accuracy: 95.28%\n",
      "Iteration:   421, Loss: 0.12214221, Accuracy: 95.37%\n",
      "Iteration:   422, Loss: 0.12116701, Accuracy: 95.35%\n",
      "Iteration:   423, Loss: 0.11931223, Accuracy: 95.40%\n",
      "Iteration:   424, Loss: 0.11799562, Accuracy: 95.31%\n",
      "Iteration:   425, Loss: 0.11719935, Accuracy: 95.35%\n",
      "Iteration:   426, Loss: 0.11634881, Accuracy: 95.35%\n",
      "Iteration:   427, Loss: 0.11563579, Accuracy: 95.47%\n",
      "Iteration:   428, Loss: 0.11486750, Accuracy: 95.37%\n",
      "Iteration:   429, Loss: 0.11428850, Accuracy: 95.46%\n",
      "Iteration:   430, Loss: 0.11368948, Accuracy: 95.44%\n",
      "Iteration:   431, Loss: 0.11374290, Accuracy: 95.39%\n",
      "Iteration:   432, Loss: 0.11344942, Accuracy: 95.39%\n",
      "Iteration:   433, Loss: 0.11364344, Accuracy: 95.44%\n",
      "Iteration:   434, Loss: 0.11193226, Accuracy: 95.45%\n",
      "Iteration:   435, Loss: 0.11165434, Accuracy: 95.41%\n",
      "Iteration:   436, Loss: 0.11099705, Accuracy: 95.45%\n",
      "Iteration:   437, Loss: 0.10955080, Accuracy: 95.36%\n",
      "Iteration:   438, Loss: 0.10892024, Accuracy: 95.52%\n",
      "Iteration:   439, Loss: 0.10825108, Accuracy: 95.41%\n",
      "Iteration:   440, Loss: 0.10706875, Accuracy: 95.50%\n",
      "Iteration:   441, Loss: 0.10731077, Accuracy: 95.45%\n",
      "Iteration:   442, Loss: 0.10680149, Accuracy: 95.52%\n",
      "Iteration:   443, Loss: 0.10701652, Accuracy: 95.44%\n",
      "Iteration:   444, Loss: 0.10642230, Accuracy: 95.51%\n",
      "Iteration:   445, Loss: 0.10599603, Accuracy: 95.37%\n",
      "Iteration:   446, Loss: 0.10569818, Accuracy: 95.50%\n",
      "Iteration:   447, Loss: 0.10468497, Accuracy: 95.44%\n",
      "Iteration:   448, Loss: 0.10432832, Accuracy: 95.60%\n",
      "Iteration:   449, Loss: 0.10438979, Accuracy: 95.48%\n",
      "Iteration:   450, Loss: 0.10348711, Accuracy: 95.51%\n",
      "Iteration:   451, Loss: 0.10286638, Accuracy: 95.50%\n",
      "Iteration:   452, Loss: 0.10184564, Accuracy: 95.53%\n",
      "Iteration:   453, Loss: 0.10135803, Accuracy: 95.43%\n",
      "Iteration:   454, Loss: 0.10098536, Accuracy: 95.62%\n",
      "Iteration:   455, Loss: 0.10078308, Accuracy: 95.48%\n",
      "Iteration:   456, Loss: 0.10066413, Accuracy: 95.54%\n",
      "Iteration:   457, Loss: 0.10033792, Accuracy: 95.51%\n",
      "Iteration:   458, Loss: 0.09954691, Accuracy: 95.52%\n",
      "Iteration:   459, Loss: 0.09948877, Accuracy: 95.50%\n",
      "Iteration:   460, Loss: 0.09969506, Accuracy: 95.56%\n",
      "Iteration:   461, Loss: 0.09970384, Accuracy: 95.57%\n",
      "Iteration:   462, Loss: 0.09915301, Accuracy: 95.56%\n",
      "Iteration:   463, Loss: 0.09898033, Accuracy: 95.50%\n",
      "Iteration:   464, Loss: 0.09780135, Accuracy: 95.63%\n",
      "Iteration:   465, Loss: 0.09690399, Accuracy: 95.56%\n",
      "Iteration:   466, Loss: 0.09590957, Accuracy: 95.62%\n",
      "Iteration:   467, Loss: 0.09538811, Accuracy: 95.56%\n",
      "Iteration:   468, Loss: 0.09480403, Accuracy: 95.62%\n",
      "Iteration:   469, Loss: 0.09446685, Accuracy: 95.62%\n",
      "Iteration:   470, Loss: 0.09439150, Accuracy: 95.60%\n",
      "Iteration:   471, Loss: 0.09394621, Accuracy: 95.60%\n",
      "Iteration:   472, Loss: 0.09354777, Accuracy: 95.64%\n",
      "Iteration:   473, Loss: 0.09359095, Accuracy: 95.61%\n",
      "Iteration:   474, Loss: 0.09372346, Accuracy: 95.59%\n",
      "Iteration:   475, Loss: 0.09359982, Accuracy: 95.59%\n",
      "Iteration:   476, Loss: 0.09318412, Accuracy: 95.61%\n",
      "Iteration:   477, Loss: 0.09310159, Accuracy: 95.55%\n",
      "Iteration:   478, Loss: 0.09327661, Accuracy: 95.63%\n",
      "Iteration:   479, Loss: 0.09346438, Accuracy: 95.53%\n",
      "Iteration:   480, Loss: 0.09270509, Accuracy: 95.66%\n",
      "Iteration:   481, Loss: 0.09283746, Accuracy: 95.53%\n",
      "Iteration:   482, Loss: 0.09282023, Accuracy: 95.64%\n",
      "Iteration:   483, Loss: 0.09196568, Accuracy: 95.66%\n",
      "Iteration:   484, Loss: 0.09140358, Accuracy: 95.57%\n",
      "Iteration:   485, Loss: 0.09176308, Accuracy: 95.60%\n",
      "Iteration:   486, Loss: 0.09187121, Accuracy: 95.68%\n",
      "Iteration:   487, Loss: 0.09177590, Accuracy: 95.66%\n",
      "Iteration:   488, Loss: 0.08971130, Accuracy: 95.68%\n",
      "Iteration:   489, Loss: 0.08949696, Accuracy: 95.65%\n",
      "Iteration:   490, Loss: 0.08908635, Accuracy: 95.71%\n",
      "Iteration:   491, Loss: 0.08872301, Accuracy: 95.66%\n",
      "Iteration:   492, Loss: 0.08853017, Accuracy: 95.67%\n",
      "Iteration:   493, Loss: 0.08721136, Accuracy: 95.66%\n",
      "Iteration:   494, Loss: 0.08743165, Accuracy: 95.64%\n",
      "Iteration:   495, Loss: 0.08657977, Accuracy: 95.68%\n",
      "Iteration:   496, Loss: 0.08616251, Accuracy: 95.78%\n",
      "Iteration:   497, Loss: 0.08588872, Accuracy: 95.62%\n",
      "Iteration:   498, Loss: 0.08583072, Accuracy: 95.81%\n",
      "Iteration:   499, Loss: 0.08600696, Accuracy: 95.68%\n",
      "Iteration:   500, Loss: 0.08614599, Accuracy: 95.61%\n",
      "Iteration:   501, Loss: 0.08541761, Accuracy: 95.69%\n",
      "Iteration:   502, Loss: 0.08532080, Accuracy: 95.70%\n",
      "Iteration:   503, Loss: 0.08667902, Accuracy: 95.72%\n",
      "Iteration:   504, Loss: 0.08713211, Accuracy: 95.68%\n",
      "Iteration:   505, Loss: 0.08858515, Accuracy: 95.65%\n",
      "Iteration:   506, Loss: 0.08918619, Accuracy: 95.65%\n",
      "Iteration:   507, Loss: 0.09057748, Accuracy: 95.50%\n",
      "Iteration:   508, Loss: 0.09117011, Accuracy: 95.63%\n",
      "Iteration:   509, Loss: 0.09091928, Accuracy: 95.68%\n",
      "Iteration:   510, Loss: 0.08999400, Accuracy: 95.61%\n",
      "Iteration:   511, Loss: 0.08930617, Accuracy: 95.65%\n",
      "Iteration:   512, Loss: 0.08793964, Accuracy: 95.71%\n",
      "Iteration:   513, Loss: 0.08670674, Accuracy: 95.67%\n",
      "Iteration:   514, Loss: 0.08392013, Accuracy: 95.69%\n",
      "Iteration:   515, Loss: 0.08247455, Accuracy: 95.66%\n",
      "Iteration:   516, Loss: 0.08087573, Accuracy: 95.76%\n",
      "Iteration:   517, Loss: 0.08041257, Accuracy: 95.74%\n",
      "Iteration:   518, Loss: 0.07976502, Accuracy: 95.75%\n",
      "Iteration:   519, Loss: 0.07966486, Accuracy: 95.67%\n",
      "Iteration:   520, Loss: 0.07914208, Accuracy: 95.76%\n",
      "Iteration:   521, Loss: 0.07909798, Accuracy: 95.75%\n",
      "Iteration:   522, Loss: 0.07897733, Accuracy: 95.79%\n",
      "Iteration:   523, Loss: 0.07970768, Accuracy: 95.72%\n",
      "Iteration:   524, Loss: 0.07964671, Accuracy: 95.73%\n",
      "Iteration:   525, Loss: 0.08018527, Accuracy: 95.78%\n",
      "Iteration:   526, Loss: 0.08057231, Accuracy: 95.75%\n",
      "Iteration:   527, Loss: 0.08070892, Accuracy: 95.78%\n",
      "Iteration:   528, Loss: 0.07992724, Accuracy: 95.78%\n",
      "Iteration:   529, Loss: 0.08010804, Accuracy: 95.74%\n",
      "Iteration:   530, Loss: 0.08025978, Accuracy: 95.69%\n",
      "Iteration:   531, Loss: 0.08070642, Accuracy: 95.64%\n",
      "Iteration:   532, Loss: 0.07986443, Accuracy: 95.77%\n",
      "Iteration:   533, Loss: 0.07975821, Accuracy: 95.69%\n",
      "Iteration:   534, Loss: 0.07950565, Accuracy: 95.67%\n",
      "Iteration:   535, Loss: 0.08035972, Accuracy: 95.81%\n",
      "Iteration:   536, Loss: 0.07906986, Accuracy: 95.77%\n",
      "Iteration:   537, Loss: 0.07924910, Accuracy: 95.75%\n",
      "Iteration:   538, Loss: 0.07890123, Accuracy: 95.82%\n",
      "Iteration:   539, Loss: 0.07993804, Accuracy: 95.76%\n",
      "Iteration:   540, Loss: 0.07925836, Accuracy: 95.83%\n",
      "Iteration:   541, Loss: 0.07907439, Accuracy: 95.72%\n",
      "Iteration:   542, Loss: 0.07841656, Accuracy: 95.79%\n",
      "Iteration:   543, Loss: 0.07780964, Accuracy: 95.75%\n",
      "Iteration:   544, Loss: 0.07730662, Accuracy: 95.73%\n",
      "Iteration:   545, Loss: 0.07663760, Accuracy: 95.71%\n",
      "Iteration:   546, Loss: 0.07603275, Accuracy: 95.77%\n",
      "Iteration:   547, Loss: 0.07583891, Accuracy: 95.74%\n",
      "Iteration:   548, Loss: 0.07454344, Accuracy: 95.76%\n",
      "Iteration:   549, Loss: 0.07459360, Accuracy: 95.83%\n",
      "Iteration:   550, Loss: 0.07381989, Accuracy: 95.83%\n",
      "Iteration:   551, Loss: 0.07433071, Accuracy: 95.73%\n",
      "Iteration:   552, Loss: 0.07348508, Accuracy: 95.72%\n",
      "Iteration:   553, Loss: 0.07352930, Accuracy: 95.87%\n",
      "Iteration:   554, Loss: 0.07297516, Accuracy: 95.80%\n",
      "Iteration:   555, Loss: 0.07298792, Accuracy: 95.90%\n",
      "Iteration:   556, Loss: 0.07282710, Accuracy: 95.79%\n",
      "Iteration:   557, Loss: 0.07368304, Accuracy: 95.84%\n",
      "Iteration:   558, Loss: 0.07303810, Accuracy: 95.79%\n",
      "Iteration:   559, Loss: 0.07309589, Accuracy: 95.72%\n",
      "Iteration:   560, Loss: 0.07291202, Accuracy: 95.73%\n",
      "Iteration:   561, Loss: 0.07273274, Accuracy: 95.84%\n",
      "Iteration:   562, Loss: 0.07295346, Accuracy: 95.82%\n",
      "Iteration:   563, Loss: 0.07337325, Accuracy: 95.78%\n",
      "Iteration:   564, Loss: 0.07334597, Accuracy: 95.90%\n",
      "Iteration:   565, Loss: 0.07453565, Accuracy: 95.86%\n",
      "Iteration:   566, Loss: 0.07498932, Accuracy: 95.77%\n",
      "Iteration:   567, Loss: 0.07697006, Accuracy: 95.74%\n",
      "Iteration:   568, Loss: 0.07691207, Accuracy: 95.68%\n",
      "Iteration:   569, Loss: 0.07867783, Accuracy: 95.75%\n",
      "Iteration:   570, Loss: 0.07861508, Accuracy: 95.80%\n",
      "Iteration:   571, Loss: 0.07948884, Accuracy: 95.75%\n",
      "Iteration:   572, Loss: 0.07879676, Accuracy: 95.68%\n",
      "Iteration:   573, Loss: 0.07831818, Accuracy: 95.75%\n",
      "Iteration:   574, Loss: 0.07679660, Accuracy: 95.75%\n",
      "Iteration:   575, Loss: 0.07543037, Accuracy: 95.82%\n",
      "Iteration:   576, Loss: 0.07381030, Accuracy: 95.87%\n",
      "Iteration:   577, Loss: 0.07243962, Accuracy: 95.87%\n",
      "Iteration:   578, Loss: 0.07089245, Accuracy: 95.85%\n",
      "Iteration:   579, Loss: 0.06980989, Accuracy: 95.98%\n",
      "Iteration:   580, Loss: 0.06870221, Accuracy: 95.84%\n",
      "Iteration:   581, Loss: 0.06821625, Accuracy: 95.95%\n",
      "Iteration:   582, Loss: 0.06757422, Accuracy: 95.92%\n",
      "Iteration:   583, Loss: 0.06718935, Accuracy: 95.96%\n",
      "Iteration:   584, Loss: 0.06687126, Accuracy: 95.87%\n",
      "Iteration:   585, Loss: 0.06683053, Accuracy: 95.95%\n",
      "Iteration:   586, Loss: 0.06665645, Accuracy: 95.87%\n",
      "Iteration:   587, Loss: 0.06710749, Accuracy: 95.91%\n",
      "Iteration:   588, Loss: 0.06725840, Accuracy: 95.88%\n",
      "Iteration:   589, Loss: 0.06687815, Accuracy: 95.89%\n",
      "Iteration:   590, Loss: 0.06690709, Accuracy: 95.79%\n",
      "Iteration:   591, Loss: 0.06760954, Accuracy: 95.83%\n",
      "Iteration:   592, Loss: 0.06755006, Accuracy: 95.84%\n",
      "Iteration:   593, Loss: 0.06836244, Accuracy: 95.93%\n",
      "Iteration:   594, Loss: 0.06924342, Accuracy: 95.90%\n",
      "Iteration:   595, Loss: 0.06907451, Accuracy: 95.89%\n",
      "Iteration:   596, Loss: 0.06881653, Accuracy: 95.91%\n",
      "Iteration:   597, Loss: 0.06960794, Accuracy: 95.87%\n",
      "Iteration:   598, Loss: 0.06972349, Accuracy: 95.86%\n",
      "Iteration:   599, Loss: 0.06940609, Accuracy: 95.90%\n",
      "Iteration:   600, Loss: 0.06941446, Accuracy: 95.84%\n",
      "Iteration:   601, Loss: 0.06990449, Accuracy: 95.89%\n",
      "Iteration:   602, Loss: 0.06968598, Accuracy: 95.71%\n",
      "Iteration:   603, Loss: 0.07009398, Accuracy: 95.89%\n",
      "Iteration:   604, Loss: 0.06848912, Accuracy: 95.87%\n",
      "Iteration:   605, Loss: 0.06735274, Accuracy: 95.89%\n",
      "Iteration:   606, Loss: 0.06665830, Accuracy: 95.85%\n",
      "Iteration:   607, Loss: 0.06598518, Accuracy: 95.93%\n",
      "Iteration:   608, Loss: 0.06548915, Accuracy: 96.00%\n",
      "Iteration:   609, Loss: 0.06454979, Accuracy: 95.92%\n",
      "Iteration:   610, Loss: 0.06396096, Accuracy: 95.88%\n",
      "Iteration:   611, Loss: 0.06360775, Accuracy: 95.96%\n",
      "Iteration:   612, Loss: 0.06335676, Accuracy: 95.89%\n",
      "Iteration:   613, Loss: 0.06295991, Accuracy: 96.01%\n",
      "Iteration:   614, Loss: 0.06273662, Accuracy: 95.81%\n",
      "Iteration:   615, Loss: 0.06251983, Accuracy: 95.95%\n",
      "Iteration:   616, Loss: 0.06219091, Accuracy: 95.90%\n",
      "Iteration:   617, Loss: 0.06263721, Accuracy: 96.00%\n",
      "Iteration:   618, Loss: 0.06231982, Accuracy: 95.91%\n",
      "Iteration:   619, Loss: 0.06275301, Accuracy: 95.90%\n",
      "Iteration:   620, Loss: 0.06267371, Accuracy: 95.95%\n",
      "Iteration:   621, Loss: 0.06345344, Accuracy: 95.94%\n",
      "Iteration:   622, Loss: 0.06362362, Accuracy: 95.91%\n",
      "Iteration:   623, Loss: 0.06375535, Accuracy: 95.97%\n",
      "Iteration:   624, Loss: 0.06496223, Accuracy: 95.87%\n",
      "Iteration:   625, Loss: 0.06574393, Accuracy: 96.01%\n",
      "Iteration:   626, Loss: 0.06594776, Accuracy: 95.83%\n",
      "Iteration:   627, Loss: 0.06756327, Accuracy: 96.02%\n",
      "Iteration:   628, Loss: 0.06738533, Accuracy: 95.84%\n",
      "Iteration:   629, Loss: 0.06849445, Accuracy: 95.97%\n",
      "Iteration:   630, Loss: 0.06848469, Accuracy: 95.84%\n",
      "Iteration:   631, Loss: 0.06828448, Accuracy: 95.85%\n",
      "Iteration:   632, Loss: 0.06827190, Accuracy: 96.01%\n",
      "Iteration:   633, Loss: 0.06757222, Accuracy: 95.94%\n",
      "Iteration:   634, Loss: 0.06608765, Accuracy: 95.85%\n",
      "Iteration:   635, Loss: 0.06559872, Accuracy: 95.90%\n",
      "Iteration:   636, Loss: 0.06431488, Accuracy: 95.89%\n",
      "Iteration:   637, Loss: 0.06343327, Accuracy: 95.99%\n",
      "Iteration:   638, Loss: 0.06282476, Accuracy: 95.89%\n",
      "Iteration:   639, Loss: 0.06124243, Accuracy: 95.94%\n",
      "Iteration:   640, Loss: 0.06084776, Accuracy: 95.90%\n",
      "Iteration:   641, Loss: 0.06112531, Accuracy: 96.00%\n",
      "Iteration:   642, Loss: 0.06044960, Accuracy: 95.97%\n",
      "Iteration:   643, Loss: 0.06003982, Accuracy: 96.02%\n",
      "Iteration:   644, Loss: 0.05970748, Accuracy: 95.98%\n",
      "Iteration:   645, Loss: 0.05978985, Accuracy: 95.90%\n",
      "Iteration:   646, Loss: 0.05974032, Accuracy: 96.01%\n",
      "Iteration:   647, Loss: 0.06005962, Accuracy: 95.86%\n",
      "Iteration:   648, Loss: 0.06054216, Accuracy: 96.04%\n",
      "Iteration:   649, Loss: 0.06103760, Accuracy: 95.99%\n",
      "Iteration:   650, Loss: 0.06143024, Accuracy: 95.96%\n",
      "Iteration:   651, Loss: 0.06126862, Accuracy: 95.96%\n",
      "Iteration:   652, Loss: 0.06098652, Accuracy: 95.94%\n",
      "Iteration:   653, Loss: 0.06087111, Accuracy: 95.92%\n",
      "Iteration:   654, Loss: 0.06005223, Accuracy: 95.93%\n",
      "Iteration:   655, Loss: 0.06021632, Accuracy: 95.97%\n",
      "Iteration:   656, Loss: 0.06014021, Accuracy: 95.94%\n",
      "Iteration:   657, Loss: 0.05975239, Accuracy: 96.05%\n",
      "Iteration:   658, Loss: 0.06077501, Accuracy: 95.92%\n",
      "Iteration:   659, Loss: 0.06124669, Accuracy: 96.06%\n",
      "Iteration:   660, Loss: 0.06101322, Accuracy: 95.97%\n",
      "Iteration:   661, Loss: 0.06077314, Accuracy: 96.03%\n",
      "Iteration:   662, Loss: 0.06045764, Accuracy: 95.99%\n",
      "Iteration:   663, Loss: 0.06069756, Accuracy: 96.04%\n",
      "Iteration:   664, Loss: 0.06094211, Accuracy: 95.97%\n",
      "Iteration:   665, Loss: 0.06089556, Accuracy: 95.97%\n",
      "Iteration:   666, Loss: 0.06016963, Accuracy: 95.96%\n",
      "Iteration:   667, Loss: 0.05965805, Accuracy: 96.12%\n",
      "Iteration:   668, Loss: 0.06025230, Accuracy: 95.98%\n",
      "Iteration:   669, Loss: 0.05937743, Accuracy: 96.07%\n",
      "Iteration:   670, Loss: 0.05878023, Accuracy: 96.02%\n",
      "Iteration:   671, Loss: 0.05789162, Accuracy: 96.10%\n",
      "Iteration:   672, Loss: 0.05770009, Accuracy: 95.99%\n",
      "Iteration:   673, Loss: 0.05673670, Accuracy: 96.02%\n",
      "Iteration:   674, Loss: 0.05652882, Accuracy: 95.93%\n",
      "Iteration:   675, Loss: 0.05617303, Accuracy: 96.06%\n",
      "Iteration:   676, Loss: 0.05589421, Accuracy: 95.99%\n",
      "Iteration:   677, Loss: 0.05536600, Accuracy: 96.09%\n",
      "Iteration:   678, Loss: 0.05493530, Accuracy: 96.05%\n",
      "Iteration:   679, Loss: 0.05488224, Accuracy: 96.12%\n",
      "Iteration:   680, Loss: 0.05470223, Accuracy: 96.08%\n",
      "Iteration:   681, Loss: 0.05450749, Accuracy: 96.01%\n",
      "Iteration:   682, Loss: 0.05425173, Accuracy: 96.05%\n",
      "Iteration:   683, Loss: 0.05436295, Accuracy: 96.04%\n",
      "Iteration:   684, Loss: 0.05447397, Accuracy: 96.01%\n",
      "Iteration:   685, Loss: 0.05469143, Accuracy: 96.02%\n",
      "Iteration:   686, Loss: 0.05469121, Accuracy: 96.10%\n",
      "Iteration:   687, Loss: 0.05473169, Accuracy: 96.08%\n",
      "Iteration:   688, Loss: 0.05490860, Accuracy: 95.97%\n",
      "Iteration:   689, Loss: 0.05573927, Accuracy: 96.14%\n",
      "Iteration:   690, Loss: 0.05641833, Accuracy: 96.06%\n",
      "Iteration:   691, Loss: 0.05779384, Accuracy: 96.01%\n",
      "Iteration:   692, Loss: 0.05877512, Accuracy: 96.03%\n",
      "Iteration:   693, Loss: 0.06175289, Accuracy: 95.95%\n",
      "Iteration:   694, Loss: 0.06318358, Accuracy: 95.99%\n",
      "Iteration:   695, Loss: 0.06552588, Accuracy: 96.00%\n",
      "Iteration:   696, Loss: 0.06430826, Accuracy: 95.94%\n",
      "Iteration:   697, Loss: 0.06533055, Accuracy: 96.01%\n",
      "Iteration:   698, Loss: 0.06564930, Accuracy: 95.96%\n",
      "Iteration:   699, Loss: 0.06569618, Accuracy: 95.93%\n",
      "Iteration:   700, Loss: 0.06403097, Accuracy: 95.99%\n",
      "Iteration:   701, Loss: 0.06321813, Accuracy: 96.00%\n",
      "Iteration:   702, Loss: 0.06150323, Accuracy: 96.05%\n",
      "Iteration:   703, Loss: 0.06070921, Accuracy: 96.00%\n",
      "Iteration:   704, Loss: 0.05844156, Accuracy: 96.05%\n",
      "Iteration:   705, Loss: 0.05705970, Accuracy: 96.03%\n",
      "Iteration:   706, Loss: 0.05520288, Accuracy: 96.01%\n",
      "Iteration:   707, Loss: 0.05379007, Accuracy: 96.13%\n",
      "Iteration:   708, Loss: 0.05301651, Accuracy: 96.10%\n",
      "Iteration:   709, Loss: 0.05259198, Accuracy: 96.09%\n",
      "Iteration:   710, Loss: 0.05230598, Accuracy: 96.08%\n",
      "Iteration:   711, Loss: 0.05173328, Accuracy: 96.02%\n",
      "Iteration:   712, Loss: 0.05160640, Accuracy: 96.05%\n",
      "Iteration:   713, Loss: 0.05111216, Accuracy: 96.08%\n",
      "Iteration:   714, Loss: 0.05111646, Accuracy: 96.08%\n",
      "Iteration:   715, Loss: 0.05104945, Accuracy: 96.09%\n",
      "Iteration:   716, Loss: 0.05100447, Accuracy: 96.02%\n",
      "Iteration:   717, Loss: 0.05091521, Accuracy: 96.14%\n",
      "Iteration:   718, Loss: 0.05098100, Accuracy: 96.08%\n",
      "Iteration:   719, Loss: 0.05108962, Accuracy: 96.09%\n",
      "Iteration:   720, Loss: 0.05128534, Accuracy: 96.07%\n",
      "Iteration:   721, Loss: 0.05140092, Accuracy: 96.18%\n",
      "Iteration:   722, Loss: 0.05158878, Accuracy: 96.07%\n",
      "Iteration:   723, Loss: 0.05226355, Accuracy: 96.07%\n",
      "Iteration:   724, Loss: 0.05221087, Accuracy: 96.04%\n",
      "Iteration:   725, Loss: 0.05275273, Accuracy: 96.12%\n",
      "Iteration:   726, Loss: 0.05242326, Accuracy: 96.07%\n",
      "Iteration:   727, Loss: 0.05258222, Accuracy: 96.08%\n",
      "Iteration:   728, Loss: 0.05298933, Accuracy: 96.13%\n",
      "Iteration:   729, Loss: 0.05354790, Accuracy: 96.07%\n",
      "Iteration:   730, Loss: 0.05413146, Accuracy: 96.06%\n",
      "Iteration:   731, Loss: 0.05595043, Accuracy: 96.18%\n",
      "Iteration:   732, Loss: 0.05639302, Accuracy: 95.98%\n",
      "Iteration:   733, Loss: 0.05748732, Accuracy: 96.11%\n",
      "Iteration:   734, Loss: 0.05754299, Accuracy: 96.09%\n",
      "Iteration:   735, Loss: 0.05703137, Accuracy: 96.16%\n",
      "Iteration:   736, Loss: 0.05635411, Accuracy: 96.08%\n",
      "Iteration:   737, Loss: 0.05546218, Accuracy: 96.15%\n",
      "Iteration:   738, Loss: 0.05506951, Accuracy: 96.00%\n",
      "Iteration:   739, Loss: 0.05459772, Accuracy: 96.10%\n",
      "Iteration:   740, Loss: 0.05268541, Accuracy: 96.09%\n",
      "Iteration:   741, Loss: 0.05237325, Accuracy: 96.18%\n",
      "Iteration:   742, Loss: 0.05112941, Accuracy: 96.13%\n",
      "Iteration:   743, Loss: 0.05052817, Accuracy: 96.12%\n",
      "Iteration:   744, Loss: 0.04985924, Accuracy: 96.11%\n",
      "Iteration:   745, Loss: 0.04940949, Accuracy: 96.15%\n",
      "Iteration:   746, Loss: 0.04892454, Accuracy: 96.10%\n",
      "Iteration:   747, Loss: 0.04859833, Accuracy: 96.28%\n",
      "Iteration:   748, Loss: 0.04839525, Accuracy: 96.11%\n",
      "Iteration:   749, Loss: 0.04809961, Accuracy: 96.21%\n",
      "Iteration:   750, Loss: 0.04791080, Accuracy: 96.13%\n",
      "Iteration:   751, Loss: 0.04802756, Accuracy: 96.21%\n",
      "Iteration:   752, Loss: 0.04784717, Accuracy: 96.09%\n",
      "Iteration:   753, Loss: 0.04776563, Accuracy: 96.19%\n",
      "Iteration:   754, Loss: 0.04770806, Accuracy: 96.13%\n",
      "Iteration:   755, Loss: 0.04794002, Accuracy: 96.13%\n",
      "Iteration:   756, Loss: 0.04807882, Accuracy: 96.11%\n",
      "Iteration:   757, Loss: 0.04802620, Accuracy: 96.19%\n",
      "Iteration:   758, Loss: 0.04822015, Accuracy: 96.04%\n",
      "Iteration:   759, Loss: 0.04864530, Accuracy: 96.24%\n",
      "Iteration:   760, Loss: 0.04871264, Accuracy: 96.05%\n",
      "Iteration:   761, Loss: 0.04886640, Accuracy: 96.18%\n",
      "Iteration:   762, Loss: 0.04899560, Accuracy: 96.09%\n",
      "Iteration:   763, Loss: 0.05001173, Accuracy: 96.15%\n",
      "Iteration:   764, Loss: 0.05075090, Accuracy: 95.98%\n",
      "Iteration:   765, Loss: 0.05096371, Accuracy: 96.19%\n",
      "Iteration:   766, Loss: 0.05217682, Accuracy: 96.13%\n",
      "Iteration:   767, Loss: 0.05265525, Accuracy: 96.21%\n",
      "Iteration:   768, Loss: 0.05203528, Accuracy: 96.12%\n",
      "Iteration:   769, Loss: 0.05247868, Accuracy: 96.20%\n",
      "Iteration:   770, Loss: 0.05200983, Accuracy: 96.14%\n",
      "Iteration:   771, Loss: 0.05325020, Accuracy: 96.21%\n",
      "Iteration:   772, Loss: 0.05328436, Accuracy: 95.93%\n",
      "Iteration:   773, Loss: 0.05313349, Accuracy: 96.12%\n",
      "Iteration:   774, Loss: 0.05308917, Accuracy: 96.07%\n",
      "Iteration:   775, Loss: 0.05278217, Accuracy: 96.21%\n",
      "Iteration:   776, Loss: 0.05285186, Accuracy: 96.14%\n",
      "Iteration:   777, Loss: 0.05290636, Accuracy: 96.17%\n",
      "Iteration:   778, Loss: 0.05083150, Accuracy: 96.12%\n",
      "Iteration:   779, Loss: 0.04975799, Accuracy: 96.20%\n",
      "Iteration:   780, Loss: 0.04874496, Accuracy: 96.14%\n",
      "Iteration:   781, Loss: 0.04778186, Accuracy: 96.20%\n",
      "Iteration:   782, Loss: 0.04731834, Accuracy: 96.16%\n",
      "Iteration:   783, Loss: 0.04667623, Accuracy: 96.25%\n",
      "Iteration:   784, Loss: 0.04613719, Accuracy: 96.17%\n",
      "Iteration:   785, Loss: 0.04557656, Accuracy: 96.17%\n",
      "Iteration:   786, Loss: 0.04538298, Accuracy: 96.23%\n",
      "Iteration:   787, Loss: 0.04520483, Accuracy: 96.17%\n",
      "Iteration:   788, Loss: 0.04509563, Accuracy: 96.15%\n",
      "Iteration:   789, Loss: 0.04487576, Accuracy: 96.20%\n",
      "Iteration:   790, Loss: 0.04514057, Accuracy: 96.15%\n",
      "Iteration:   791, Loss: 0.04505626, Accuracy: 96.16%\n",
      "Iteration:   792, Loss: 0.04481983, Accuracy: 96.12%\n",
      "Iteration:   793, Loss: 0.04457927, Accuracy: 96.18%\n",
      "Iteration:   794, Loss: 0.04466944, Accuracy: 96.12%\n",
      "Iteration:   795, Loss: 0.04465300, Accuracy: 96.19%\n",
      "Iteration:   796, Loss: 0.04477133, Accuracy: 96.09%\n",
      "Iteration:   797, Loss: 0.04483140, Accuracy: 96.19%\n",
      "Iteration:   798, Loss: 0.04486307, Accuracy: 96.15%\n",
      "Iteration:   799, Loss: 0.04487389, Accuracy: 96.25%\n",
      "Iteration:   800, Loss: 0.04482267, Accuracy: 96.12%\n",
      "Iteration:   801, Loss: 0.04487801, Accuracy: 96.22%\n",
      "Iteration:   802, Loss: 0.04507332, Accuracy: 96.13%\n",
      "Iteration:   803, Loss: 0.04512456, Accuracy: 96.18%\n",
      "Iteration:   804, Loss: 0.04527451, Accuracy: 96.15%\n",
      "Iteration:   805, Loss: 0.04540619, Accuracy: 96.20%\n",
      "Iteration:   806, Loss: 0.04574091, Accuracy: 96.17%\n",
      "Iteration:   807, Loss: 0.04621042, Accuracy: 96.16%\n",
      "Iteration:   808, Loss: 0.04690364, Accuracy: 96.11%\n",
      "Iteration:   809, Loss: 0.04698251, Accuracy: 96.15%\n",
      "Iteration:   810, Loss: 0.04760701, Accuracy: 96.11%\n",
      "Iteration:   811, Loss: 0.04794047, Accuracy: 96.17%\n",
      "Iteration:   812, Loss: 0.04974729, Accuracy: 96.06%\n",
      "Iteration:   813, Loss: 0.05139156, Accuracy: 96.11%\n",
      "Iteration:   814, Loss: 0.05406269, Accuracy: 96.15%\n",
      "Iteration:   815, Loss: 0.05871045, Accuracy: 95.96%\n",
      "Iteration:   816, Loss: 0.06573495, Accuracy: 95.70%\n",
      "Iteration:   817, Loss: 0.06657753, Accuracy: 95.85%\n",
      "Iteration:   818, Loss: 0.06831122, Accuracy: 95.78%\n",
      "Iteration:   819, Loss: 0.06986930, Accuracy: 95.79%\n",
      "Iteration:   820, Loss: 0.06799828, Accuracy: 95.80%\n",
      "Iteration:   821, Loss: 0.06401123, Accuracy: 95.84%\n",
      "Iteration:   822, Loss: 0.06305279, Accuracy: 95.75%\n",
      "Iteration:   823, Loss: 0.06352863, Accuracy: 95.88%\n",
      "Iteration:   824, Loss: 0.06203305, Accuracy: 95.98%\n",
      "Iteration:   825, Loss: 0.06212047, Accuracy: 96.12%\n",
      "Iteration:   826, Loss: 0.06116454, Accuracy: 96.00%\n",
      "Iteration:   827, Loss: 0.05884513, Accuracy: 95.99%\n",
      "Iteration:   828, Loss: 0.05648223, Accuracy: 96.15%\n",
      "Iteration:   829, Loss: 0.05373667, Accuracy: 96.04%\n",
      "Iteration:   830, Loss: 0.05131199, Accuracy: 96.21%\n",
      "Iteration:   831, Loss: 0.04839815, Accuracy: 96.22%\n",
      "Iteration:   832, Loss: 0.04624223, Accuracy: 96.27%\n",
      "Iteration:   833, Loss: 0.04479802, Accuracy: 96.24%\n",
      "Iteration:   834, Loss: 0.04404279, Accuracy: 96.33%\n",
      "Iteration:   835, Loss: 0.04335463, Accuracy: 96.31%\n",
      "Iteration:   836, Loss: 0.04319249, Accuracy: 96.27%\n",
      "Iteration:   837, Loss: 0.04265159, Accuracy: 96.26%\n",
      "Iteration:   838, Loss: 0.04258066, Accuracy: 96.25%\n",
      "Iteration:   839, Loss: 0.04231929, Accuracy: 96.25%\n",
      "Iteration:   840, Loss: 0.04219973, Accuracy: 96.21%\n",
      "Iteration:   841, Loss: 0.04203932, Accuracy: 96.25%\n",
      "Iteration:   842, Loss: 0.04196829, Accuracy: 96.25%\n",
      "Iteration:   843, Loss: 0.04210177, Accuracy: 96.27%\n",
      "Iteration:   844, Loss: 0.04212681, Accuracy: 96.24%\n",
      "Iteration:   845, Loss: 0.04198495, Accuracy: 96.26%\n",
      "Iteration:   846, Loss: 0.04195632, Accuracy: 96.23%\n",
      "Iteration:   847, Loss: 0.04182361, Accuracy: 96.29%\n",
      "Iteration:   848, Loss: 0.04202934, Accuracy: 96.28%\n",
      "Iteration:   849, Loss: 0.04194389, Accuracy: 96.23%\n",
      "Iteration:   850, Loss: 0.04226363, Accuracy: 96.28%\n",
      "Iteration:   851, Loss: 0.04186551, Accuracy: 96.29%\n",
      "Iteration:   852, Loss: 0.04229193, Accuracy: 96.25%\n",
      "Iteration:   853, Loss: 0.04191404, Accuracy: 96.21%\n",
      "Iteration:   854, Loss: 0.04197885, Accuracy: 96.24%\n",
      "Iteration:   855, Loss: 0.04202916, Accuracy: 96.22%\n",
      "Iteration:   856, Loss: 0.04227388, Accuracy: 96.25%\n",
      "Iteration:   857, Loss: 0.04190787, Accuracy: 96.24%\n",
      "Iteration:   858, Loss: 0.04173909, Accuracy: 96.16%\n",
      "Iteration:   859, Loss: 0.04141904, Accuracy: 96.25%\n",
      "Iteration:   860, Loss: 0.04115359, Accuracy: 96.23%\n",
      "Iteration:   861, Loss: 0.04090207, Accuracy: 96.21%\n",
      "Iteration:   862, Loss: 0.04059346, Accuracy: 96.18%\n",
      "Iteration:   863, Loss: 0.04042619, Accuracy: 96.24%\n",
      "Iteration:   864, Loss: 0.04037794, Accuracy: 96.19%\n",
      "Iteration:   865, Loss: 0.04042907, Accuracy: 96.26%\n",
      "Iteration:   866, Loss: 0.04033449, Accuracy: 96.20%\n",
      "Iteration:   867, Loss: 0.04021608, Accuracy: 96.20%\n",
      "Iteration:   868, Loss: 0.04014403, Accuracy: 96.20%\n",
      "Iteration:   869, Loss: 0.04000879, Accuracy: 96.17%\n",
      "Iteration:   870, Loss: 0.04009849, Accuracy: 96.20%\n",
      "Iteration:   871, Loss: 0.04008340, Accuracy: 96.19%\n",
      "Iteration:   872, Loss: 0.04019840, Accuracy: 96.20%\n",
      "Iteration:   873, Loss: 0.04053015, Accuracy: 96.19%\n",
      "Iteration:   874, Loss: 0.04071917, Accuracy: 96.20%\n",
      "Iteration:   875, Loss: 0.04046004, Accuracy: 96.21%\n",
      "Iteration:   876, Loss: 0.04073190, Accuracy: 96.30%\n",
      "Iteration:   877, Loss: 0.04064208, Accuracy: 96.29%\n",
      "Iteration:   878, Loss: 0.04134085, Accuracy: 96.25%\n",
      "Iteration:   879, Loss: 0.04097500, Accuracy: 96.18%\n",
      "Iteration:   880, Loss: 0.04125601, Accuracy: 96.26%\n",
      "Iteration:   881, Loss: 0.04107751, Accuracy: 96.21%\n",
      "Iteration:   882, Loss: 0.04122946, Accuracy: 96.36%\n",
      "Iteration:   883, Loss: 0.04077376, Accuracy: 96.20%\n",
      "Iteration:   884, Loss: 0.04121886, Accuracy: 96.37%\n",
      "Iteration:   885, Loss: 0.04101547, Accuracy: 96.22%\n",
      "Iteration:   886, Loss: 0.04128293, Accuracy: 96.21%\n",
      "Iteration:   887, Loss: 0.04138168, Accuracy: 96.32%\n",
      "Iteration:   888, Loss: 0.04073064, Accuracy: 96.24%\n",
      "Iteration:   889, Loss: 0.04031784, Accuracy: 96.33%\n",
      "Iteration:   890, Loss: 0.03988583, Accuracy: 96.25%\n",
      "Iteration:   891, Loss: 0.03943255, Accuracy: 96.22%\n",
      "Iteration:   892, Loss: 0.03958276, Accuracy: 96.25%\n",
      "Iteration:   893, Loss: 0.03939488, Accuracy: 96.17%\n",
      "Iteration:   894, Loss: 0.03897101, Accuracy: 96.20%\n",
      "Iteration:   895, Loss: 0.03903675, Accuracy: 96.25%\n",
      "Iteration:   896, Loss: 0.03889668, Accuracy: 96.23%\n",
      "Iteration:   897, Loss: 0.03879844, Accuracy: 96.25%\n",
      "Iteration:   898, Loss: 0.03901344, Accuracy: 96.21%\n",
      "Iteration:   899, Loss: 0.03876391, Accuracy: 96.22%\n",
      "Iteration:   900, Loss: 0.03865468, Accuracy: 96.22%\n",
      "Iteration:   901, Loss: 0.03872052, Accuracy: 96.21%\n",
      "Iteration:   902, Loss: 0.03850466, Accuracy: 96.23%\n",
      "Iteration:   903, Loss: 0.03825588, Accuracy: 96.23%\n",
      "Iteration:   904, Loss: 0.03830240, Accuracy: 96.26%\n",
      "Iteration:   905, Loss: 0.03836465, Accuracy: 96.23%\n",
      "Iteration:   906, Loss: 0.03845166, Accuracy: 96.24%\n",
      "Iteration:   907, Loss: 0.03867810, Accuracy: 96.28%\n",
      "Iteration:   908, Loss: 0.03836632, Accuracy: 96.19%\n",
      "Iteration:   909, Loss: 0.03853996, Accuracy: 96.27%\n",
      "Iteration:   910, Loss: 0.03825843, Accuracy: 96.20%\n",
      "Iteration:   911, Loss: 0.03848763, Accuracy: 96.28%\n",
      "Iteration:   912, Loss: 0.03839855, Accuracy: 96.23%\n",
      "Iteration:   913, Loss: 0.03814365, Accuracy: 96.30%\n",
      "Iteration:   914, Loss: 0.03833643, Accuracy: 96.19%\n",
      "Iteration:   915, Loss: 0.03843169, Accuracy: 96.29%\n",
      "Iteration:   916, Loss: 0.03847392, Accuracy: 96.19%\n",
      "Iteration:   917, Loss: 0.03848574, Accuracy: 96.25%\n",
      "Iteration:   918, Loss: 0.03839147, Accuracy: 96.19%\n",
      "Iteration:   919, Loss: 0.03880112, Accuracy: 96.27%\n",
      "Iteration:   920, Loss: 0.03854622, Accuracy: 96.19%\n",
      "Iteration:   921, Loss: 0.03915463, Accuracy: 96.26%\n",
      "Iteration:   922, Loss: 0.03959439, Accuracy: 96.27%\n",
      "Iteration:   923, Loss: 0.04019508, Accuracy: 96.34%\n",
      "Iteration:   924, Loss: 0.04092759, Accuracy: 96.20%\n",
      "Iteration:   925, Loss: 0.04291873, Accuracy: 96.37%\n",
      "Iteration:   926, Loss: 0.04383556, Accuracy: 96.19%\n",
      "Iteration:   927, Loss: 0.04789987, Accuracy: 96.28%\n",
      "Iteration:   928, Loss: 0.05200332, Accuracy: 96.03%\n",
      "Iteration:   929, Loss: 0.05658222, Accuracy: 96.15%\n",
      "Iteration:   930, Loss: 0.06296539, Accuracy: 96.10%\n",
      "Iteration:   931, Loss: 0.06862444, Accuracy: 95.90%\n",
      "Iteration:   932, Loss: 0.07356285, Accuracy: 95.72%\n",
      "Iteration:   933, Loss: 0.07493546, Accuracy: 95.71%\n",
      "Iteration:   934, Loss: 0.07524185, Accuracy: 95.70%\n",
      "Iteration:   935, Loss: 0.07268130, Accuracy: 95.97%\n",
      "Iteration:   936, Loss: 0.07441509, Accuracy: 95.71%\n",
      "Iteration:   937, Loss: 0.06730650, Accuracy: 95.94%\n",
      "Iteration:   938, Loss: 0.06279314, Accuracy: 96.01%\n",
      "Iteration:   939, Loss: 0.05647781, Accuracy: 96.21%\n",
      "Iteration:   940, Loss: 0.05088831, Accuracy: 96.12%\n",
      "Iteration:   941, Loss: 0.04650546, Accuracy: 96.33%\n",
      "Iteration:   942, Loss: 0.04287739, Accuracy: 96.27%\n",
      "Iteration:   943, Loss: 0.04098250, Accuracy: 96.34%\n",
      "Iteration:   944, Loss: 0.03951278, Accuracy: 96.38%\n",
      "Iteration:   945, Loss: 0.03856974, Accuracy: 96.39%\n",
      "Iteration:   946, Loss: 0.03795382, Accuracy: 96.35%\n",
      "Iteration:   947, Loss: 0.03758389, Accuracy: 96.30%\n",
      "Iteration:   948, Loss: 0.03718796, Accuracy: 96.42%\n",
      "Iteration:   949, Loss: 0.03701893, Accuracy: 96.36%\n",
      "Iteration:   950, Loss: 0.03681593, Accuracy: 96.34%\n",
      "Iteration:   951, Loss: 0.03666329, Accuracy: 96.35%\n",
      "Iteration:   952, Loss: 0.03657520, Accuracy: 96.30%\n",
      "Iteration:   953, Loss: 0.03641529, Accuracy: 96.31%\n",
      "Iteration:   954, Loss: 0.03632227, Accuracy: 96.32%\n",
      "Iteration:   955, Loss: 0.03632773, Accuracy: 96.35%\n",
      "Iteration:   956, Loss: 0.03620424, Accuracy: 96.36%\n",
      "Iteration:   957, Loss: 0.03624184, Accuracy: 96.34%\n",
      "Iteration:   958, Loss: 0.03624488, Accuracy: 96.29%\n",
      "Iteration:   959, Loss: 0.03642105, Accuracy: 96.32%\n",
      "Iteration:   960, Loss: 0.03622364, Accuracy: 96.33%\n",
      "Iteration:   961, Loss: 0.03632741, Accuracy: 96.34%\n",
      "Iteration:   962, Loss: 0.03597773, Accuracy: 96.36%\n",
      "Iteration:   963, Loss: 0.03613574, Accuracy: 96.28%\n",
      "Iteration:   964, Loss: 0.03596482, Accuracy: 96.30%\n",
      "Iteration:   965, Loss: 0.03597869, Accuracy: 96.31%\n",
      "Iteration:   966, Loss: 0.03564281, Accuracy: 96.30%\n",
      "Iteration:   967, Loss: 0.03573984, Accuracy: 96.29%\n",
      "Iteration:   968, Loss: 0.03553704, Accuracy: 96.28%\n",
      "Iteration:   969, Loss: 0.03567620, Accuracy: 96.32%\n",
      "Iteration:   970, Loss: 0.03549907, Accuracy: 96.25%\n",
      "Iteration:   971, Loss: 0.03542339, Accuracy: 96.32%\n",
      "Iteration:   972, Loss: 0.03546811, Accuracy: 96.26%\n",
      "Iteration:   973, Loss: 0.03560064, Accuracy: 96.32%\n",
      "Iteration:   974, Loss: 0.03532385, Accuracy: 96.27%\n",
      "Iteration:   975, Loss: 0.03542943, Accuracy: 96.39%\n",
      "Iteration:   976, Loss: 0.03521464, Accuracy: 96.33%\n",
      "Iteration:   977, Loss: 0.03538299, Accuracy: 96.31%\n",
      "Iteration:   978, Loss: 0.03510483, Accuracy: 96.30%\n",
      "Iteration:   979, Loss: 0.03531720, Accuracy: 96.34%\n",
      "Iteration:   980, Loss: 0.03501531, Accuracy: 96.23%\n",
      "Iteration:   981, Loss: 0.03523530, Accuracy: 96.32%\n",
      "Iteration:   982, Loss: 0.03488764, Accuracy: 96.27%\n",
      "Iteration:   983, Loss: 0.03483067, Accuracy: 96.28%\n",
      "Iteration:   984, Loss: 0.03477195, Accuracy: 96.34%\n",
      "Iteration:   985, Loss: 0.03455639, Accuracy: 96.28%\n",
      "Iteration:   986, Loss: 0.03460959, Accuracy: 96.34%\n",
      "Iteration:   987, Loss: 0.03469394, Accuracy: 96.31%\n",
      "Iteration:   988, Loss: 0.03448259, Accuracy: 96.32%\n",
      "Iteration:   989, Loss: 0.03452353, Accuracy: 96.33%\n",
      "Iteration:   990, Loss: 0.03434518, Accuracy: 96.33%\n",
      "Iteration:   991, Loss: 0.03456465, Accuracy: 96.30%\n",
      "Iteration:   992, Loss: 0.03430235, Accuracy: 96.31%\n",
      "Iteration:   993, Loss: 0.03434798, Accuracy: 96.29%\n",
      "Iteration:   994, Loss: 0.03440235, Accuracy: 96.28%\n",
      "Iteration:   995, Loss: 0.03455013, Accuracy: 96.28%\n",
      "Iteration:   996, Loss: 0.03443049, Accuracy: 96.32%\n",
      "Iteration:   997, Loss: 0.03467048, Accuracy: 96.37%\n",
      "Iteration:   998, Loss: 0.03444643, Accuracy: 96.33%\n",
      "Iteration:   999, Loss: 0.03446419, Accuracy: 96.32%\n",
      "Iteration:  1000, Loss: 0.03424399, Accuracy: 96.32%\n",
      "Iteration:  1001, Loss: 0.03437577, Accuracy: 96.31%\n",
      "Iteration:  1002, Loss: 0.03420217, Accuracy: 96.36%\n",
      "Iteration:  1003, Loss: 0.03422000, Accuracy: 96.33%\n",
      "Iteration:  1004, Loss: 0.03415087, Accuracy: 96.36%\n",
      "Iteration:  1005, Loss: 0.03398222, Accuracy: 96.33%\n",
      "Iteration:  1006, Loss: 0.03392700, Accuracy: 96.33%\n",
      "Iteration:  1007, Loss: 0.03421211, Accuracy: 96.37%\n",
      "Iteration:  1008, Loss: 0.03410104, Accuracy: 96.29%\n",
      "Iteration:  1009, Loss: 0.03395604, Accuracy: 96.37%\n",
      "Iteration:  1010, Loss: 0.03387780, Accuracy: 96.25%\n",
      "Iteration:  1011, Loss: 0.03381630, Accuracy: 96.30%\n",
      "Iteration:  1012, Loss: 0.03385845, Accuracy: 96.29%\n",
      "Iteration:  1013, Loss: 0.03367985, Accuracy: 96.33%\n",
      "Iteration:  1014, Loss: 0.03356123, Accuracy: 96.24%\n",
      "Iteration:  1015, Loss: 0.03356189, Accuracy: 96.30%\n",
      "Iteration:  1016, Loss: 0.03333186, Accuracy: 96.27%\n",
      "Iteration:  1017, Loss: 0.03341097, Accuracy: 96.33%\n",
      "Iteration:  1018, Loss: 0.03338186, Accuracy: 96.27%\n",
      "Iteration:  1019, Loss: 0.03343800, Accuracy: 96.32%\n",
      "Iteration:  1020, Loss: 0.03331984, Accuracy: 96.29%\n",
      "Iteration:  1021, Loss: 0.03332726, Accuracy: 96.37%\n",
      "Iteration:  1022, Loss: 0.03323985, Accuracy: 96.27%\n",
      "Iteration:  1023, Loss: 0.03308904, Accuracy: 96.35%\n",
      "Iteration:  1024, Loss: 0.03315505, Accuracy: 96.30%\n",
      "Iteration:  1025, Loss: 0.03322706, Accuracy: 96.36%\n",
      "Iteration:  1026, Loss: 0.03292083, Accuracy: 96.33%\n",
      "Iteration:  1027, Loss: 0.03292151, Accuracy: 96.37%\n",
      "Iteration:  1028, Loss: 0.03299783, Accuracy: 96.35%\n",
      "Iteration:  1029, Loss: 0.03315772, Accuracy: 96.40%\n",
      "Iteration:  1030, Loss: 0.03318620, Accuracy: 96.34%\n",
      "Iteration:  1031, Loss: 0.03320523, Accuracy: 96.36%\n",
      "Iteration:  1032, Loss: 0.03300540, Accuracy: 96.33%\n",
      "Iteration:  1033, Loss: 0.03333058, Accuracy: 96.32%\n",
      "Iteration:  1034, Loss: 0.03302043, Accuracy: 96.32%\n",
      "Iteration:  1035, Loss: 0.03307176, Accuracy: 96.34%\n",
      "Iteration:  1036, Loss: 0.03289630, Accuracy: 96.31%\n",
      "Iteration:  1037, Loss: 0.03294467, Accuracy: 96.32%\n",
      "Iteration:  1038, Loss: 0.03282195, Accuracy: 96.35%\n",
      "Iteration:  1039, Loss: 0.03305918, Accuracy: 96.31%\n",
      "Iteration:  1040, Loss: 0.03262137, Accuracy: 96.38%\n",
      "Iteration:  1041, Loss: 0.03283022, Accuracy: 96.31%\n",
      "Iteration:  1042, Loss: 0.03272750, Accuracy: 96.36%\n",
      "Iteration:  1043, Loss: 0.03276778, Accuracy: 96.30%\n",
      "Iteration:  1044, Loss: 0.03270483, Accuracy: 96.33%\n",
      "Iteration:  1045, Loss: 0.03275359, Accuracy: 96.32%\n",
      "Iteration:  1046, Loss: 0.03275193, Accuracy: 96.33%\n",
      "Iteration:  1047, Loss: 0.03276397, Accuracy: 96.32%\n",
      "Iteration:  1048, Loss: 0.03283774, Accuracy: 96.30%\n",
      "Iteration:  1049, Loss: 0.03282723, Accuracy: 96.29%\n",
      "Iteration:  1050, Loss: 0.03262356, Accuracy: 96.31%\n",
      "Iteration:  1051, Loss: 0.03253428, Accuracy: 96.31%\n",
      "Iteration:  1052, Loss: 0.03244365, Accuracy: 96.31%\n",
      "Iteration:  1053, Loss: 0.03247806, Accuracy: 96.36%\n",
      "Iteration:  1054, Loss: 0.03224782, Accuracy: 96.32%\n",
      "Iteration:  1055, Loss: 0.03238013, Accuracy: 96.32%\n",
      "Iteration:  1056, Loss: 0.03218984, Accuracy: 96.31%\n",
      "Iteration:  1057, Loss: 0.03229545, Accuracy: 96.33%\n",
      "Iteration:  1058, Loss: 0.03196674, Accuracy: 96.37%\n",
      "Iteration:  1059, Loss: 0.03183185, Accuracy: 96.33%\n",
      "Iteration:  1060, Loss: 0.03180945, Accuracy: 96.35%\n",
      "Iteration:  1061, Loss: 0.03172771, Accuracy: 96.34%\n",
      "Iteration:  1062, Loss: 0.03169612, Accuracy: 96.34%\n",
      "Iteration:  1063, Loss: 0.03164866, Accuracy: 96.33%\n",
      "Iteration:  1064, Loss: 0.03159121, Accuracy: 96.33%\n",
      "Iteration:  1065, Loss: 0.03150432, Accuracy: 96.28%\n",
      "Iteration:  1066, Loss: 0.03141013, Accuracy: 96.34%\n",
      "Iteration:  1067, Loss: 0.03144761, Accuracy: 96.31%\n",
      "Iteration:  1068, Loss: 0.03138481, Accuracy: 96.34%\n",
      "Iteration:  1069, Loss: 0.03156112, Accuracy: 96.33%\n",
      "Iteration:  1070, Loss: 0.03141252, Accuracy: 96.30%\n",
      "Iteration:  1071, Loss: 0.03161212, Accuracy: 96.27%\n",
      "Iteration:  1072, Loss: 0.03166943, Accuracy: 96.31%\n",
      "Iteration:  1073, Loss: 0.03173065, Accuracy: 96.27%\n",
      "Iteration:  1074, Loss: 0.03163930, Accuracy: 96.33%\n",
      "Iteration:  1075, Loss: 0.03187871, Accuracy: 96.33%\n",
      "Iteration:  1076, Loss: 0.03156551, Accuracy: 96.40%\n",
      "Iteration:  1077, Loss: 0.03188080, Accuracy: 96.32%\n",
      "Iteration:  1078, Loss: 0.03184811, Accuracy: 96.38%\n",
      "Iteration:  1079, Loss: 0.03194426, Accuracy: 96.40%\n",
      "Iteration:  1080, Loss: 0.03179777, Accuracy: 96.36%\n",
      "Iteration:  1081, Loss: 0.03186123, Accuracy: 96.34%\n",
      "Iteration:  1082, Loss: 0.03186252, Accuracy: 96.35%\n",
      "Iteration:  1083, Loss: 0.03209803, Accuracy: 96.34%\n",
      "Iteration:  1084, Loss: 0.03193724, Accuracy: 96.35%\n",
      "Iteration:  1085, Loss: 0.03193614, Accuracy: 96.30%\n",
      "Iteration:  1086, Loss: 0.03187893, Accuracy: 96.36%\n",
      "Iteration:  1087, Loss: 0.03194476, Accuracy: 96.36%\n",
      "Iteration:  1088, Loss: 0.03157798, Accuracy: 96.36%\n",
      "Iteration:  1089, Loss: 0.03197006, Accuracy: 96.43%\n",
      "Iteration:  1090, Loss: 0.03172993, Accuracy: 96.35%\n",
      "Iteration:  1091, Loss: 0.03156506, Accuracy: 96.36%\n",
      "Iteration:  1092, Loss: 0.03130452, Accuracy: 96.34%\n",
      "Iteration:  1093, Loss: 0.03128931, Accuracy: 96.34%\n",
      "Iteration:  1094, Loss: 0.03100338, Accuracy: 96.34%\n",
      "Iteration:  1095, Loss: 0.03097118, Accuracy: 96.37%\n",
      "Iteration:  1096, Loss: 0.03091942, Accuracy: 96.36%\n",
      "Iteration:  1097, Loss: 0.03109228, Accuracy: 96.33%\n",
      "Iteration:  1098, Loss: 0.03070873, Accuracy: 96.40%\n",
      "Iteration:  1099, Loss: 0.03077723, Accuracy: 96.35%\n",
      "Iteration:  1100, Loss: 0.03055853, Accuracy: 96.40%\n",
      "Iteration:  1101, Loss: 0.03065191, Accuracy: 96.32%\n",
      "Iteration:  1102, Loss: 0.03049872, Accuracy: 96.39%\n",
      "Iteration:  1103, Loss: 0.03056752, Accuracy: 96.33%\n",
      "Iteration:  1104, Loss: 0.03051402, Accuracy: 96.39%\n",
      "Iteration:  1105, Loss: 0.03042657, Accuracy: 96.34%\n",
      "Iteration:  1106, Loss: 0.03046245, Accuracy: 96.40%\n",
      "Iteration:  1107, Loss: 0.03041370, Accuracy: 96.35%\n",
      "Iteration:  1108, Loss: 0.03049134, Accuracy: 96.40%\n",
      "Iteration:  1109, Loss: 0.03045987, Accuracy: 96.30%\n",
      "Iteration:  1110, Loss: 0.03036680, Accuracy: 96.44%\n",
      "Iteration:  1111, Loss: 0.03026975, Accuracy: 96.32%\n",
      "Iteration:  1112, Loss: 0.03026703, Accuracy: 96.43%\n",
      "Iteration:  1113, Loss: 0.03024674, Accuracy: 96.31%\n",
      "Iteration:  1114, Loss: 0.03021650, Accuracy: 96.45%\n",
      "Iteration:  1115, Loss: 0.03024027, Accuracy: 96.38%\n",
      "Iteration:  1116, Loss: 0.03032754, Accuracy: 96.41%\n",
      "Iteration:  1117, Loss: 0.03023426, Accuracy: 96.32%\n",
      "Iteration:  1118, Loss: 0.03018692, Accuracy: 96.39%\n",
      "Iteration:  1119, Loss: 0.03043416, Accuracy: 96.40%\n",
      "Iteration:  1120, Loss: 0.03030239, Accuracy: 96.36%\n",
      "Iteration:  1121, Loss: 0.03030067, Accuracy: 96.34%\n",
      "Iteration:  1122, Loss: 0.03033313, Accuracy: 96.38%\n",
      "Iteration:  1123, Loss: 0.03026829, Accuracy: 96.37%\n",
      "Iteration:  1124, Loss: 0.03022353, Accuracy: 96.39%\n",
      "Iteration:  1125, Loss: 0.03032810, Accuracy: 96.34%\n",
      "Iteration:  1126, Loss: 0.03003567, Accuracy: 96.38%\n",
      "Iteration:  1127, Loss: 0.03013971, Accuracy: 96.36%\n",
      "Iteration:  1128, Loss: 0.02995211, Accuracy: 96.40%\n",
      "Iteration:  1129, Loss: 0.03002614, Accuracy: 96.35%\n",
      "Iteration:  1130, Loss: 0.02985681, Accuracy: 96.39%\n",
      "Iteration:  1131, Loss: 0.02968158, Accuracy: 96.34%\n",
      "Iteration:  1132, Loss: 0.02964033, Accuracy: 96.37%\n",
      "Iteration:  1133, Loss: 0.02968747, Accuracy: 96.35%\n",
      "Iteration:  1134, Loss: 0.02967791, Accuracy: 96.40%\n",
      "Iteration:  1135, Loss: 0.02961708, Accuracy: 96.34%\n",
      "Iteration:  1136, Loss: 0.02956567, Accuracy: 96.37%\n",
      "Iteration:  1137, Loss: 0.02954444, Accuracy: 96.38%\n",
      "Iteration:  1138, Loss: 0.02950065, Accuracy: 96.37%\n",
      "Iteration:  1139, Loss: 0.02959663, Accuracy: 96.41%\n",
      "Iteration:  1140, Loss: 0.02956286, Accuracy: 96.36%\n",
      "Iteration:  1141, Loss: 0.02969705, Accuracy: 96.39%\n",
      "Iteration:  1142, Loss: 0.02952366, Accuracy: 96.38%\n",
      "Iteration:  1143, Loss: 0.02954249, Accuracy: 96.33%\n",
      "Iteration:  1144, Loss: 0.02937243, Accuracy: 96.40%\n",
      "Iteration:  1145, Loss: 0.02950292, Accuracy: 96.34%\n",
      "Iteration:  1146, Loss: 0.02943520, Accuracy: 96.37%\n",
      "Iteration:  1147, Loss: 0.02948463, Accuracy: 96.36%\n",
      "Iteration:  1148, Loss: 0.02938415, Accuracy: 96.40%\n",
      "Iteration:  1149, Loss: 0.02940901, Accuracy: 96.37%\n",
      "Iteration:  1150, Loss: 0.02931684, Accuracy: 96.35%\n",
      "Iteration:  1151, Loss: 0.02933630, Accuracy: 96.37%\n",
      "Iteration:  1152, Loss: 0.02934599, Accuracy: 96.39%\n",
      "Iteration:  1153, Loss: 0.02940927, Accuracy: 96.39%\n",
      "Iteration:  1154, Loss: 0.02953571, Accuracy: 96.43%\n",
      "Iteration:  1155, Loss: 0.02937757, Accuracy: 96.38%\n",
      "Iteration:  1156, Loss: 0.02931992, Accuracy: 96.43%\n",
      "Iteration:  1157, Loss: 0.02941179, Accuracy: 96.35%\n",
      "Iteration:  1158, Loss: 0.02930851, Accuracy: 96.40%\n",
      "Iteration:  1159, Loss: 0.02963428, Accuracy: 96.36%\n",
      "Iteration:  1160, Loss: 0.02935990, Accuracy: 96.38%\n",
      "Iteration:  1161, Loss: 0.02987654, Accuracy: 96.39%\n",
      "Iteration:  1162, Loss: 0.02951999, Accuracy: 96.35%\n",
      "Iteration:  1163, Loss: 0.02950904, Accuracy: 96.37%\n",
      "Iteration:  1164, Loss: 0.02969731, Accuracy: 96.41%\n",
      "Iteration:  1165, Loss: 0.02983331, Accuracy: 96.44%\n",
      "Iteration:  1166, Loss: 0.02961146, Accuracy: 96.39%\n",
      "Iteration:  1167, Loss: 0.02980442, Accuracy: 96.37%\n",
      "Iteration:  1168, Loss: 0.02955360, Accuracy: 96.38%\n",
      "Iteration:  1169, Loss: 0.02959145, Accuracy: 96.34%\n",
      "Iteration:  1170, Loss: 0.02918924, Accuracy: 96.40%\n",
      "Iteration:  1171, Loss: 0.02902546, Accuracy: 96.35%\n",
      "Iteration:  1172, Loss: 0.02897703, Accuracy: 96.37%\n",
      "Iteration:  1173, Loss: 0.02892267, Accuracy: 96.38%\n",
      "Iteration:  1174, Loss: 0.02881384, Accuracy: 96.39%\n",
      "Iteration:  1175, Loss: 0.02881192, Accuracy: 96.34%\n",
      "Iteration:  1176, Loss: 0.02853844, Accuracy: 96.42%\n",
      "Iteration:  1177, Loss: 0.02851812, Accuracy: 96.36%\n",
      "Iteration:  1178, Loss: 0.02844958, Accuracy: 96.37%\n",
      "Iteration:  1179, Loss: 0.02849201, Accuracy: 96.37%\n",
      "Iteration:  1180, Loss: 0.02847718, Accuracy: 96.39%\n",
      "Iteration:  1181, Loss: 0.02847659, Accuracy: 96.35%\n",
      "Iteration:  1182, Loss: 0.02838179, Accuracy: 96.37%\n",
      "Iteration:  1183, Loss: 0.02852135, Accuracy: 96.35%\n",
      "Iteration:  1184, Loss: 0.02835668, Accuracy: 96.37%\n",
      "Iteration:  1185, Loss: 0.02834199, Accuracy: 96.39%\n",
      "Iteration:  1186, Loss: 0.02828033, Accuracy: 96.37%\n",
      "Iteration:  1187, Loss: 0.02832299, Accuracy: 96.36%\n",
      "Iteration:  1188, Loss: 0.02825512, Accuracy: 96.36%\n",
      "Iteration:  1189, Loss: 0.02822751, Accuracy: 96.39%\n",
      "Iteration:  1190, Loss: 0.02828701, Accuracy: 96.35%\n",
      "Iteration:  1191, Loss: 0.02820440, Accuracy: 96.38%\n",
      "Iteration:  1192, Loss: 0.02814291, Accuracy: 96.32%\n",
      "Iteration:  1193, Loss: 0.02812505, Accuracy: 96.36%\n",
      "Iteration:  1194, Loss: 0.02820334, Accuracy: 96.34%\n",
      "Iteration:  1195, Loss: 0.02845069, Accuracy: 96.36%\n",
      "Iteration:  1196, Loss: 0.02832757, Accuracy: 96.36%\n",
      "Iteration:  1197, Loss: 0.02843517, Accuracy: 96.38%\n",
      "Iteration:  1198, Loss: 0.02814672, Accuracy: 96.39%\n",
      "Iteration:  1199, Loss: 0.02827516, Accuracy: 96.40%\n",
      "Iteration:  1200, Loss: 0.02820830, Accuracy: 96.37%\n",
      "Iteration:  1201, Loss: 0.02843318, Accuracy: 96.41%\n",
      "Iteration:  1202, Loss: 0.02828437, Accuracy: 96.39%\n",
      "Iteration:  1203, Loss: 0.02848536, Accuracy: 96.40%\n",
      "Iteration:  1204, Loss: 0.02864481, Accuracy: 96.40%\n",
      "Iteration:  1205, Loss: 0.02846748, Accuracy: 96.36%\n",
      "Iteration:  1206, Loss: 0.02830679, Accuracy: 96.38%\n",
      "Iteration:  1207, Loss: 0.02850540, Accuracy: 96.37%\n",
      "Iteration:  1208, Loss: 0.02837999, Accuracy: 96.40%\n",
      "Iteration:  1209, Loss: 0.02839599, Accuracy: 96.43%\n",
      "Iteration:  1210, Loss: 0.02820763, Accuracy: 96.36%\n",
      "Iteration:  1211, Loss: 0.02830963, Accuracy: 96.40%\n",
      "Iteration:  1212, Loss: 0.02801693, Accuracy: 96.38%\n",
      "Iteration:  1213, Loss: 0.02805468, Accuracy: 96.36%\n",
      "Iteration:  1214, Loss: 0.02819888, Accuracy: 96.33%\n",
      "Iteration:  1215, Loss: 0.02835933, Accuracy: 96.35%\n",
      "Iteration:  1216, Loss: 0.02824509, Accuracy: 96.34%\n",
      "Iteration:  1217, Loss: 0.02802511, Accuracy: 96.37%\n",
      "Iteration:  1218, Loss: 0.02779243, Accuracy: 96.36%\n",
      "Iteration:  1219, Loss: 0.02771729, Accuracy: 96.39%\n",
      "Iteration:  1220, Loss: 0.02768869, Accuracy: 96.36%\n",
      "Iteration:  1221, Loss: 0.02774534, Accuracy: 96.39%\n",
      "Iteration:  1222, Loss: 0.02760461, Accuracy: 96.37%\n",
      "Iteration:  1223, Loss: 0.02773596, Accuracy: 96.38%\n",
      "Iteration:  1224, Loss: 0.02774903, Accuracy: 96.38%\n",
      "Iteration:  1225, Loss: 0.02752764, Accuracy: 96.33%\n",
      "Iteration:  1226, Loss: 0.02751126, Accuracy: 96.33%\n",
      "Iteration:  1227, Loss: 0.02746640, Accuracy: 96.40%\n",
      "Iteration:  1228, Loss: 0.02741097, Accuracy: 96.34%\n",
      "Iteration:  1229, Loss: 0.02732268, Accuracy: 96.37%\n",
      "Iteration:  1230, Loss: 0.02731003, Accuracy: 96.36%\n",
      "Iteration:  1231, Loss: 0.02738586, Accuracy: 96.35%\n",
      "Iteration:  1232, Loss: 0.02726079, Accuracy: 96.36%\n",
      "Iteration:  1233, Loss: 0.02739616, Accuracy: 96.37%\n",
      "Iteration:  1234, Loss: 0.02722474, Accuracy: 96.36%\n",
      "Iteration:  1235, Loss: 0.02717595, Accuracy: 96.37%\n",
      "Iteration:  1236, Loss: 0.02723025, Accuracy: 96.38%\n",
      "Iteration:  1237, Loss: 0.02719685, Accuracy: 96.37%\n",
      "Iteration:  1238, Loss: 0.02710650, Accuracy: 96.36%\n",
      "Iteration:  1239, Loss: 0.02714553, Accuracy: 96.38%\n",
      "Iteration:  1240, Loss: 0.02708378, Accuracy: 96.36%\n",
      "Iteration:  1241, Loss: 0.02706591, Accuracy: 96.39%\n",
      "Iteration:  1242, Loss: 0.02709441, Accuracy: 96.36%\n",
      "Iteration:  1243, Loss: 0.02719515, Accuracy: 96.39%\n",
      "Iteration:  1244, Loss: 0.02705570, Accuracy: 96.35%\n",
      "Iteration:  1245, Loss: 0.02695116, Accuracy: 96.38%\n",
      "Iteration:  1246, Loss: 0.02700527, Accuracy: 96.37%\n",
      "Iteration:  1247, Loss: 0.02711245, Accuracy: 96.39%\n",
      "Iteration:  1248, Loss: 0.02704506, Accuracy: 96.41%\n",
      "Iteration:  1249, Loss: 0.02707856, Accuracy: 96.39%\n",
      "Iteration:  1250, Loss: 0.02694369, Accuracy: 96.41%\n",
      "Iteration:  1251, Loss: 0.02687430, Accuracy: 96.41%\n",
      "Iteration:  1252, Loss: 0.02682576, Accuracy: 96.40%\n",
      "Iteration:  1253, Loss: 0.02678087, Accuracy: 96.40%\n",
      "Iteration:  1254, Loss: 0.02674407, Accuracy: 96.41%\n",
      "Iteration:  1255, Loss: 0.02676070, Accuracy: 96.42%\n",
      "Iteration:  1256, Loss: 0.02668966, Accuracy: 96.40%\n",
      "Iteration:  1257, Loss: 0.02668792, Accuracy: 96.40%\n",
      "Iteration:  1258, Loss: 0.02679558, Accuracy: 96.38%\n",
      "Iteration:  1259, Loss: 0.02666346, Accuracy: 96.39%\n",
      "Iteration:  1260, Loss: 0.02664170, Accuracy: 96.41%\n",
      "Iteration:  1261, Loss: 0.02670991, Accuracy: 96.42%\n",
      "Iteration:  1262, Loss: 0.02657023, Accuracy: 96.39%\n",
      "Iteration:  1263, Loss: 0.02665478, Accuracy: 96.42%\n",
      "Iteration:  1264, Loss: 0.02654213, Accuracy: 96.41%\n",
      "Iteration:  1265, Loss: 0.02654504, Accuracy: 96.42%\n",
      "Iteration:  1266, Loss: 0.02650550, Accuracy: 96.43%\n",
      "Iteration:  1267, Loss: 0.02646720, Accuracy: 96.40%\n",
      "Iteration:  1268, Loss: 0.02647720, Accuracy: 96.42%\n",
      "Iteration:  1269, Loss: 0.02657581, Accuracy: 96.38%\n",
      "Iteration:  1270, Loss: 0.02647630, Accuracy: 96.45%\n",
      "Iteration:  1271, Loss: 0.02640850, Accuracy: 96.42%\n",
      "Iteration:  1272, Loss: 0.02641798, Accuracy: 96.44%\n",
      "Iteration:  1273, Loss: 0.02637109, Accuracy: 96.41%\n",
      "Iteration:  1274, Loss: 0.02637442, Accuracy: 96.45%\n",
      "Iteration:  1275, Loss: 0.02643228, Accuracy: 96.40%\n",
      "Iteration:  1276, Loss: 0.02656166, Accuracy: 96.45%\n",
      "Iteration:  1277, Loss: 0.02641171, Accuracy: 96.39%\n",
      "Iteration:  1278, Loss: 0.02638329, Accuracy: 96.46%\n",
      "Iteration:  1279, Loss: 0.02634393, Accuracy: 96.40%\n",
      "Iteration:  1280, Loss: 0.02630883, Accuracy: 96.44%\n",
      "Iteration:  1281, Loss: 0.02638046, Accuracy: 96.41%\n",
      "Iteration:  1282, Loss: 0.02627038, Accuracy: 96.47%\n",
      "Iteration:  1283, Loss: 0.02623373, Accuracy: 96.39%\n",
      "Iteration:  1284, Loss: 0.02616435, Accuracy: 96.43%\n",
      "Iteration:  1285, Loss: 0.02617142, Accuracy: 96.39%\n",
      "Iteration:  1286, Loss: 0.02623034, Accuracy: 96.42%\n",
      "Iteration:  1287, Loss: 0.02644574, Accuracy: 96.43%\n",
      "Iteration:  1288, Loss: 0.02622546, Accuracy: 96.45%\n",
      "Iteration:  1289, Loss: 0.02641897, Accuracy: 96.44%\n",
      "Iteration:  1290, Loss: 0.02634179, Accuracy: 96.43%\n",
      "Iteration:  1291, Loss: 0.02631320, Accuracy: 96.39%\n",
      "Iteration:  1292, Loss: 0.02620450, Accuracy: 96.46%\n",
      "Iteration:  1293, Loss: 0.02621333, Accuracy: 96.39%\n",
      "Iteration:  1294, Loss: 0.02619029, Accuracy: 96.46%\n",
      "Iteration:  1295, Loss: 0.02632282, Accuracy: 96.40%\n",
      "Iteration:  1296, Loss: 0.02619070, Accuracy: 96.43%\n",
      "Iteration:  1297, Loss: 0.02636588, Accuracy: 96.44%\n",
      "Iteration:  1298, Loss: 0.02626030, Accuracy: 96.45%\n",
      "Iteration:  1299, Loss: 0.02624404, Accuracy: 96.43%\n",
      "Iteration:  1300, Loss: 0.02633574, Accuracy: 96.43%\n",
      "Iteration:  1301, Loss: 0.02637007, Accuracy: 96.44%\n",
      "Iteration:  1302, Loss: 0.02639678, Accuracy: 96.46%\n",
      "Iteration:  1303, Loss: 0.02660991, Accuracy: 96.44%\n",
      "Iteration:  1304, Loss: 0.02626495, Accuracy: 96.44%\n",
      "Iteration:  1305, Loss: 0.02638257, Accuracy: 96.46%\n",
      "Iteration:  1306, Loss: 0.02630122, Accuracy: 96.43%\n",
      "Iteration:  1307, Loss: 0.02624530, Accuracy: 96.44%\n",
      "Iteration:  1308, Loss: 0.02597010, Accuracy: 96.41%\n",
      "Iteration:  1309, Loss: 0.02586126, Accuracy: 96.41%\n",
      "Iteration:  1310, Loss: 0.02583888, Accuracy: 96.42%\n",
      "Iteration:  1311, Loss: 0.02577468, Accuracy: 96.38%\n",
      "Iteration:  1312, Loss: 0.02575681, Accuracy: 96.38%\n",
      "Iteration:  1313, Loss: 0.02571851, Accuracy: 96.42%\n",
      "Iteration:  1314, Loss: 0.02563454, Accuracy: 96.43%\n",
      "Iteration:  1315, Loss: 0.02561901, Accuracy: 96.42%\n",
      "Iteration:  1316, Loss: 0.02556639, Accuracy: 96.42%\n",
      "Iteration:  1317, Loss: 0.02549940, Accuracy: 96.42%\n",
      "Iteration:  1318, Loss: 0.02547271, Accuracy: 96.43%\n",
      "Iteration:  1319, Loss: 0.02546309, Accuracy: 96.42%\n",
      "Iteration:  1320, Loss: 0.02548250, Accuracy: 96.42%\n",
      "Iteration:  1321, Loss: 0.02548036, Accuracy: 96.42%\n",
      "Iteration:  1322, Loss: 0.02541120, Accuracy: 96.41%\n",
      "Iteration:  1323, Loss: 0.02542658, Accuracy: 96.41%\n",
      "Iteration:  1324, Loss: 0.02553406, Accuracy: 96.41%\n",
      "Iteration:  1325, Loss: 0.02549296, Accuracy: 96.40%\n",
      "Iteration:  1326, Loss: 0.02545131, Accuracy: 96.44%\n",
      "Iteration:  1327, Loss: 0.02548145, Accuracy: 96.40%\n",
      "Iteration:  1328, Loss: 0.02542919, Accuracy: 96.40%\n",
      "Iteration:  1329, Loss: 0.02533433, Accuracy: 96.43%\n",
      "Iteration:  1330, Loss: 0.02531758, Accuracy: 96.42%\n",
      "Iteration:  1331, Loss: 0.02541341, Accuracy: 96.44%\n",
      "Iteration:  1332, Loss: 0.02527710, Accuracy: 96.41%\n",
      "Iteration:  1333, Loss: 0.02522965, Accuracy: 96.43%\n",
      "Iteration:  1334, Loss: 0.02522007, Accuracy: 96.42%\n",
      "Iteration:  1335, Loss: 0.02522112, Accuracy: 96.40%\n",
      "Iteration:  1336, Loss: 0.02523436, Accuracy: 96.42%\n",
      "Iteration:  1337, Loss: 0.02526757, Accuracy: 96.45%\n",
      "Iteration:  1338, Loss: 0.02515761, Accuracy: 96.43%\n",
      "Iteration:  1339, Loss: 0.02512806, Accuracy: 96.43%\n",
      "Iteration:  1340, Loss: 0.02508218, Accuracy: 96.42%\n",
      "Iteration:  1341, Loss: 0.02508441, Accuracy: 96.44%\n",
      "Iteration:  1342, Loss: 0.02509419, Accuracy: 96.42%\n",
      "Iteration:  1343, Loss: 0.02515074, Accuracy: 96.47%\n",
      "Iteration:  1344, Loss: 0.02512412, Accuracy: 96.44%\n",
      "Iteration:  1345, Loss: 0.02503874, Accuracy: 96.43%\n",
      "Iteration:  1346, Loss: 0.02507415, Accuracy: 96.42%\n",
      "Iteration:  1347, Loss: 0.02513312, Accuracy: 96.45%\n",
      "Iteration:  1348, Loss: 0.02508282, Accuracy: 96.41%\n",
      "Iteration:  1349, Loss: 0.02511750, Accuracy: 96.45%\n",
      "Iteration:  1350, Loss: 0.02499631, Accuracy: 96.41%\n",
      "Iteration:  1351, Loss: 0.02503763, Accuracy: 96.43%\n",
      "Iteration:  1352, Loss: 0.02498586, Accuracy: 96.41%\n",
      "Iteration:  1353, Loss: 0.02491659, Accuracy: 96.44%\n",
      "Iteration:  1354, Loss: 0.02492691, Accuracy: 96.44%\n",
      "Iteration:  1355, Loss: 0.02502692, Accuracy: 96.43%\n",
      "Iteration:  1356, Loss: 0.02496223, Accuracy: 96.43%\n",
      "Iteration:  1357, Loss: 0.02501004, Accuracy: 96.42%\n",
      "Iteration:  1358, Loss: 0.02489626, Accuracy: 96.43%\n",
      "Iteration:  1359, Loss: 0.02484307, Accuracy: 96.44%\n",
      "Iteration:  1360, Loss: 0.02481774, Accuracy: 96.43%\n",
      "Iteration:  1361, Loss: 0.02487975, Accuracy: 96.46%\n",
      "Iteration:  1362, Loss: 0.02489577, Accuracy: 96.46%\n",
      "Iteration:  1363, Loss: 0.02486120, Accuracy: 96.45%\n",
      "Iteration:  1364, Loss: 0.02501382, Accuracy: 96.45%\n",
      "Iteration:  1365, Loss: 0.02474445, Accuracy: 96.43%\n",
      "Iteration:  1366, Loss: 0.02473881, Accuracy: 96.45%\n",
      "Iteration:  1367, Loss: 0.02480704, Accuracy: 96.44%\n",
      "Iteration:  1368, Loss: 0.02483321, Accuracy: 96.46%\n",
      "Iteration:  1369, Loss: 0.02472179, Accuracy: 96.42%\n",
      "Iteration:  1370, Loss: 0.02483241, Accuracy: 96.46%\n",
      "Iteration:  1371, Loss: 0.02471518, Accuracy: 96.46%\n",
      "Iteration:  1372, Loss: 0.02458391, Accuracy: 96.47%\n",
      "Iteration:  1373, Loss: 0.02470177, Accuracy: 96.46%\n",
      "Iteration:  1374, Loss: 0.02468180, Accuracy: 96.47%\n",
      "Iteration:  1375, Loss: 0.02459461, Accuracy: 96.45%\n",
      "Iteration:  1376, Loss: 0.02457346, Accuracy: 96.47%\n",
      "Iteration:  1377, Loss: 0.02467223, Accuracy: 96.45%\n",
      "Iteration:  1378, Loss: 0.02465988, Accuracy: 96.45%\n",
      "Iteration:  1379, Loss: 0.02455589, Accuracy: 96.45%\n",
      "Iteration:  1380, Loss: 0.02444316, Accuracy: 96.48%\n",
      "Iteration:  1381, Loss: 0.02443950, Accuracy: 96.46%\n",
      "Iteration:  1382, Loss: 0.02441857, Accuracy: 96.48%\n",
      "Iteration:  1383, Loss: 0.02444711, Accuracy: 96.44%\n",
      "Iteration:  1384, Loss: 0.02448509, Accuracy: 96.48%\n",
      "Iteration:  1385, Loss: 0.02454547, Accuracy: 96.44%\n",
      "Iteration:  1386, Loss: 0.02443856, Accuracy: 96.48%\n",
      "Iteration:  1387, Loss: 0.02437110, Accuracy: 96.45%\n",
      "Iteration:  1388, Loss: 0.02430447, Accuracy: 96.48%\n",
      "Iteration:  1389, Loss: 0.02436515, Accuracy: 96.45%\n",
      "Iteration:  1390, Loss: 0.02430281, Accuracy: 96.48%\n",
      "Iteration:  1391, Loss: 0.02437349, Accuracy: 96.44%\n",
      "Iteration:  1392, Loss: 0.02429977, Accuracy: 96.46%\n",
      "Iteration:  1393, Loss: 0.02425729, Accuracy: 96.46%\n",
      "Iteration:  1394, Loss: 0.02420819, Accuracy: 96.47%\n",
      "Iteration:  1395, Loss: 0.02415164, Accuracy: 96.47%\n",
      "Iteration:  1396, Loss: 0.02414535, Accuracy: 96.46%\n",
      "Iteration:  1397, Loss: 0.02413615, Accuracy: 96.46%\n",
      "Iteration:  1398, Loss: 0.02419131, Accuracy: 96.45%\n",
      "Iteration:  1399, Loss: 0.02411628, Accuracy: 96.45%\n",
      "Iteration:  1400, Loss: 0.02404740, Accuracy: 96.47%\n",
      "Iteration:  1401, Loss: 0.02403158, Accuracy: 96.46%\n",
      "Iteration:  1402, Loss: 0.02401939, Accuracy: 96.45%\n",
      "Iteration:  1403, Loss: 0.02403090, Accuracy: 96.46%\n",
      "Iteration:  1404, Loss: 0.02401760, Accuracy: 96.45%\n",
      "Iteration:  1405, Loss: 0.02412056, Accuracy: 96.45%\n",
      "Iteration:  1406, Loss: 0.02399969, Accuracy: 96.45%\n",
      "Iteration:  1407, Loss: 0.02400258, Accuracy: 96.46%\n",
      "Iteration:  1408, Loss: 0.02397758, Accuracy: 96.46%\n",
      "Iteration:  1409, Loss: 0.02393602, Accuracy: 96.47%\n",
      "Iteration:  1410, Loss: 0.02398057, Accuracy: 96.48%\n",
      "Iteration:  1411, Loss: 0.02404324, Accuracy: 96.49%\n",
      "Iteration:  1412, Loss: 0.02409747, Accuracy: 96.46%\n",
      "Iteration:  1413, Loss: 0.02395722, Accuracy: 96.50%\n",
      "Iteration:  1414, Loss: 0.02390845, Accuracy: 96.46%\n",
      "Iteration:  1415, Loss: 0.02386343, Accuracy: 96.49%\n",
      "Iteration:  1416, Loss: 0.02383736, Accuracy: 96.46%\n",
      "Iteration:  1417, Loss: 0.02387233, Accuracy: 96.47%\n",
      "Iteration:  1418, Loss: 0.02381020, Accuracy: 96.46%\n",
      "Iteration:  1419, Loss: 0.02384731, Accuracy: 96.48%\n",
      "Iteration:  1420, Loss: 0.02391664, Accuracy: 96.47%\n",
      "Iteration:  1421, Loss: 0.02388354, Accuracy: 96.50%\n",
      "Iteration:  1422, Loss: 0.02383882, Accuracy: 96.47%\n",
      "Iteration:  1423, Loss: 0.02379966, Accuracy: 96.51%\n",
      "Iteration:  1424, Loss: 0.02381464, Accuracy: 96.48%\n",
      "Iteration:  1425, Loss: 0.02375362, Accuracy: 96.49%\n",
      "Iteration:  1426, Loss: 0.02369018, Accuracy: 96.48%\n",
      "Iteration:  1427, Loss: 0.02367926, Accuracy: 96.47%\n",
      "Iteration:  1428, Loss: 0.02371583, Accuracy: 96.51%\n",
      "Iteration:  1429, Loss: 0.02365655, Accuracy: 96.47%\n",
      "Iteration:  1430, Loss: 0.02371058, Accuracy: 96.50%\n",
      "Iteration:  1431, Loss: 0.02366028, Accuracy: 96.48%\n",
      "Iteration:  1432, Loss: 0.02371426, Accuracy: 96.47%\n",
      "Iteration:  1433, Loss: 0.02371362, Accuracy: 96.46%\n",
      "Iteration:  1434, Loss: 0.02354399, Accuracy: 96.50%\n",
      "Iteration:  1435, Loss: 0.02353343, Accuracy: 96.49%\n",
      "Iteration:  1436, Loss: 0.02351799, Accuracy: 96.48%\n",
      "Iteration:  1437, Loss: 0.02350926, Accuracy: 96.49%\n",
      "Iteration:  1438, Loss: 0.02349565, Accuracy: 96.47%\n",
      "Iteration:  1439, Loss: 0.02348751, Accuracy: 96.49%\n",
      "Iteration:  1440, Loss: 0.02351444, Accuracy: 96.47%\n",
      "Iteration:  1441, Loss: 0.02347710, Accuracy: 96.52%\n",
      "Iteration:  1442, Loss: 0.02346616, Accuracy: 96.48%\n",
      "Iteration:  1443, Loss: 0.02356853, Accuracy: 96.51%\n",
      "Iteration:  1444, Loss: 0.02365369, Accuracy: 96.50%\n",
      "Iteration:  1445, Loss: 0.02374399, Accuracy: 96.47%\n",
      "Iteration:  1446, Loss: 0.02345791, Accuracy: 96.49%\n",
      "Iteration:  1447, Loss: 0.02336855, Accuracy: 96.47%\n",
      "Iteration:  1448, Loss: 0.02341448, Accuracy: 96.49%\n",
      "Iteration:  1449, Loss: 0.02349825, Accuracy: 96.52%\n",
      "Iteration:  1450, Loss: 0.02346002, Accuracy: 96.49%\n",
      "Iteration:  1451, Loss: 0.02339480, Accuracy: 96.49%\n",
      "Iteration:  1452, Loss: 0.02340103, Accuracy: 96.45%\n",
      "Iteration:  1453, Loss: 0.02338971, Accuracy: 96.51%\n",
      "Iteration:  1454, Loss: 0.02325861, Accuracy: 96.49%\n",
      "Iteration:  1455, Loss: 0.02322847, Accuracy: 96.50%\n",
      "Iteration:  1456, Loss: 0.02321116, Accuracy: 96.49%\n",
      "Iteration:  1457, Loss: 0.02322491, Accuracy: 96.53%\n",
      "Iteration:  1458, Loss: 0.02320551, Accuracy: 96.48%\n",
      "Iteration:  1459, Loss: 0.02322623, Accuracy: 96.53%\n",
      "Iteration:  1460, Loss: 0.02314334, Accuracy: 96.50%\n",
      "Iteration:  1461, Loss: 0.02313155, Accuracy: 96.50%\n",
      "Iteration:  1462, Loss: 0.02312206, Accuracy: 96.50%\n",
      "Iteration:  1463, Loss: 0.02310704, Accuracy: 96.50%\n",
      "Iteration:  1464, Loss: 0.02311810, Accuracy: 96.51%\n",
      "Iteration:  1465, Loss: 0.02311660, Accuracy: 96.51%\n",
      "Iteration:  1466, Loss: 0.02320421, Accuracy: 96.51%\n",
      "Iteration:  1467, Loss: 0.02305657, Accuracy: 96.50%\n",
      "Iteration:  1468, Loss: 0.02305567, Accuracy: 96.52%\n",
      "Iteration:  1469, Loss: 0.02302049, Accuracy: 96.51%\n",
      "Iteration:  1470, Loss: 0.02302204, Accuracy: 96.51%\n",
      "Iteration:  1471, Loss: 0.02298755, Accuracy: 96.51%\n",
      "Iteration:  1472, Loss: 0.02301277, Accuracy: 96.49%\n",
      "Iteration:  1473, Loss: 0.02310782, Accuracy: 96.49%\n",
      "Iteration:  1474, Loss: 0.02317612, Accuracy: 96.48%\n",
      "Iteration:  1475, Loss: 0.02310750, Accuracy: 96.46%\n",
      "Iteration:  1476, Loss: 0.02305948, Accuracy: 96.50%\n",
      "Iteration:  1477, Loss: 0.02304990, Accuracy: 96.47%\n",
      "Iteration:  1478, Loss: 0.02312942, Accuracy: 96.51%\n",
      "Iteration:  1479, Loss: 0.02297950, Accuracy: 96.50%\n",
      "Iteration:  1480, Loss: 0.02294617, Accuracy: 96.50%\n",
      "Iteration:  1481, Loss: 0.02297054, Accuracy: 96.49%\n",
      "Iteration:  1482, Loss: 0.02289956, Accuracy: 96.48%\n",
      "Iteration:  1483, Loss: 0.02290353, Accuracy: 96.48%\n",
      "Iteration:  1484, Loss: 0.02290121, Accuracy: 96.48%\n",
      "Iteration:  1485, Loss: 0.02284366, Accuracy: 96.48%\n",
      "Iteration:  1486, Loss: 0.02277400, Accuracy: 96.51%\n",
      "Iteration:  1487, Loss: 0.02276726, Accuracy: 96.49%\n",
      "Iteration:  1488, Loss: 0.02276240, Accuracy: 96.50%\n",
      "Iteration:  1489, Loss: 0.02277963, Accuracy: 96.49%\n",
      "Iteration:  1490, Loss: 0.02280691, Accuracy: 96.48%\n",
      "Iteration:  1491, Loss: 0.02280389, Accuracy: 96.48%\n",
      "Iteration:  1492, Loss: 0.02271552, Accuracy: 96.50%\n",
      "Iteration:  1493, Loss: 0.02268957, Accuracy: 96.49%\n",
      "Iteration:  1494, Loss: 0.02265061, Accuracy: 96.49%\n",
      "Iteration:  1495, Loss: 0.02265468, Accuracy: 96.49%\n",
      "Iteration:  1496, Loss: 0.02264821, Accuracy: 96.50%\n",
      "Iteration:  1497, Loss: 0.02273980, Accuracy: 96.46%\n",
      "Iteration:  1498, Loss: 0.02276789, Accuracy: 96.52%\n",
      "Iteration:  1499, Loss: 0.02283260, Accuracy: 96.50%\n",
      "Iteration:  1500, Loss: 0.02266551, Accuracy: 96.49%\n",
      "Iteration:  1501, Loss: 0.02262073, Accuracy: 96.50%\n",
      "Iteration:  1502, Loss: 0.02255260, Accuracy: 96.47%\n",
      "Iteration:  1503, Loss: 0.02254963, Accuracy: 96.49%\n",
      "Iteration:  1504, Loss: 0.02250796, Accuracy: 96.50%\n",
      "Iteration:  1505, Loss: 0.02252552, Accuracy: 96.48%\n",
      "Iteration:  1506, Loss: 0.02247968, Accuracy: 96.51%\n",
      "Iteration:  1507, Loss: 0.02248933, Accuracy: 96.49%\n",
      "Iteration:  1508, Loss: 0.02249865, Accuracy: 96.50%\n",
      "Iteration:  1509, Loss: 0.02248465, Accuracy: 96.50%\n",
      "Iteration:  1510, Loss: 0.02245779, Accuracy: 96.50%\n",
      "Iteration:  1511, Loss: 0.02244387, Accuracy: 96.51%\n",
      "Iteration:  1512, Loss: 0.02252362, Accuracy: 96.48%\n",
      "Iteration:  1513, Loss: 0.02243362, Accuracy: 96.48%\n",
      "Iteration:  1514, Loss: 0.02242017, Accuracy: 96.51%\n",
      "Iteration:  1515, Loss: 0.02235962, Accuracy: 96.50%\n",
      "Iteration:  1516, Loss: 0.02237223, Accuracy: 96.50%\n",
      "Iteration:  1517, Loss: 0.02243088, Accuracy: 96.49%\n",
      "Iteration:  1518, Loss: 0.02234200, Accuracy: 96.47%\n",
      "Iteration:  1519, Loss: 0.02237427, Accuracy: 96.49%\n",
      "Iteration:  1520, Loss: 0.02242051, Accuracy: 96.49%\n",
      "Iteration:  1521, Loss: 0.02243211, Accuracy: 96.49%\n",
      "Iteration:  1522, Loss: 0.02245588, Accuracy: 96.52%\n",
      "Iteration:  1523, Loss: 0.02241826, Accuracy: 96.46%\n",
      "Iteration:  1524, Loss: 0.02245466, Accuracy: 96.50%\n",
      "Iteration:  1525, Loss: 0.02225726, Accuracy: 96.47%\n",
      "Iteration:  1526, Loss: 0.02224066, Accuracy: 96.51%\n",
      "Iteration:  1527, Loss: 0.02229703, Accuracy: 96.48%\n",
      "Iteration:  1528, Loss: 0.02227102, Accuracy: 96.50%\n",
      "Iteration:  1529, Loss: 0.02234562, Accuracy: 96.45%\n",
      "Iteration:  1530, Loss: 0.02230623, Accuracy: 96.52%\n",
      "Iteration:  1531, Loss: 0.02220345, Accuracy: 96.48%\n",
      "Iteration:  1532, Loss: 0.02224447, Accuracy: 96.53%\n",
      "Iteration:  1533, Loss: 0.02216226, Accuracy: 96.47%\n",
      "Iteration:  1534, Loss: 0.02224125, Accuracy: 96.54%\n",
      "Iteration:  1535, Loss: 0.02213590, Accuracy: 96.52%\n",
      "Iteration:  1536, Loss: 0.02213965, Accuracy: 96.53%\n",
      "Iteration:  1537, Loss: 0.02214594, Accuracy: 96.50%\n",
      "Iteration:  1538, Loss: 0.02220185, Accuracy: 96.50%\n",
      "Iteration:  1539, Loss: 0.02212241, Accuracy: 96.50%\n",
      "Iteration:  1540, Loss: 0.02205426, Accuracy: 96.50%\n",
      "Iteration:  1541, Loss: 0.02217050, Accuracy: 96.47%\n",
      "Iteration:  1542, Loss: 0.02202158, Accuracy: 96.50%\n",
      "Iteration:  1543, Loss: 0.02197961, Accuracy: 96.49%\n",
      "Iteration:  1544, Loss: 0.02202357, Accuracy: 96.48%\n",
      "Iteration:  1545, Loss: 0.02204371, Accuracy: 96.49%\n",
      "Iteration:  1546, Loss: 0.02194200, Accuracy: 96.50%\n",
      "Iteration:  1547, Loss: 0.02200127, Accuracy: 96.49%\n",
      "Iteration:  1548, Loss: 0.02195273, Accuracy: 96.49%\n",
      "Iteration:  1549, Loss: 0.02187560, Accuracy: 96.49%\n",
      "Iteration:  1550, Loss: 0.02188819, Accuracy: 96.50%\n",
      "Iteration:  1551, Loss: 0.02185503, Accuracy: 96.49%\n",
      "Iteration:  1552, Loss: 0.02188482, Accuracy: 96.50%\n",
      "Iteration:  1553, Loss: 0.02192905, Accuracy: 96.49%\n",
      "Iteration:  1554, Loss: 0.02198111, Accuracy: 96.48%\n",
      "Iteration:  1555, Loss: 0.02189031, Accuracy: 96.48%\n",
      "Iteration:  1556, Loss: 0.02179393, Accuracy: 96.49%\n",
      "Iteration:  1557, Loss: 0.02180733, Accuracy: 96.49%\n",
      "Iteration:  1558, Loss: 0.02185224, Accuracy: 96.50%\n",
      "Iteration:  1559, Loss: 0.02178381, Accuracy: 96.46%\n",
      "Iteration:  1560, Loss: 0.02180926, Accuracy: 96.50%\n",
      "Iteration:  1561, Loss: 0.02176627, Accuracy: 96.46%\n",
      "Iteration:  1562, Loss: 0.02170566, Accuracy: 96.51%\n",
      "Iteration:  1563, Loss: 0.02171028, Accuracy: 96.46%\n",
      "Iteration:  1564, Loss: 0.02171388, Accuracy: 96.50%\n",
      "Iteration:  1565, Loss: 0.02181652, Accuracy: 96.47%\n",
      "Iteration:  1566, Loss: 0.02172045, Accuracy: 96.52%\n",
      "Iteration:  1567, Loss: 0.02170294, Accuracy: 96.45%\n",
      "Iteration:  1568, Loss: 0.02174213, Accuracy: 96.50%\n",
      "Iteration:  1569, Loss: 0.02174640, Accuracy: 96.45%\n",
      "Iteration:  1570, Loss: 0.02185764, Accuracy: 96.53%\n",
      "Iteration:  1571, Loss: 0.02170673, Accuracy: 96.46%\n",
      "Iteration:  1572, Loss: 0.02169341, Accuracy: 96.49%\n",
      "Iteration:  1573, Loss: 0.02165507, Accuracy: 96.49%\n",
      "Iteration:  1574, Loss: 0.02158937, Accuracy: 96.49%\n",
      "Iteration:  1575, Loss: 0.02154027, Accuracy: 96.48%\n",
      "Iteration:  1576, Loss: 0.02159641, Accuracy: 96.51%\n",
      "Iteration:  1577, Loss: 0.02150744, Accuracy: 96.50%\n",
      "Iteration:  1578, Loss: 0.02152494, Accuracy: 96.50%\n",
      "Iteration:  1579, Loss: 0.02146506, Accuracy: 96.49%\n",
      "Iteration:  1580, Loss: 0.02146657, Accuracy: 96.50%\n",
      "Iteration:  1581, Loss: 0.02149905, Accuracy: 96.46%\n",
      "Iteration:  1582, Loss: 0.02143633, Accuracy: 96.47%\n",
      "Iteration:  1583, Loss: 0.02142995, Accuracy: 96.48%\n",
      "Iteration:  1584, Loss: 0.02144280, Accuracy: 96.49%\n",
      "Iteration:  1585, Loss: 0.02143707, Accuracy: 96.45%\n",
      "Iteration:  1586, Loss: 0.02156403, Accuracy: 96.45%\n",
      "Iteration:  1587, Loss: 0.02144231, Accuracy: 96.50%\n",
      "Iteration:  1588, Loss: 0.02141946, Accuracy: 96.48%\n",
      "Iteration:  1589, Loss: 0.02138779, Accuracy: 96.49%\n",
      "Iteration:  1590, Loss: 0.02135977, Accuracy: 96.49%\n",
      "Iteration:  1591, Loss: 0.02136513, Accuracy: 96.47%\n",
      "Iteration:  1592, Loss: 0.02141037, Accuracy: 96.55%\n",
      "Iteration:  1593, Loss: 0.02134942, Accuracy: 96.47%\n",
      "Iteration:  1594, Loss: 0.02153398, Accuracy: 96.51%\n",
      "Iteration:  1595, Loss: 0.02144733, Accuracy: 96.43%\n",
      "Iteration:  1596, Loss: 0.02135879, Accuracy: 96.50%\n",
      "Iteration:  1597, Loss: 0.02133099, Accuracy: 96.47%\n",
      "Iteration:  1598, Loss: 0.02140850, Accuracy: 96.53%\n",
      "Iteration:  1599, Loss: 0.02134212, Accuracy: 96.49%\n",
      "Iteration:  1600, Loss: 0.02135882, Accuracy: 96.52%\n",
      "Iteration:  1601, Loss: 0.02124334, Accuracy: 96.49%\n",
      "Iteration:  1602, Loss: 0.02127556, Accuracy: 96.56%\n",
      "Iteration:  1603, Loss: 0.02125697, Accuracy: 96.48%\n",
      "Iteration:  1604, Loss: 0.02119796, Accuracy: 96.53%\n",
      "Iteration:  1605, Loss: 0.02119753, Accuracy: 96.49%\n",
      "Iteration:  1606, Loss: 0.02134639, Accuracy: 96.51%\n",
      "Iteration:  1607, Loss: 0.02119419, Accuracy: 96.48%\n",
      "Iteration:  1608, Loss: 0.02134770, Accuracy: 96.54%\n",
      "Iteration:  1609, Loss: 0.02115400, Accuracy: 96.49%\n",
      "Iteration:  1610, Loss: 0.02115443, Accuracy: 96.49%\n",
      "Iteration:  1611, Loss: 0.02107807, Accuracy: 96.49%\n",
      "Iteration:  1612, Loss: 0.02103904, Accuracy: 96.48%\n",
      "Iteration:  1613, Loss: 0.02102561, Accuracy: 96.50%\n",
      "Iteration:  1614, Loss: 0.02103521, Accuracy: 96.51%\n",
      "Iteration:  1615, Loss: 0.02104726, Accuracy: 96.49%\n",
      "Iteration:  1616, Loss: 0.02098122, Accuracy: 96.52%\n",
      "Iteration:  1617, Loss: 0.02095901, Accuracy: 96.49%\n",
      "Iteration:  1618, Loss: 0.02096336, Accuracy: 96.52%\n",
      "Iteration:  1619, Loss: 0.02098304, Accuracy: 96.49%\n",
      "Iteration:  1620, Loss: 0.02109917, Accuracy: 96.51%\n",
      "Iteration:  1621, Loss: 0.02097272, Accuracy: 96.50%\n",
      "Iteration:  1622, Loss: 0.02091310, Accuracy: 96.50%\n",
      "Iteration:  1623, Loss: 0.02088305, Accuracy: 96.48%\n",
      "Iteration:  1624, Loss: 0.02087968, Accuracy: 96.50%\n",
      "Iteration:  1625, Loss: 0.02085974, Accuracy: 96.49%\n",
      "Iteration:  1626, Loss: 0.02086023, Accuracy: 96.50%\n",
      "Iteration:  1627, Loss: 0.02086290, Accuracy: 96.51%\n",
      "Iteration:  1628, Loss: 0.02086270, Accuracy: 96.50%\n",
      "Iteration:  1629, Loss: 0.02086999, Accuracy: 96.52%\n",
      "Iteration:  1630, Loss: 0.02088715, Accuracy: 96.50%\n",
      "Iteration:  1631, Loss: 0.02083903, Accuracy: 96.48%\n",
      "Iteration:  1632, Loss: 0.02076583, Accuracy: 96.49%\n",
      "Iteration:  1633, Loss: 0.02075931, Accuracy: 96.50%\n",
      "Iteration:  1634, Loss: 0.02079138, Accuracy: 96.49%\n",
      "Iteration:  1635, Loss: 0.02072416, Accuracy: 96.50%\n",
      "Iteration:  1636, Loss: 0.02071629, Accuracy: 96.49%\n",
      "Iteration:  1637, Loss: 0.02070228, Accuracy: 96.49%\n",
      "Iteration:  1638, Loss: 0.02070179, Accuracy: 96.49%\n",
      "Iteration:  1639, Loss: 0.02069578, Accuracy: 96.49%\n",
      "Iteration:  1640, Loss: 0.02078217, Accuracy: 96.50%\n",
      "Iteration:  1641, Loss: 0.02069165, Accuracy: 96.48%\n",
      "Iteration:  1642, Loss: 0.02075097, Accuracy: 96.51%\n",
      "Iteration:  1643, Loss: 0.02070022, Accuracy: 96.49%\n",
      "Iteration:  1644, Loss: 0.02064987, Accuracy: 96.49%\n",
      "Iteration:  1645, Loss: 0.02069480, Accuracy: 96.48%\n",
      "Iteration:  1646, Loss: 0.02062515, Accuracy: 96.51%\n",
      "Iteration:  1647, Loss: 0.02064650, Accuracy: 96.49%\n",
      "Iteration:  1648, Loss: 0.02067195, Accuracy: 96.51%\n",
      "Iteration:  1649, Loss: 0.02071965, Accuracy: 96.49%\n",
      "Iteration:  1650, Loss: 0.02065455, Accuracy: 96.51%\n",
      "Iteration:  1651, Loss: 0.02056832, Accuracy: 96.50%\n",
      "Iteration:  1652, Loss: 0.02054135, Accuracy: 96.52%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m lr_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(lr)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     10\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rates_train\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_test\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_1h\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_2h\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_iters\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_lr\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_batchsize\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_train, n_test, h1, h2, n_iters, lr_str, batchsize) \n\u001b[0;32m---> 12\u001b[0m (w1, w2, w3, sr, ls, ts) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mh1\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh2\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mh2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m success_rates[name] \u001b[38;5;241m=\u001b[39m sr \u001b[38;5;66;03m# store results\u001b[39;00m\n\u001b[1;32m     16\u001b[0m times[name] \u001b[38;5;241m=\u001b[39m ts\n",
      "Cell \u001b[0;32mIn[14], line 60\u001b[0m, in \u001b[0;36mtrain3\u001b[0;34m(X_train, Y_train, X_test, Y_test, h1, h2, iterations, lr)\u001b[0m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(iterations):\n\u001b[1;32m     59\u001b[0m     y_hat, H2, H1 \u001b[38;5;241m=\u001b[39m forward3(X_train, w1, w2, w3)\n\u001b[0;32m---> 60\u001b[0m     w1_gradient, w2_gradient, w3_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mback3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw3\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mH2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     61\u001b[0m     w1 \u001b[38;5;241m=\u001b[39m w1 \u001b[38;5;241m-\u001b[39m (w1_gradient \u001b[38;5;241m*\u001b[39m lr)\n\u001b[1;32m     62\u001b[0m     w2 \u001b[38;5;241m=\u001b[39m w2 \u001b[38;5;241m-\u001b[39m (w2_gradient \u001b[38;5;241m*\u001b[39m lr)\n",
      "Cell \u001b[0;32mIn[14], line 13\u001b[0m, in \u001b[0;36mback3\u001b[0;34m(X, Y, Y_hat, W2, W3, H1, H2)\u001b[0m\n\u001b[1;32m     10\u001b[0m W3_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(prepend_bias(H2)\u001b[38;5;241m.\u001b[39mT, (Y_hat \u001b[38;5;241m-\u001b[39m Y)) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     11\u001b[0m W2_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(prepend_bias(H1)\u001b[38;5;241m.\u001b[39mT, np\u001b[38;5;241m.\u001b[39mmatmul( Y_hat \u001b[38;5;241m-\u001b[39m Y, W3[\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mT) \n\u001b[1;32m     12\u001b[0m                         \u001b[38;5;241m*\u001b[39m sigmoid_gradient(H2))  \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 13\u001b[0m W1_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(prepend_bias( X)\u001b[38;5;241m.\u001b[39mT, \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_hat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW3\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigmoid_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mH2\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mW2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m*\u001b[39m sigmoid_gradient(H1)) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m (W1_gradient, W2_gradient, W3_gradient)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_train = 60000 # number of examples in training data (max is 60,000)\n",
    "n_test  = 10000 # number of examples in training data (max is 10,000)\n",
    "h1      = 400 # number of nodes in hidden layer 1\n",
    "h2      = 200 # number of nodes in hidden layer 1\n",
    "n_iters = 8000 # number of iterations\n",
    "lr      = 0.8\n",
    "batchsize = 10000\n",
    "\n",
    "lr_str = str(lr).replace('.', 'p')\n",
    "name = 'success_rates_train{}_test{}_1h{}_2h{}_iters{}_lr{}_batchsize{}'.format(n_train, n_test, h1, h2, n_iters, lr_str, batchsize) \n",
    "\n",
    "(w1, w2, w3, sr, ls, ts) = train3(X_train[:n_train], Y_train[:n_train], \n",
    "                   X_test[:n_test], Y_test[:n_test], \n",
    "                   h1=h1, h2=h2, iterations=n_iters, lr=lr)\n",
    "success_rates[name] = sr # store results\n",
    "times[name] = ts\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "31ea1bda-6cb2-4a34-a38b-f49447bd084c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# plot results as a function of wall time\n",
    "plt.figure(figsize = (8,5))\n",
    "plt.plot(        times['success_rates_train10000_test10000_1h50_2h50_iters300_lr0p1_batchsize500'], \n",
    "         success_rates['success_rates_train10000_test10000_1h50_2h50_iters300_lr0p1_batchsize500'], \n",
    "               label = 'success_rates_train10000_test10000_1h50_2h50_iters300_lr0.1_batchsize500')\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('time')\n",
    "plt.ylabel('success rate in %')\n",
    "plt.grid()\n",
    "#plt.ylim([80,95]) # uncomment line to restrict the y-range for more detailed view on the late training\n",
    "#plt.xlim([0,100]) # uncomment line to restrict the y-range for more detailed view on the late training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "920140a4-e824-4792-a233-b627f9593d4d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
