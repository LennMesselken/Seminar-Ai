{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "108c15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "981c1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a NumPy array of bytes:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the pixels into a matrix where each line is an image:\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "\n",
    "# 60000 images, each 784 elements (28 * 28 pixels)\n",
    "X_train = load_images(\"../data/mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 784 elements, with the same structure as X_train\n",
    "X_test = load_images(\"../data/mnist/t10k-images-idx3-ubyte.gz\")\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    n_labels = Y.shape[0]\n",
    "    n_classes = 10\n",
    "    encoded_Y = np.zeros((n_labels, n_classes))\n",
    "    for i in range(n_labels):\n",
    "        label = Y[i]\n",
    "        encoded_Y[i][label] = 1\n",
    "    return encoded_Y\n",
    "\n",
    "\n",
    "# !!! EDIT PATHS TO WHERE YOUR MNIST DATA IS !!!\n",
    "\n",
    "# 60K labels, each a single digit from 0 to 9\n",
    "Y_train_unencoded = load_labels(\"../data/mnist/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# 60K labels, each consisting of 10 one-hot encoded elements\n",
    "Y_train = one_hot_encode(Y_train_unencoded)\n",
    "\n",
    "# 10000 labels, each a single digit from 0 to 9\n",
    "Y_test = load_labels(\"../data/mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848b839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0de8956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network implementation (almost the same as backpropagation.py,\n",
    "# except for a tiny refactoring in the back() function).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exponentials = np.exp(logits)\n",
    "    return exponentials / np.sum(exponentials, axis=1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def sigmoid_gradient(sigmoid):\n",
    "    return np.multiply(sigmoid, (1 - sigmoid))\n",
    "\n",
    "\n",
    "def loss(Y, y_hat):\n",
    "    return -np.sum(Y * np.log(y_hat)) / Y.shape[0]\n",
    "\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "def forward(X, w1, w2):\n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))\n",
    "    return (y_hat, h)\n",
    "\n",
    "\n",
    "def back(X, Y, y_hat, w2, h):\n",
    "    w2_gradient = np.matmul(prepend_bias(h).T, (y_hat - Y)) / X.shape[0]\n",
    "    w1_gradient = np.matmul(prepend_bias(X).T, np.matmul(y_hat - Y, w2[1:].T)\n",
    "                            * sigmoid_gradient(h)) / X.shape[0]\n",
    "    return (w1_gradient, w2_gradient)\n",
    "\n",
    "\n",
    "def classify(X, w1, w2):\n",
    "    y_hat, _ = forward(X, w1, w2)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def initialize_weights(n_input_variables, n_hidden_nodes, n_classes):\n",
    "    w1_rows = n_input_variables + 1\n",
    "    w1 = np.random.randn(w1_rows, n_hidden_nodes) * np.sqrt(1 / w1_rows)\n",
    "\n",
    "    w2_rows = n_hidden_nodes + 1\n",
    "    w2 = np.random.randn(w2_rows, n_classes) * np.sqrt(1 / w2_rows)\n",
    "\n",
    "    return (w1, w2)\n",
    "\n",
    "\n",
    "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat, _ = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(\"Iteration: %5d, Loss: %.8f, Accuracy: %.2f%%\" %\n",
    "          (iteration, training_loss, accuracy))\n",
    "    return accuracy\n",
    "\n",
    "def report_JR(iteration, X_train, Y_train, X_test, Y_test, w):\n",
    "    matches = np.count_nonzero(classify(X_test, w) == Y_test)\n",
    "    n_test_ex = Y_test.shape[0]\n",
    "    matches = matches * 100.0 / n_test_ex\n",
    "    training_loss = loss(X_train, Y_train, w)\n",
    "    print('iteration {} - loss: {:.2f}, matches: {:.2f}%'.format(iteration, training_loss, matches))\n",
    "    return matches\n",
    "\n",
    "    \n",
    "\n",
    "def train(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr):\n",
    "    success_rates = []\n",
    "    n_input_variables = X_train.shape[1]\n",
    "    n_classes = Y_train.shape[1]\n",
    "    w1, w2 = initialize_weights(n_input_variables, n_hidden_nodes, n_classes)\n",
    "    for iteration in range(iterations):\n",
    "        y_hat, h = forward(X_train, w1, w2)\n",
    "        w1_gradient, w2_gradient = back(X_train, Y_train, y_hat, w2, h)\n",
    "        w1 = w1 - (w1_gradient * lr)\n",
    "        w2 = w2 - (w2_gradient * lr)\n",
    "        accuracy = report(iteration, X_train, Y_train, X_test, Y_test, w1, w2)\n",
    "        success_rates.append(accuracy)\n",
    "    return (w1, w2, success_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f6912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f066d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "aa9a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store your success rates\n",
    "# !!! UNCOMMENT, EXECUTE, AND COMMENT THE LINE BELOW AGAIN !!!\n",
    "# !!! DON'T EXECUTE THE LINE AGAIN AFTER STARTING TO COLLECT RESULTS !!!\n",
    "\n",
    "#success_rates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "78a59014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:     0, Loss: 7.55596524, Accuracy: 10.50%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/r7d1zvcs4g5_lpdwx6rf5tgr0000gn/T/ipykernel_87127/3873833549.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:     1, Loss: 9.02008059, Accuracy: 24.20%\n",
      "Iteration:     2, Loss: 7.88470494, Accuracy: 27.40%\n",
      "Iteration:     3, Loss: 5.29328895, Accuracy: 23.90%\n",
      "Iteration:     4, Loss: 3.59404825, Accuracy: 20.40%\n",
      "Iteration:     5, Loss: 2.42933864, Accuracy: 20.10%\n",
      "Iteration:     6, Loss: 1.82232256, Accuracy: 42.70%\n",
      "Iteration:     7, Loss: 1.45714610, Accuracy: 54.90%\n",
      "Iteration:     8, Loss: 1.43292484, Accuracy: 51.40%\n",
      "Iteration:     9, Loss: 1.45395308, Accuracy: 53.00%\n",
      "Iteration:    10, Loss: 1.31036668, Accuracy: 59.70%\n",
      "Iteration:    11, Loss: 1.12405678, Accuracy: 65.20%\n",
      "Iteration:    12, Loss: 1.02814429, Accuracy: 65.70%\n",
      "Iteration:    13, Loss: 1.08572453, Accuracy: 63.00%\n",
      "Iteration:    14, Loss: 1.19354102, Accuracy: 53.80%\n",
      "Iteration:    15, Loss: 1.31791217, Accuracy: 67.30%\n",
      "Iteration:    16, Loss: 0.94599639, Accuracy: 68.40%\n",
      "Iteration:    17, Loss: 0.93589192, Accuracy: 74.10%\n",
      "Iteration:    18, Loss: 0.74932729, Accuracy: 73.10%\n",
      "Iteration:    19, Loss: 0.72070630, Accuracy: 78.70%\n",
      "Iteration:    20, Loss: 0.64734881, Accuracy: 77.50%\n",
      "Iteration:    21, Loss: 0.67095771, Accuracy: 80.10%\n",
      "Iteration:    22, Loss: 0.64395074, Accuracy: 78.20%\n",
      "Iteration:    23, Loss: 0.77307957, Accuracy: 75.00%\n",
      "Iteration:    24, Loss: 0.66678684, Accuracy: 76.00%\n",
      "Iteration:    25, Loss: 0.69247282, Accuracy: 73.50%\n",
      "Iteration:    26, Loss: 0.75269240, Accuracy: 72.00%\n",
      "Iteration:    27, Loss: 0.75645389, Accuracy: 77.40%\n",
      "Iteration:    28, Loss: 0.64960737, Accuracy: 76.00%\n",
      "Iteration:    29, Loss: 0.66731021, Accuracy: 76.60%\n",
      "Iteration:    30, Loss: 0.70134486, Accuracy: 74.70%\n",
      "Iteration:    31, Loss: 0.75037412, Accuracy: 75.30%\n",
      "Iteration:    32, Loss: 0.60494082, Accuracy: 78.00%\n",
      "Iteration:    33, Loss: 0.54513294, Accuracy: 82.50%\n",
      "Iteration:    34, Loss: 0.52461835, Accuracy: 82.10%\n",
      "Iteration:    35, Loss: 0.49567726, Accuracy: 83.00%\n",
      "Iteration:    36, Loss: 0.47007753, Accuracy: 83.40%\n",
      "Iteration:    37, Loss: 0.45977911, Accuracy: 84.30%\n",
      "Iteration:    38, Loss: 0.42409553, Accuracy: 84.50%\n",
      "Iteration:    39, Loss: 0.42221897, Accuracy: 86.10%\n",
      "Iteration:    40, Loss: 0.39509465, Accuracy: 86.00%\n",
      "Iteration:    41, Loss: 0.38523233, Accuracy: 87.80%\n",
      "Iteration:    42, Loss: 0.36951895, Accuracy: 87.50%\n",
      "Iteration:    43, Loss: 0.36086371, Accuracy: 88.90%\n",
      "Iteration:    44, Loss: 0.34989624, Accuracy: 88.30%\n",
      "Iteration:    45, Loss: 0.34161694, Accuracy: 89.60%\n",
      "Iteration:    46, Loss: 0.33859635, Accuracy: 89.30%\n",
      "Iteration:    47, Loss: 0.33458554, Accuracy: 88.60%\n",
      "Iteration:    48, Loss: 0.33124008, Accuracy: 89.30%\n",
      "Iteration:    49, Loss: 0.32763987, Accuracy: 89.50%\n",
      "Iteration:    50, Loss: 0.32710857, Accuracy: 89.40%\n",
      "Iteration:    51, Loss: 0.32316945, Accuracy: 89.20%\n",
      "Iteration:    52, Loss: 0.32249517, Accuracy: 89.50%\n",
      "Iteration:    53, Loss: 0.31718746, Accuracy: 90.40%\n",
      "Iteration:    54, Loss: 0.32013238, Accuracy: 89.70%\n",
      "Iteration:    55, Loss: 0.31299743, Accuracy: 91.10%\n",
      "Iteration:    56, Loss: 0.30607325, Accuracy: 90.20%\n",
      "Iteration:    57, Loss: 0.30013751, Accuracy: 91.30%\n",
      "Iteration:    58, Loss: 0.29471294, Accuracy: 90.60%\n",
      "Iteration:    59, Loss: 0.28280621, Accuracy: 91.00%\n",
      "Iteration:    60, Loss: 0.27879276, Accuracy: 91.20%\n",
      "Iteration:    61, Loss: 0.27375763, Accuracy: 90.90%\n",
      "Iteration:    62, Loss: 0.27025130, Accuracy: 91.70%\n",
      "Iteration:    63, Loss: 0.26814599, Accuracy: 90.70%\n",
      "Iteration:    64, Loss: 0.26864550, Accuracy: 91.40%\n",
      "Iteration:    65, Loss: 0.26819658, Accuracy: 90.50%\n",
      "Iteration:    66, Loss: 0.26509498, Accuracy: 91.10%\n",
      "Iteration:    67, Loss: 0.26690184, Accuracy: 90.90%\n",
      "Iteration:    68, Loss: 0.26816580, Accuracy: 90.50%\n",
      "Iteration:    69, Loss: 0.27380599, Accuracy: 90.90%\n",
      "Iteration:    70, Loss: 0.28052086, Accuracy: 90.20%\n",
      "Iteration:    71, Loss: 0.28812543, Accuracy: 90.80%\n",
      "Iteration:    72, Loss: 0.29977984, Accuracy: 89.90%\n",
      "Iteration:    73, Loss: 0.31199727, Accuracy: 89.30%\n",
      "Iteration:    74, Loss: 0.31956051, Accuracy: 89.10%\n",
      "Iteration:    75, Loss: 0.31594554, Accuracy: 88.80%\n",
      "Iteration:    76, Loss: 0.30359671, Accuracy: 90.10%\n",
      "Iteration:    77, Loss: 0.27735684, Accuracy: 90.40%\n",
      "Iteration:    78, Loss: 0.26061481, Accuracy: 91.60%\n",
      "Iteration:    79, Loss: 0.24158295, Accuracy: 91.90%\n",
      "Iteration:    80, Loss: 0.23248904, Accuracy: 92.40%\n",
      "Iteration:    81, Loss: 0.22872934, Accuracy: 92.90%\n",
      "Iteration:    82, Loss: 0.22511179, Accuracy: 93.10%\n",
      "Iteration:    83, Loss: 0.22409131, Accuracy: 92.60%\n",
      "Iteration:    84, Loss: 0.22199809, Accuracy: 92.70%\n",
      "Iteration:    85, Loss: 0.22227939, Accuracy: 92.50%\n",
      "Iteration:    86, Loss: 0.22119572, Accuracy: 92.70%\n",
      "Iteration:    87, Loss: 0.22020526, Accuracy: 93.10%\n",
      "Iteration:    88, Loss: 0.21846763, Accuracy: 92.40%\n",
      "Iteration:    89, Loss: 0.21806274, Accuracy: 92.60%\n",
      "Iteration:    90, Loss: 0.21714231, Accuracy: 93.00%\n",
      "Iteration:    91, Loss: 0.21707602, Accuracy: 92.80%\n",
      "Iteration:    92, Loss: 0.21708000, Accuracy: 93.50%\n",
      "Iteration:    93, Loss: 0.21936405, Accuracy: 92.80%\n",
      "Iteration:    94, Loss: 0.21615931, Accuracy: 93.20%\n",
      "Iteration:    95, Loss: 0.21597951, Accuracy: 92.90%\n",
      "Iteration:    96, Loss: 0.21419285, Accuracy: 92.70%\n",
      "Iteration:    97, Loss: 0.21545203, Accuracy: 92.60%\n",
      "Iteration:    98, Loss: 0.21415247, Accuracy: 93.30%\n",
      "Iteration:    99, Loss: 0.21774648, Accuracy: 92.90%\n",
      "Iteration:   100, Loss: 0.21523278, Accuracy: 93.30%\n",
      "Iteration:   101, Loss: 0.21850477, Accuracy: 92.90%\n",
      "Iteration:   102, Loss: 0.21849215, Accuracy: 92.60%\n",
      "Iteration:   103, Loss: 0.21465382, Accuracy: 92.60%\n",
      "Iteration:   104, Loss: 0.20930277, Accuracy: 93.60%\n",
      "Iteration:   105, Loss: 0.20903920, Accuracy: 93.40%\n",
      "Iteration:   106, Loss: 0.20392583, Accuracy: 93.60%\n",
      "Iteration:   107, Loss: 0.20102253, Accuracy: 93.30%\n",
      "Iteration:   108, Loss: 0.19972491, Accuracy: 93.80%\n",
      "Iteration:   109, Loss: 0.19452980, Accuracy: 93.50%\n",
      "Iteration:   110, Loss: 0.19258786, Accuracy: 93.70%\n",
      "Iteration:   111, Loss: 0.18900335, Accuracy: 93.70%\n",
      "Iteration:   112, Loss: 0.18825632, Accuracy: 93.70%\n",
      "Iteration:   113, Loss: 0.18917640, Accuracy: 93.20%\n",
      "Iteration:   114, Loss: 0.18771937, Accuracy: 94.40%\n",
      "Iteration:   115, Loss: 0.18854845, Accuracy: 93.60%\n",
      "Iteration:   116, Loss: 0.19230887, Accuracy: 93.70%\n",
      "Iteration:   117, Loss: 0.19566299, Accuracy: 93.10%\n",
      "Iteration:   118, Loss: 0.19706535, Accuracy: 93.10%\n",
      "Iteration:   119, Loss: 0.19154236, Accuracy: 93.40%\n",
      "Iteration:   120, Loss: 0.19116719, Accuracy: 93.60%\n",
      "Iteration:   121, Loss: 0.18919260, Accuracy: 93.90%\n",
      "Iteration:   122, Loss: 0.18873765, Accuracy: 93.50%\n",
      "Iteration:   123, Loss: 0.18603944, Accuracy: 93.30%\n",
      "Iteration:   124, Loss: 0.18437863, Accuracy: 94.00%\n",
      "Iteration:   125, Loss: 0.18292835, Accuracy: 93.50%\n",
      "Iteration:   126, Loss: 0.18193763, Accuracy: 93.90%\n",
      "Iteration:   127, Loss: 0.18278636, Accuracy: 93.50%\n",
      "Iteration:   128, Loss: 0.18167738, Accuracy: 93.90%\n",
      "Iteration:   129, Loss: 0.17904301, Accuracy: 93.30%\n",
      "Iteration:   130, Loss: 0.17741465, Accuracy: 93.60%\n",
      "Iteration:   131, Loss: 0.17933682, Accuracy: 93.70%\n",
      "Iteration:   132, Loss: 0.17799465, Accuracy: 94.20%\n",
      "Iteration:   133, Loss: 0.17794822, Accuracy: 93.60%\n",
      "Iteration:   134, Loss: 0.17854411, Accuracy: 94.50%\n",
      "Iteration:   135, Loss: 0.18019154, Accuracy: 93.90%\n",
      "Iteration:   136, Loss: 0.17786048, Accuracy: 94.60%\n",
      "Iteration:   137, Loss: 0.17898033, Accuracy: 94.20%\n",
      "Iteration:   138, Loss: 0.17885073, Accuracy: 94.90%\n",
      "Iteration:   139, Loss: 0.17647604, Accuracy: 93.90%\n",
      "Iteration:   140, Loss: 0.17794140, Accuracy: 94.30%\n",
      "Iteration:   141, Loss: 0.17731437, Accuracy: 94.10%\n",
      "Iteration:   142, Loss: 0.17614935, Accuracy: 94.00%\n",
      "Iteration:   143, Loss: 0.17503587, Accuracy: 93.70%\n",
      "Iteration:   144, Loss: 0.17308359, Accuracy: 94.00%\n",
      "Iteration:   145, Loss: 0.16807667, Accuracy: 94.20%\n",
      "Iteration:   146, Loss: 0.16770303, Accuracy: 94.70%\n",
      "Iteration:   147, Loss: 0.16807399, Accuracy: 94.40%\n",
      "Iteration:   148, Loss: 0.16777962, Accuracy: 94.40%\n",
      "Iteration:   149, Loss: 0.16624923, Accuracy: 94.10%\n",
      "Iteration:   150, Loss: 0.16612514, Accuracy: 94.40%\n",
      "Iteration:   151, Loss: 0.16667849, Accuracy: 94.90%\n",
      "Iteration:   152, Loss: 0.16707049, Accuracy: 94.00%\n",
      "Iteration:   153, Loss: 0.16419191, Accuracy: 95.10%\n",
      "Iteration:   154, Loss: 0.16734595, Accuracy: 94.00%\n",
      "Iteration:   155, Loss: 0.16633860, Accuracy: 94.30%\n",
      "Iteration:   156, Loss: 0.16660603, Accuracy: 94.00%\n",
      "Iteration:   157, Loss: 0.16802856, Accuracy: 94.90%\n",
      "Iteration:   158, Loss: 0.16941163, Accuracy: 94.80%\n",
      "Iteration:   159, Loss: 0.16884639, Accuracy: 95.00%\n",
      "Iteration:   160, Loss: 0.16881109, Accuracy: 94.90%\n",
      "Iteration:   161, Loss: 0.17068064, Accuracy: 94.80%\n",
      "Iteration:   162, Loss: 0.16992447, Accuracy: 94.70%\n",
      "Iteration:   163, Loss: 0.17155061, Accuracy: 95.00%\n",
      "Iteration:   164, Loss: 0.16691190, Accuracy: 94.50%\n",
      "Iteration:   165, Loss: 0.16474342, Accuracy: 94.40%\n",
      "Iteration:   166, Loss: 0.16411961, Accuracy: 94.50%\n",
      "Iteration:   167, Loss: 0.16177493, Accuracy: 94.20%\n",
      "Iteration:   168, Loss: 0.16060819, Accuracy: 94.90%\n",
      "Iteration:   169, Loss: 0.15680362, Accuracy: 94.60%\n",
      "Iteration:   170, Loss: 0.15752921, Accuracy: 95.00%\n",
      "Iteration:   171, Loss: 0.15556995, Accuracy: 94.10%\n",
      "Iteration:   172, Loss: 0.15610536, Accuracy: 94.80%\n",
      "Iteration:   173, Loss: 0.15052700, Accuracy: 94.50%\n",
      "Iteration:   174, Loss: 0.14949570, Accuracy: 95.20%\n",
      "Iteration:   175, Loss: 0.14769083, Accuracy: 94.60%\n",
      "Iteration:   176, Loss: 0.14985341, Accuracy: 95.20%\n",
      "Iteration:   177, Loss: 0.14612793, Accuracy: 94.40%\n",
      "Iteration:   178, Loss: 0.14418780, Accuracy: 95.20%\n",
      "Iteration:   179, Loss: 0.14436868, Accuracy: 94.60%\n",
      "Iteration:   180, Loss: 0.14517443, Accuracy: 95.00%\n",
      "Iteration:   181, Loss: 0.14335926, Accuracy: 94.30%\n",
      "Iteration:   182, Loss: 0.14714686, Accuracy: 94.90%\n",
      "Iteration:   183, Loss: 0.14455275, Accuracy: 94.70%\n",
      "Iteration:   184, Loss: 0.14647700, Accuracy: 94.20%\n",
      "Iteration:   185, Loss: 0.14786466, Accuracy: 94.50%\n",
      "Iteration:   186, Loss: 0.14950887, Accuracy: 94.70%\n",
      "Iteration:   187, Loss: 0.14848466, Accuracy: 94.00%\n",
      "Iteration:   188, Loss: 0.15066528, Accuracy: 94.40%\n",
      "Iteration:   189, Loss: 0.14821411, Accuracy: 94.30%\n",
      "Iteration:   190, Loss: 0.14478248, Accuracy: 94.60%\n",
      "Iteration:   191, Loss: 0.14350735, Accuracy: 94.60%\n",
      "Iteration:   192, Loss: 0.14244231, Accuracy: 94.40%\n",
      "Iteration:   193, Loss: 0.14203062, Accuracy: 94.60%\n",
      "Iteration:   194, Loss: 0.13802230, Accuracy: 95.10%\n",
      "Iteration:   195, Loss: 0.13599471, Accuracy: 94.80%\n",
      "Iteration:   196, Loss: 0.13619899, Accuracy: 94.80%\n",
      "Iteration:   197, Loss: 0.13539967, Accuracy: 94.80%\n",
      "Iteration:   198, Loss: 0.13360862, Accuracy: 94.70%\n",
      "Iteration:   199, Loss: 0.13367643, Accuracy: 94.60%\n",
      "Iteration:   200, Loss: 0.13298206, Accuracy: 94.70%\n",
      "Iteration:   201, Loss: 0.13456535, Accuracy: 95.20%\n",
      "Iteration:   202, Loss: 0.13646551, Accuracy: 95.30%\n",
      "Iteration:   203, Loss: 0.13783014, Accuracy: 95.20%\n",
      "Iteration:   204, Loss: 0.14022189, Accuracy: 94.50%\n",
      "Iteration:   205, Loss: 0.13912012, Accuracy: 94.90%\n",
      "Iteration:   206, Loss: 0.13979493, Accuracy: 94.60%\n",
      "Iteration:   207, Loss: 0.13913383, Accuracy: 95.00%\n",
      "Iteration:   208, Loss: 0.13840279, Accuracy: 94.60%\n",
      "Iteration:   209, Loss: 0.14022409, Accuracy: 95.50%\n",
      "Iteration:   210, Loss: 0.14361080, Accuracy: 95.10%\n",
      "Iteration:   211, Loss: 0.14760915, Accuracy: 95.10%\n",
      "Iteration:   212, Loss: 0.14808545, Accuracy: 94.50%\n",
      "Iteration:   213, Loss: 0.15339272, Accuracy: 95.10%\n",
      "Iteration:   214, Loss: 0.15041738, Accuracy: 94.60%\n",
      "Iteration:   215, Loss: 0.14704066, Accuracy: 94.80%\n",
      "Iteration:   216, Loss: 0.14368114, Accuracy: 94.60%\n",
      "Iteration:   217, Loss: 0.14554396, Accuracy: 94.90%\n",
      "Iteration:   218, Loss: 0.13874878, Accuracy: 95.00%\n",
      "Iteration:   219, Loss: 0.13526196, Accuracy: 95.10%\n",
      "Iteration:   220, Loss: 0.13070098, Accuracy: 94.80%\n",
      "Iteration:   221, Loss: 0.12924646, Accuracy: 95.10%\n",
      "Iteration:   222, Loss: 0.12836341, Accuracy: 95.30%\n",
      "Iteration:   223, Loss: 0.13153316, Accuracy: 94.70%\n",
      "Iteration:   224, Loss: 0.13057405, Accuracy: 95.60%\n",
      "Iteration:   225, Loss: 0.13020015, Accuracy: 94.60%\n",
      "Iteration:   226, Loss: 0.12929097, Accuracy: 95.50%\n",
      "Iteration:   227, Loss: 0.13066247, Accuracy: 94.70%\n",
      "Iteration:   228, Loss: 0.12823554, Accuracy: 95.30%\n",
      "Iteration:   229, Loss: 0.12775531, Accuracy: 95.30%\n",
      "Iteration:   230, Loss: 0.12567976, Accuracy: 94.80%\n",
      "Iteration:   231, Loss: 0.12637423, Accuracy: 94.90%\n",
      "Iteration:   232, Loss: 0.12605288, Accuracy: 95.10%\n",
      "Iteration:   233, Loss: 0.12617829, Accuracy: 95.50%\n",
      "Iteration:   234, Loss: 0.12708991, Accuracy: 94.80%\n",
      "Iteration:   235, Loss: 0.12716579, Accuracy: 94.80%\n",
      "Iteration:   236, Loss: 0.12536430, Accuracy: 95.10%\n",
      "Iteration:   237, Loss: 0.12227068, Accuracy: 95.10%\n",
      "Iteration:   238, Loss: 0.12113490, Accuracy: 95.20%\n",
      "Iteration:   239, Loss: 0.11981757, Accuracy: 94.80%\n",
      "Iteration:   240, Loss: 0.12002113, Accuracy: 95.20%\n",
      "Iteration:   241, Loss: 0.12095798, Accuracy: 95.10%\n",
      "Iteration:   242, Loss: 0.12169311, Accuracy: 94.80%\n",
      "Iteration:   243, Loss: 0.12079306, Accuracy: 94.80%\n",
      "Iteration:   244, Loss: 0.12007641, Accuracy: 95.40%\n",
      "Iteration:   245, Loss: 0.11847064, Accuracy: 95.40%\n",
      "Iteration:   246, Loss: 0.11620471, Accuracy: 95.50%\n",
      "Iteration:   247, Loss: 0.11531654, Accuracy: 95.10%\n",
      "Iteration:   248, Loss: 0.11442641, Accuracy: 95.30%\n",
      "Iteration:   249, Loss: 0.11483355, Accuracy: 95.40%\n",
      "Iteration:   250, Loss: 0.11444996, Accuracy: 95.40%\n",
      "Iteration:   251, Loss: 0.11595458, Accuracy: 95.00%\n",
      "Iteration:   252, Loss: 0.11756698, Accuracy: 95.60%\n",
      "Iteration:   253, Loss: 0.11833311, Accuracy: 95.00%\n",
      "Iteration:   254, Loss: 0.11649841, Accuracy: 95.40%\n",
      "Iteration:   255, Loss: 0.11727897, Accuracy: 94.90%\n",
      "Iteration:   256, Loss: 0.12166281, Accuracy: 95.20%\n",
      "Iteration:   257, Loss: 0.12419010, Accuracy: 95.00%\n",
      "Iteration:   258, Loss: 0.12617508, Accuracy: 94.80%\n",
      "Iteration:   259, Loss: 0.13277281, Accuracy: 95.20%\n",
      "Iteration:   260, Loss: 0.13619885, Accuracy: 94.40%\n",
      "Iteration:   261, Loss: 0.14323894, Accuracy: 95.00%\n",
      "Iteration:   262, Loss: 0.14662849, Accuracy: 94.30%\n",
      "Iteration:   263, Loss: 0.16365950, Accuracy: 94.10%\n",
      "Iteration:   264, Loss: 0.18269428, Accuracy: 92.70%\n",
      "Iteration:   265, Loss: 0.22531486, Accuracy: 91.10%\n",
      "Iteration:   266, Loss: 0.29509910, Accuracy: 88.30%\n",
      "Iteration:   267, Loss: 0.41569905, Accuracy: 82.50%\n",
      "Iteration:   268, Loss: 0.54562144, Accuracy: 85.50%\n",
      "Iteration:   269, Loss: 0.26376647, Accuracy: 90.50%\n",
      "Iteration:   270, Loss: 0.21401655, Accuracy: 91.60%\n",
      "Iteration:   271, Loss: 0.19608955, Accuracy: 91.60%\n",
      "Iteration:   272, Loss: 0.14880060, Accuracy: 94.20%\n",
      "Iteration:   273, Loss: 0.13020120, Accuracy: 95.40%\n",
      "Iteration:   274, Loss: 0.12345353, Accuracy: 94.50%\n",
      "Iteration:   275, Loss: 0.11975191, Accuracy: 95.50%\n",
      "Iteration:   276, Loss: 0.11749677, Accuracy: 94.70%\n",
      "Iteration:   277, Loss: 0.11762836, Accuracy: 95.70%\n",
      "Iteration:   278, Loss: 0.11689646, Accuracy: 95.10%\n",
      "Iteration:   279, Loss: 0.11537615, Accuracy: 95.70%\n",
      "Iteration:   280, Loss: 0.11367420, Accuracy: 95.50%\n",
      "Iteration:   281, Loss: 0.11419551, Accuracy: 95.80%\n",
      "Iteration:   282, Loss: 0.11375234, Accuracy: 95.20%\n",
      "Iteration:   283, Loss: 0.11398025, Accuracy: 95.40%\n",
      "Iteration:   284, Loss: 0.11230029, Accuracy: 95.40%\n",
      "Iteration:   285, Loss: 0.11426582, Accuracy: 95.70%\n",
      "Iteration:   286, Loss: 0.11101585, Accuracy: 96.10%\n",
      "Iteration:   287, Loss: 0.11192808, Accuracy: 95.50%\n",
      "Iteration:   288, Loss: 0.11287686, Accuracy: 95.80%\n",
      "Iteration:   289, Loss: 0.11324620, Accuracy: 95.80%\n",
      "Iteration:   290, Loss: 0.11382771, Accuracy: 95.40%\n",
      "Iteration:   291, Loss: 0.11355740, Accuracy: 95.70%\n",
      "Iteration:   292, Loss: 0.11390497, Accuracy: 95.60%\n",
      "Iteration:   293, Loss: 0.11657694, Accuracy: 95.90%\n",
      "Iteration:   294, Loss: 0.11523559, Accuracy: 95.40%\n",
      "Iteration:   295, Loss: 0.11588468, Accuracy: 95.20%\n",
      "Iteration:   296, Loss: 0.11367718, Accuracy: 95.20%\n",
      "Iteration:   297, Loss: 0.11395068, Accuracy: 95.90%\n",
      "Iteration:   298, Loss: 0.11058012, Accuracy: 95.50%\n",
      "Iteration:   299, Loss: 0.11031825, Accuracy: 95.70%\n",
      "Iteration:   300, Loss: 0.11050829, Accuracy: 95.60%\n",
      "Iteration:   301, Loss: 0.10746639, Accuracy: 95.80%\n",
      "Iteration:   302, Loss: 0.10682937, Accuracy: 95.60%\n",
      "Iteration:   303, Loss: 0.10748625, Accuracy: 96.10%\n",
      "Iteration:   304, Loss: 0.10607933, Accuracy: 95.90%\n",
      "Iteration:   305, Loss: 0.10571427, Accuracy: 95.90%\n",
      "Iteration:   306, Loss: 0.10717788, Accuracy: 95.80%\n",
      "Iteration:   307, Loss: 0.10680863, Accuracy: 96.00%\n",
      "Iteration:   308, Loss: 0.10413262, Accuracy: 95.60%\n",
      "Iteration:   309, Loss: 0.10445541, Accuracy: 95.90%\n",
      "Iteration:   310, Loss: 0.10324010, Accuracy: 95.70%\n",
      "Iteration:   311, Loss: 0.10527140, Accuracy: 96.00%\n",
      "Iteration:   312, Loss: 0.10316242, Accuracy: 96.30%\n",
      "Iteration:   313, Loss: 0.10366839, Accuracy: 95.60%\n",
      "Iteration:   314, Loss: 0.10272294, Accuracy: 96.20%\n",
      "Iteration:   315, Loss: 0.10235760, Accuracy: 95.70%\n",
      "Iteration:   316, Loss: 0.10091446, Accuracy: 95.90%\n",
      "Iteration:   317, Loss: 0.10229622, Accuracy: 95.50%\n",
      "Iteration:   318, Loss: 0.10061125, Accuracy: 95.70%\n",
      "Iteration:   319, Loss: 0.10284666, Accuracy: 95.90%\n",
      "Iteration:   320, Loss: 0.10337315, Accuracy: 95.50%\n",
      "Iteration:   321, Loss: 0.10680317, Accuracy: 96.10%\n",
      "Iteration:   322, Loss: 0.10777508, Accuracy: 95.30%\n",
      "Iteration:   323, Loss: 0.11467046, Accuracy: 95.90%\n",
      "Iteration:   324, Loss: 0.11296000, Accuracy: 95.70%\n",
      "Iteration:   325, Loss: 0.11595194, Accuracy: 95.60%\n",
      "Iteration:   326, Loss: 0.11524411, Accuracy: 95.40%\n",
      "Iteration:   327, Loss: 0.11518228, Accuracy: 95.70%\n",
      "Iteration:   328, Loss: 0.11421650, Accuracy: 95.90%\n",
      "Iteration:   329, Loss: 0.11401469, Accuracy: 96.20%\n",
      "Iteration:   330, Loss: 0.11101657, Accuracy: 95.50%\n",
      "Iteration:   331, Loss: 0.10623719, Accuracy: 95.90%\n",
      "Iteration:   332, Loss: 0.10580066, Accuracy: 96.00%\n",
      "Iteration:   333, Loss: 0.10388173, Accuracy: 95.90%\n",
      "Iteration:   334, Loss: 0.10050151, Accuracy: 95.60%\n",
      "Iteration:   335, Loss: 0.09786076, Accuracy: 95.60%\n",
      "Iteration:   336, Loss: 0.09682116, Accuracy: 95.80%\n",
      "Iteration:   337, Loss: 0.09557208, Accuracy: 96.10%\n",
      "Iteration:   338, Loss: 0.09574959, Accuracy: 95.70%\n",
      "Iteration:   339, Loss: 0.09655896, Accuracy: 95.80%\n",
      "Iteration:   340, Loss: 0.09453261, Accuracy: 95.90%\n",
      "Iteration:   341, Loss: 0.09288774, Accuracy: 96.10%\n",
      "Iteration:   342, Loss: 0.09298994, Accuracy: 95.70%\n",
      "Iteration:   343, Loss: 0.09461675, Accuracy: 96.20%\n",
      "Iteration:   344, Loss: 0.09520742, Accuracy: 95.70%\n",
      "Iteration:   345, Loss: 0.09721542, Accuracy: 95.80%\n",
      "Iteration:   346, Loss: 0.09573260, Accuracy: 95.70%\n",
      "Iteration:   347, Loss: 0.09646653, Accuracy: 96.40%\n",
      "Iteration:   348, Loss: 0.09566202, Accuracy: 96.10%\n",
      "Iteration:   349, Loss: 0.09934259, Accuracy: 96.00%\n",
      "Iteration:   350, Loss: 0.10180180, Accuracy: 95.80%\n",
      "Iteration:   351, Loss: 0.10538238, Accuracy: 95.50%\n",
      "Iteration:   352, Loss: 0.10491256, Accuracy: 95.70%\n",
      "Iteration:   353, Loss: 0.10658201, Accuracy: 95.40%\n",
      "Iteration:   354, Loss: 0.10377900, Accuracy: 96.10%\n",
      "Iteration:   355, Loss: 0.10343465, Accuracy: 95.90%\n",
      "Iteration:   356, Loss: 0.10363769, Accuracy: 95.70%\n",
      "Iteration:   357, Loss: 0.10237418, Accuracy: 96.30%\n",
      "Iteration:   358, Loss: 0.10304476, Accuracy: 95.40%\n",
      "Iteration:   359, Loss: 0.09919734, Accuracy: 96.40%\n",
      "Iteration:   360, Loss: 0.10011457, Accuracy: 95.20%\n",
      "Iteration:   361, Loss: 0.09619884, Accuracy: 96.40%\n",
      "Iteration:   362, Loss: 0.09408793, Accuracy: 96.10%\n",
      "Iteration:   363, Loss: 0.09335712, Accuracy: 96.50%\n",
      "Iteration:   364, Loss: 0.09239250, Accuracy: 95.90%\n",
      "Iteration:   365, Loss: 0.09194780, Accuracy: 96.20%\n",
      "Iteration:   366, Loss: 0.09151129, Accuracy: 95.80%\n",
      "Iteration:   367, Loss: 0.08942976, Accuracy: 96.10%\n",
      "Iteration:   368, Loss: 0.08941803, Accuracy: 95.90%\n",
      "Iteration:   369, Loss: 0.09171573, Accuracy: 96.60%\n",
      "Iteration:   370, Loss: 0.09206995, Accuracy: 95.30%\n",
      "Iteration:   371, Loss: 0.09172443, Accuracy: 95.90%\n",
      "Iteration:   372, Loss: 0.09289400, Accuracy: 96.30%\n",
      "Iteration:   373, Loss: 0.09619975, Accuracy: 96.20%\n",
      "Iteration:   374, Loss: 0.09582464, Accuracy: 95.90%\n",
      "Iteration:   375, Loss: 0.09996636, Accuracy: 96.30%\n",
      "Iteration:   376, Loss: 0.09955157, Accuracy: 95.80%\n",
      "Iteration:   377, Loss: 0.09898180, Accuracy: 96.10%\n",
      "Iteration:   378, Loss: 0.09625021, Accuracy: 96.30%\n",
      "Iteration:   379, Loss: 0.10015102, Accuracy: 96.20%\n",
      "Iteration:   380, Loss: 0.10433453, Accuracy: 96.30%\n",
      "Iteration:   381, Loss: 0.10564041, Accuracy: 95.90%\n",
      "Iteration:   382, Loss: 0.10569742, Accuracy: 95.90%\n",
      "Iteration:   383, Loss: 0.10431673, Accuracy: 96.20%\n",
      "Iteration:   384, Loss: 0.10237960, Accuracy: 95.90%\n",
      "Iteration:   385, Loss: 0.10490590, Accuracy: 96.10%\n",
      "Iteration:   386, Loss: 0.10279536, Accuracy: 96.30%\n",
      "Iteration:   387, Loss: 0.10231958, Accuracy: 95.90%\n",
      "Iteration:   388, Loss: 0.09595745, Accuracy: 95.80%\n",
      "Iteration:   389, Loss: 0.09475618, Accuracy: 96.30%\n",
      "Iteration:   390, Loss: 0.08965726, Accuracy: 95.80%\n",
      "Iteration:   391, Loss: 0.08700001, Accuracy: 96.00%\n",
      "Iteration:   392, Loss: 0.08623127, Accuracy: 96.10%\n",
      "Iteration:   393, Loss: 0.08439310, Accuracy: 96.30%\n",
      "Iteration:   394, Loss: 0.08402705, Accuracy: 96.20%\n",
      "Iteration:   395, Loss: 0.08328421, Accuracy: 96.10%\n",
      "Iteration:   396, Loss: 0.08301823, Accuracy: 95.90%\n",
      "Iteration:   397, Loss: 0.08408355, Accuracy: 95.90%\n",
      "Iteration:   398, Loss: 0.08472144, Accuracy: 95.90%\n",
      "Iteration:   399, Loss: 0.08441338, Accuracy: 96.30%\n",
      "Iteration:   400, Loss: 0.07982614, Accuracy: 95.90%\n",
      "Iteration:   401, Loss: 0.07827221, Accuracy: 96.00%\n",
      "Iteration:   402, Loss: 0.07784760, Accuracy: 95.90%\n",
      "Iteration:   403, Loss: 0.07726154, Accuracy: 96.00%\n",
      "Iteration:   404, Loss: 0.07700127, Accuracy: 96.00%\n",
      "Iteration:   405, Loss: 0.07673159, Accuracy: 96.00%\n",
      "Iteration:   406, Loss: 0.07656635, Accuracy: 96.10%\n",
      "Iteration:   407, Loss: 0.07662251, Accuracy: 96.00%\n",
      "Iteration:   408, Loss: 0.07609771, Accuracy: 95.90%\n",
      "Iteration:   409, Loss: 0.07607993, Accuracy: 96.00%\n",
      "Iteration:   410, Loss: 0.07619575, Accuracy: 95.90%\n",
      "Iteration:   411, Loss: 0.07595642, Accuracy: 96.10%\n",
      "Iteration:   412, Loss: 0.07577110, Accuracy: 96.10%\n",
      "Iteration:   413, Loss: 0.07587693, Accuracy: 95.70%\n",
      "Iteration:   414, Loss: 0.07570166, Accuracy: 96.00%\n",
      "Iteration:   415, Loss: 0.07564544, Accuracy: 96.00%\n",
      "Iteration:   416, Loss: 0.07541653, Accuracy: 96.00%\n",
      "Iteration:   417, Loss: 0.07508151, Accuracy: 95.90%\n",
      "Iteration:   418, Loss: 0.07482843, Accuracy: 95.90%\n",
      "Iteration:   419, Loss: 0.07505436, Accuracy: 96.10%\n",
      "Iteration:   420, Loss: 0.07544181, Accuracy: 95.90%\n",
      "Iteration:   421, Loss: 0.07537193, Accuracy: 95.90%\n",
      "Iteration:   422, Loss: 0.07529617, Accuracy: 95.90%\n",
      "Iteration:   423, Loss: 0.07526844, Accuracy: 95.80%\n",
      "Iteration:   424, Loss: 0.07465953, Accuracy: 95.70%\n",
      "Iteration:   425, Loss: 0.07511457, Accuracy: 95.90%\n",
      "Iteration:   426, Loss: 0.07426255, Accuracy: 95.70%\n",
      "Iteration:   427, Loss: 0.07453695, Accuracy: 96.00%\n",
      "Iteration:   428, Loss: 0.07441382, Accuracy: 96.00%\n",
      "Iteration:   429, Loss: 0.07430470, Accuracy: 96.20%\n",
      "Iteration:   430, Loss: 0.07378677, Accuracy: 96.00%\n",
      "Iteration:   431, Loss: 0.07353975, Accuracy: 95.90%\n",
      "Iteration:   432, Loss: 0.07389327, Accuracy: 96.20%\n",
      "Iteration:   433, Loss: 0.07367092, Accuracy: 96.20%\n",
      "Iteration:   434, Loss: 0.07386842, Accuracy: 96.30%\n",
      "Iteration:   435, Loss: 0.07399397, Accuracy: 95.80%\n",
      "Iteration:   436, Loss: 0.07434884, Accuracy: 96.00%\n",
      "Iteration:   437, Loss: 0.07476809, Accuracy: 95.90%\n",
      "Iteration:   438, Loss: 0.07458857, Accuracy: 95.80%\n",
      "Iteration:   439, Loss: 0.07400931, Accuracy: 96.10%\n",
      "Iteration:   440, Loss: 0.07367537, Accuracy: 95.80%\n",
      "Iteration:   441, Loss: 0.07386973, Accuracy: 96.20%\n",
      "Iteration:   442, Loss: 0.07333470, Accuracy: 96.10%\n",
      "Iteration:   443, Loss: 0.07349123, Accuracy: 95.70%\n",
      "Iteration:   444, Loss: 0.07346738, Accuracy: 96.10%\n",
      "Iteration:   445, Loss: 0.07342755, Accuracy: 96.00%\n",
      "Iteration:   446, Loss: 0.07309180, Accuracy: 96.00%\n",
      "Iteration:   447, Loss: 0.07240102, Accuracy: 96.00%\n",
      "Iteration:   448, Loss: 0.07193774, Accuracy: 96.40%\n",
      "Iteration:   449, Loss: 0.07181427, Accuracy: 95.90%\n",
      "Iteration:   450, Loss: 0.07168060, Accuracy: 96.40%\n",
      "Iteration:   451, Loss: 0.07236954, Accuracy: 96.20%\n",
      "Iteration:   452, Loss: 0.07206609, Accuracy: 96.20%\n",
      "Iteration:   453, Loss: 0.07184119, Accuracy: 96.00%\n",
      "Iteration:   454, Loss: 0.07197787, Accuracy: 96.20%\n",
      "Iteration:   455, Loss: 0.07236840, Accuracy: 95.80%\n",
      "Iteration:   456, Loss: 0.07259853, Accuracy: 96.10%\n",
      "Iteration:   457, Loss: 0.07289873, Accuracy: 95.90%\n",
      "Iteration:   458, Loss: 0.07249059, Accuracy: 96.50%\n",
      "Iteration:   459, Loss: 0.07259360, Accuracy: 95.70%\n",
      "Iteration:   460, Loss: 0.07183378, Accuracy: 96.30%\n",
      "Iteration:   461, Loss: 0.07193719, Accuracy: 95.90%\n",
      "Iteration:   462, Loss: 0.07186239, Accuracy: 96.30%\n",
      "Iteration:   463, Loss: 0.07154880, Accuracy: 96.20%\n",
      "Iteration:   464, Loss: 0.07153565, Accuracy: 96.40%\n",
      "Iteration:   465, Loss: 0.07108881, Accuracy: 96.10%\n",
      "Iteration:   466, Loss: 0.07135397, Accuracy: 96.30%\n",
      "Iteration:   467, Loss: 0.07140115, Accuracy: 96.00%\n",
      "Iteration:   468, Loss: 0.07131649, Accuracy: 96.30%\n",
      "Iteration:   469, Loss: 0.07153679, Accuracy: 95.80%\n",
      "Iteration:   470, Loss: 0.07150164, Accuracy: 95.90%\n",
      "Iteration:   471, Loss: 0.07205085, Accuracy: 96.20%\n",
      "Iteration:   472, Loss: 0.07202407, Accuracy: 96.20%\n",
      "Iteration:   473, Loss: 0.07191251, Accuracy: 95.90%\n",
      "Iteration:   474, Loss: 0.07091794, Accuracy: 96.40%\n",
      "Iteration:   475, Loss: 0.07105551, Accuracy: 96.00%\n",
      "Iteration:   476, Loss: 0.07044523, Accuracy: 96.00%\n",
      "Iteration:   477, Loss: 0.07087750, Accuracy: 96.50%\n",
      "Iteration:   478, Loss: 0.07131951, Accuracy: 95.90%\n",
      "Iteration:   479, Loss: 0.07120613, Accuracy: 96.30%\n",
      "Iteration:   480, Loss: 0.07119109, Accuracy: 96.00%\n",
      "Iteration:   481, Loss: 0.07206264, Accuracy: 96.40%\n",
      "Iteration:   482, Loss: 0.07089706, Accuracy: 95.70%\n",
      "Iteration:   483, Loss: 0.07150023, Accuracy: 96.30%\n",
      "Iteration:   484, Loss: 0.07038522, Accuracy: 96.30%\n",
      "Iteration:   485, Loss: 0.07048451, Accuracy: 96.60%\n",
      "Iteration:   486, Loss: 0.07049643, Accuracy: 96.00%\n",
      "Iteration:   487, Loss: 0.07007351, Accuracy: 96.40%\n",
      "Iteration:   488, Loss: 0.07026328, Accuracy: 96.20%\n",
      "Iteration:   489, Loss: 0.07053772, Accuracy: 96.40%\n",
      "Iteration:   490, Loss: 0.06963939, Accuracy: 96.20%\n",
      "Iteration:   491, Loss: 0.06918572, Accuracy: 96.00%\n",
      "Iteration:   492, Loss: 0.06892640, Accuracy: 95.70%\n",
      "Iteration:   493, Loss: 0.06907022, Accuracy: 96.30%\n",
      "Iteration:   494, Loss: 0.06933783, Accuracy: 95.70%\n",
      "Iteration:   495, Loss: 0.06865507, Accuracy: 96.40%\n",
      "Iteration:   496, Loss: 0.06820475, Accuracy: 96.20%\n",
      "Iteration:   497, Loss: 0.06897775, Accuracy: 96.30%\n",
      "Iteration:   498, Loss: 0.06938842, Accuracy: 96.30%\n",
      "Iteration:   499, Loss: 0.06964560, Accuracy: 95.80%\n",
      "Iteration:   500, Loss: 0.06939263, Accuracy: 96.20%\n",
      "Iteration:   501, Loss: 0.06917267, Accuracy: 96.00%\n",
      "Iteration:   502, Loss: 0.06909960, Accuracy: 96.10%\n",
      "Iteration:   503, Loss: 0.06924241, Accuracy: 96.30%\n",
      "Iteration:   504, Loss: 0.07038946, Accuracy: 96.20%\n",
      "Iteration:   505, Loss: 0.06999076, Accuracy: 96.30%\n",
      "Iteration:   506, Loss: 0.07019021, Accuracy: 96.00%\n",
      "Iteration:   507, Loss: 0.07005758, Accuracy: 96.20%\n",
      "Iteration:   508, Loss: 0.06953569, Accuracy: 96.00%\n",
      "Iteration:   509, Loss: 0.06846057, Accuracy: 96.30%\n",
      "Iteration:   510, Loss: 0.06873330, Accuracy: 96.10%\n",
      "Iteration:   511, Loss: 0.06838882, Accuracy: 95.90%\n",
      "Iteration:   512, Loss: 0.06754507, Accuracy: 96.10%\n",
      "Iteration:   513, Loss: 0.06922961, Accuracy: 96.10%\n",
      "Iteration:   514, Loss: 0.06806687, Accuracy: 96.00%\n",
      "Iteration:   515, Loss: 0.06824142, Accuracy: 96.10%\n",
      "Iteration:   516, Loss: 0.06832770, Accuracy: 96.30%\n",
      "Iteration:   517, Loss: 0.06815196, Accuracy: 96.40%\n",
      "Iteration:   518, Loss: 0.06829091, Accuracy: 96.00%\n",
      "Iteration:   519, Loss: 0.06821454, Accuracy: 95.90%\n",
      "Iteration:   520, Loss: 0.06769478, Accuracy: 96.10%\n",
      "Iteration:   521, Loss: 0.06811218, Accuracy: 96.50%\n",
      "Iteration:   522, Loss: 0.06778978, Accuracy: 96.20%\n",
      "Iteration:   523, Loss: 0.06764825, Accuracy: 95.70%\n",
      "Iteration:   524, Loss: 0.06741662, Accuracy: 96.50%\n",
      "Iteration:   525, Loss: 0.06803476, Accuracy: 96.30%\n",
      "Iteration:   526, Loss: 0.06813074, Accuracy: 96.50%\n",
      "Iteration:   527, Loss: 0.06912853, Accuracy: 96.70%\n",
      "Iteration:   528, Loss: 0.06867284, Accuracy: 96.20%\n",
      "Iteration:   529, Loss: 0.06974797, Accuracy: 96.30%\n",
      "Iteration:   530, Loss: 0.06923643, Accuracy: 96.40%\n",
      "Iteration:   531, Loss: 0.06798856, Accuracy: 96.30%\n",
      "Iteration:   532, Loss: 0.06694654, Accuracy: 96.20%\n",
      "Iteration:   533, Loss: 0.06747765, Accuracy: 96.00%\n",
      "Iteration:   534, Loss: 0.06700255, Accuracy: 96.10%\n",
      "Iteration:   535, Loss: 0.06698667, Accuracy: 96.20%\n",
      "Iteration:   536, Loss: 0.06659492, Accuracy: 96.40%\n",
      "Iteration:   537, Loss: 0.06652063, Accuracy: 96.30%\n",
      "Iteration:   538, Loss: 0.06610160, Accuracy: 96.30%\n",
      "Iteration:   539, Loss: 0.06631824, Accuracy: 96.50%\n",
      "Iteration:   540, Loss: 0.06608613, Accuracy: 96.60%\n",
      "Iteration:   541, Loss: 0.06696329, Accuracy: 96.40%\n",
      "Iteration:   542, Loss: 0.06642043, Accuracy: 96.50%\n",
      "Iteration:   543, Loss: 0.06645798, Accuracy: 96.10%\n",
      "Iteration:   544, Loss: 0.06630025, Accuracy: 96.60%\n",
      "Iteration:   545, Loss: 0.06588403, Accuracy: 96.30%\n",
      "Iteration:   546, Loss: 0.06524917, Accuracy: 96.30%\n",
      "Iteration:   547, Loss: 0.06553982, Accuracy: 96.30%\n",
      "Iteration:   548, Loss: 0.06551971, Accuracy: 96.20%\n",
      "Iteration:   549, Loss: 0.06591500, Accuracy: 96.10%\n",
      "Iteration:   550, Loss: 0.06532916, Accuracy: 96.20%\n",
      "Iteration:   551, Loss: 0.06606598, Accuracy: 96.60%\n",
      "Iteration:   552, Loss: 0.06574074, Accuracy: 96.40%\n",
      "Iteration:   553, Loss: 0.06573956, Accuracy: 96.50%\n",
      "Iteration:   554, Loss: 0.06586427, Accuracy: 96.20%\n",
      "Iteration:   555, Loss: 0.06621158, Accuracy: 96.60%\n",
      "Iteration:   556, Loss: 0.06714166, Accuracy: 96.40%\n",
      "Iteration:   557, Loss: 0.06763400, Accuracy: 96.10%\n",
      "Iteration:   558, Loss: 0.06731969, Accuracy: 96.30%\n",
      "Iteration:   559, Loss: 0.06729059, Accuracy: 95.80%\n",
      "Iteration:   560, Loss: 0.06651510, Accuracy: 96.20%\n",
      "Iteration:   561, Loss: 0.06630653, Accuracy: 96.60%\n",
      "Iteration:   562, Loss: 0.06520812, Accuracy: 96.20%\n",
      "Iteration:   563, Loss: 0.06541437, Accuracy: 96.70%\n",
      "Iteration:   564, Loss: 0.06575094, Accuracy: 96.10%\n",
      "Iteration:   565, Loss: 0.06640431, Accuracy: 96.40%\n",
      "Iteration:   566, Loss: 0.06625193, Accuracy: 96.40%\n",
      "Iteration:   567, Loss: 0.06496571, Accuracy: 96.40%\n",
      "Iteration:   568, Loss: 0.06460237, Accuracy: 96.40%\n",
      "Iteration:   569, Loss: 0.06452097, Accuracy: 96.50%\n",
      "Iteration:   570, Loss: 0.06465507, Accuracy: 96.30%\n",
      "Iteration:   571, Loss: 0.06434481, Accuracy: 96.20%\n",
      "Iteration:   572, Loss: 0.06451455, Accuracy: 96.30%\n",
      "Iteration:   573, Loss: 0.06493527, Accuracy: 96.30%\n",
      "Iteration:   574, Loss: 0.06488044, Accuracy: 96.10%\n",
      "Iteration:   575, Loss: 0.06540099, Accuracy: 96.60%\n",
      "Iteration:   576, Loss: 0.06449845, Accuracy: 96.30%\n",
      "Iteration:   577, Loss: 0.06427492, Accuracy: 96.50%\n",
      "Iteration:   578, Loss: 0.06414447, Accuracy: 96.30%\n",
      "Iteration:   579, Loss: 0.06494460, Accuracy: 96.40%\n",
      "Iteration:   580, Loss: 0.06514568, Accuracy: 96.50%\n",
      "Iteration:   581, Loss: 0.06585132, Accuracy: 96.10%\n",
      "Iteration:   582, Loss: 0.06529126, Accuracy: 96.40%\n",
      "Iteration:   583, Loss: 0.06600122, Accuracy: 96.20%\n",
      "Iteration:   584, Loss: 0.06636182, Accuracy: 96.50%\n",
      "Iteration:   585, Loss: 0.06672907, Accuracy: 96.30%\n",
      "Iteration:   586, Loss: 0.06540949, Accuracy: 96.10%\n",
      "Iteration:   587, Loss: 0.06706783, Accuracy: 96.20%\n",
      "Iteration:   588, Loss: 0.06594225, Accuracy: 96.60%\n",
      "Iteration:   589, Loss: 0.06555204, Accuracy: 96.20%\n",
      "Iteration:   590, Loss: 0.06535510, Accuracy: 96.50%\n",
      "Iteration:   591, Loss: 0.06506353, Accuracy: 96.60%\n",
      "Iteration:   592, Loss: 0.06508307, Accuracy: 96.60%\n",
      "Iteration:   593, Loss: 0.06466636, Accuracy: 96.20%\n",
      "Iteration:   594, Loss: 0.06348211, Accuracy: 96.00%\n",
      "Iteration:   595, Loss: 0.06361655, Accuracy: 96.40%\n",
      "Iteration:   596, Loss: 0.06293926, Accuracy: 96.10%\n",
      "Iteration:   597, Loss: 0.06284466, Accuracy: 96.20%\n",
      "Iteration:   598, Loss: 0.06260002, Accuracy: 96.10%\n",
      "Iteration:   599, Loss: 0.06364073, Accuracy: 96.20%\n",
      "Iteration:   600, Loss: 0.06288147, Accuracy: 96.20%\n",
      "Iteration:   601, Loss: 0.06258429, Accuracy: 96.20%\n",
      "Iteration:   602, Loss: 0.06244403, Accuracy: 96.70%\n",
      "Iteration:   603, Loss: 0.06303252, Accuracy: 96.10%\n",
      "Iteration:   604, Loss: 0.06281517, Accuracy: 96.40%\n",
      "Iteration:   605, Loss: 0.06317962, Accuracy: 96.40%\n",
      "Iteration:   606, Loss: 0.06298346, Accuracy: 96.30%\n",
      "Iteration:   607, Loss: 0.06277703, Accuracy: 95.90%\n",
      "Iteration:   608, Loss: 0.06293075, Accuracy: 95.80%\n",
      "Iteration:   609, Loss: 0.06369613, Accuracy: 96.20%\n",
      "Iteration:   610, Loss: 0.06305334, Accuracy: 96.50%\n",
      "Iteration:   611, Loss: 0.06402334, Accuracy: 96.10%\n",
      "Iteration:   612, Loss: 0.06302441, Accuracy: 96.20%\n",
      "Iteration:   613, Loss: 0.06301872, Accuracy: 95.90%\n",
      "Iteration:   614, Loss: 0.06348171, Accuracy: 96.00%\n",
      "Iteration:   615, Loss: 0.06240423, Accuracy: 96.40%\n",
      "Iteration:   616, Loss: 0.06290433, Accuracy: 96.10%\n",
      "Iteration:   617, Loss: 0.06316057, Accuracy: 96.40%\n",
      "Iteration:   618, Loss: 0.06329952, Accuracy: 96.10%\n",
      "Iteration:   619, Loss: 0.06381982, Accuracy: 96.50%\n",
      "Iteration:   620, Loss: 0.06383721, Accuracy: 96.40%\n",
      "Iteration:   621, Loss: 0.06317799, Accuracy: 96.10%\n",
      "Iteration:   622, Loss: 0.06430917, Accuracy: 96.30%\n",
      "Iteration:   623, Loss: 0.06387068, Accuracy: 96.30%\n",
      "Iteration:   624, Loss: 0.06318643, Accuracy: 96.20%\n",
      "Iteration:   625, Loss: 0.06296386, Accuracy: 96.50%\n",
      "Iteration:   626, Loss: 0.06286207, Accuracy: 96.00%\n",
      "Iteration:   627, Loss: 0.06307053, Accuracy: 96.30%\n",
      "Iteration:   628, Loss: 0.06300618, Accuracy: 96.20%\n",
      "Iteration:   629, Loss: 0.06254633, Accuracy: 96.50%\n",
      "Iteration:   630, Loss: 0.06201236, Accuracy: 95.80%\n",
      "Iteration:   631, Loss: 0.06196635, Accuracy: 96.40%\n",
      "Iteration:   632, Loss: 0.06193443, Accuracy: 96.20%\n",
      "Iteration:   633, Loss: 0.06188785, Accuracy: 96.40%\n",
      "Iteration:   634, Loss: 0.06208483, Accuracy: 96.00%\n",
      "Iteration:   635, Loss: 0.06179782, Accuracy: 96.00%\n",
      "Iteration:   636, Loss: 0.06126784, Accuracy: 96.20%\n",
      "Iteration:   637, Loss: 0.06162201, Accuracy: 96.20%\n",
      "Iteration:   638, Loss: 0.06116300, Accuracy: 96.10%\n",
      "Iteration:   639, Loss: 0.06143527, Accuracy: 96.30%\n",
      "Iteration:   640, Loss: 0.06209778, Accuracy: 96.20%\n",
      "Iteration:   641, Loss: 0.06249317, Accuracy: 96.40%\n",
      "Iteration:   642, Loss: 0.06227592, Accuracy: 96.00%\n",
      "Iteration:   643, Loss: 0.06249788, Accuracy: 96.20%\n",
      "Iteration:   644, Loss: 0.06131050, Accuracy: 96.20%\n",
      "Iteration:   645, Loss: 0.06107110, Accuracy: 96.00%\n",
      "Iteration:   646, Loss: 0.06156615, Accuracy: 96.20%\n",
      "Iteration:   647, Loss: 0.06175574, Accuracy: 96.00%\n",
      "Iteration:   648, Loss: 0.06228941, Accuracy: 95.90%\n",
      "Iteration:   649, Loss: 0.06116444, Accuracy: 96.30%\n",
      "Iteration:   650, Loss: 0.06174861, Accuracy: 96.20%\n",
      "Iteration:   651, Loss: 0.06118960, Accuracy: 96.20%\n",
      "Iteration:   652, Loss: 0.06066919, Accuracy: 96.10%\n",
      "Iteration:   653, Loss: 0.06116518, Accuracy: 96.20%\n",
      "Iteration:   654, Loss: 0.06148297, Accuracy: 96.00%\n",
      "Iteration:   655, Loss: 0.06103123, Accuracy: 96.40%\n",
      "Iteration:   656, Loss: 0.06130032, Accuracy: 96.10%\n",
      "Iteration:   657, Loss: 0.06160958, Accuracy: 96.30%\n",
      "Iteration:   658, Loss: 0.06180277, Accuracy: 96.40%\n",
      "Iteration:   659, Loss: 0.06165641, Accuracy: 95.90%\n",
      "Iteration:   660, Loss: 0.06108124, Accuracy: 96.10%\n",
      "Iteration:   661, Loss: 0.06123760, Accuracy: 96.20%\n",
      "Iteration:   662, Loss: 0.06071016, Accuracy: 95.90%\n",
      "Iteration:   663, Loss: 0.06142026, Accuracy: 96.10%\n",
      "Iteration:   664, Loss: 0.06054125, Accuracy: 96.20%\n",
      "Iteration:   665, Loss: 0.06084773, Accuracy: 96.30%\n",
      "Iteration:   666, Loss: 0.06095229, Accuracy: 96.20%\n",
      "Iteration:   667, Loss: 0.06040996, Accuracy: 96.40%\n",
      "Iteration:   668, Loss: 0.06021635, Accuracy: 96.30%\n",
      "Iteration:   669, Loss: 0.05979979, Accuracy: 96.30%\n",
      "Iteration:   670, Loss: 0.05982928, Accuracy: 96.10%\n",
      "Iteration:   671, Loss: 0.06004185, Accuracy: 96.10%\n",
      "Iteration:   672, Loss: 0.05967771, Accuracy: 96.20%\n",
      "Iteration:   673, Loss: 0.05939225, Accuracy: 96.10%\n",
      "Iteration:   674, Loss: 0.05996165, Accuracy: 96.40%\n",
      "Iteration:   675, Loss: 0.05954360, Accuracy: 96.20%\n",
      "Iteration:   676, Loss: 0.05977280, Accuracy: 95.90%\n",
      "Iteration:   677, Loss: 0.06020693, Accuracy: 96.20%\n",
      "Iteration:   678, Loss: 0.06119002, Accuracy: 96.30%\n",
      "Iteration:   679, Loss: 0.06136226, Accuracy: 96.00%\n",
      "Iteration:   680, Loss: 0.06196989, Accuracy: 96.00%\n",
      "Iteration:   681, Loss: 0.06287430, Accuracy: 96.10%\n",
      "Iteration:   682, Loss: 0.06143877, Accuracy: 96.10%\n",
      "Iteration:   683, Loss: 0.06153332, Accuracy: 96.10%\n",
      "Iteration:   684, Loss: 0.06169772, Accuracy: 96.10%\n",
      "Iteration:   685, Loss: 0.06201738, Accuracy: 96.60%\n",
      "Iteration:   686, Loss: 0.06201496, Accuracy: 96.00%\n",
      "Iteration:   687, Loss: 0.06265833, Accuracy: 96.50%\n",
      "Iteration:   688, Loss: 0.06251381, Accuracy: 96.10%\n",
      "Iteration:   689, Loss: 0.06139927, Accuracy: 96.30%\n",
      "Iteration:   690, Loss: 0.06177179, Accuracy: 96.60%\n",
      "Iteration:   691, Loss: 0.06121816, Accuracy: 97.00%\n",
      "Iteration:   692, Loss: 0.06138900, Accuracy: 96.00%\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[27], line 12\u001b[0m\n\u001b[1;32m      8\u001b[0m lr_str \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mstr\u001b[39m(lr)\u001b[38;5;241m.\u001b[39mreplace(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mp\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      9\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rates_train\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_test\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_nodes\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_iters\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_lr_01\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(n_train, n_test, n_nodes, n_iters)\u001b[38;5;241m+\u001b[39mlr_str\n\u001b[0;32m---> 12\u001b[0m (w1, w2, sr) \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_train\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     13\u001b[0m \u001b[43m               \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_test\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43mn_test\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m     14\u001b[0m \u001b[43m               \u001b[49m\u001b[43mn_hidden_nodes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_nodes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43miterations\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mn_iters\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# store the corrent success rates in the dictionary named success rates\u001b[39;00m\n\u001b[1;32m     16\u001b[0m success_rates[name] \u001b[38;5;241m=\u001b[39m sr\n",
      "Cell \u001b[0;32mIn[23], line 97\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;66;03m# if iteration == 800:\u001b[39;00m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;66;03m#     lr = 0.4\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;66;03m# if iteration == 1200:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[38;5;66;03m# if iteration == 2800:\u001b[39;00m\n\u001b[1;32m     95\u001b[0m \u001b[38;5;66;03m#     lr = 0.01\u001b[39;00m\n\u001b[1;32m     96\u001b[0m y_hat, h \u001b[38;5;241m=\u001b[39m forward(X_train, w1, w2)\n\u001b[0;32m---> 97\u001b[0m w1_gradient, w2_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mback\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mY_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     98\u001b[0m w1 \u001b[38;5;241m=\u001b[39m w1 \u001b[38;5;241m-\u001b[39m (w1_gradient \u001b[38;5;241m*\u001b[39m lr)\n\u001b[1;32m     99\u001b[0m w2 \u001b[38;5;241m=\u001b[39m w2 \u001b[38;5;241m-\u001b[39m (w2_gradient \u001b[38;5;241m*\u001b[39m lr)\n",
      "Cell \u001b[0;32mIn[23], line 36\u001b[0m, in \u001b[0;36mback\u001b[0;34m(X, Y, y_hat, w2, h)\u001b[0m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mback\u001b[39m(X, Y, y_hat, w2, h):\n\u001b[1;32m     35\u001b[0m     w2_gradient \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmatmul(prepend_bias(h)\u001b[38;5;241m.\u001b[39mT, (y_hat \u001b[38;5;241m-\u001b[39m Y)) \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m---> 36\u001b[0m     w1_gradient \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprepend_bias\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_hat\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mY\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mw2\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mT\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m                            \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msigmoid_gradient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mh\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m/\u001b[39m X\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m     38\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m (w1_gradient, w2_gradient)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_train = 60000 # number of examples in training data (max is 60,000)\n",
    "n_test  = 1000 # number of examples in training data (max is 10,000)\n",
    "n_nodes = 600 # number of hidden nodes\n",
    "n_iters = 1000 # number of iterations\n",
    "lr      = 0.7\n",
    "\n",
    "# create a string with the parameter values\n",
    "lr_str = str(lr).replace('.', 'p')\n",
    "name = 'success_rates_train{}_test{}_nodes{}_iters{}_lr_01'.format(n_train, n_test, n_nodes, n_iters)+lr_str\n",
    "\n",
    "\n",
    "(w1, w2, sr) = train(X_train[:n_train], Y_train[:n_train], \n",
    "               X_test[:n_test], Y_test[:n_test], \n",
    "               n_hidden_nodes=n_nodes, iterations=n_iters, lr=lr)\n",
    "# store the corrent success rates in the dictionary named success rates\n",
    "success_rates[name] = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f64a9f16",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'success_rates_train6000_test1000_nodes400_iters4000_lr0p8'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m plt\u001b[38;5;241m.\u001b[39mfigure(figsize \u001b[38;5;241m=\u001b[39m (\u001b[38;5;241m7\u001b[39m,\u001b[38;5;241m5\u001b[39m))\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m#plt.plot(success_rates['success_rates_train6000_test1000_nodes30_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes30_iters1000_lr0.03')\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43msuccess_rates\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43msuccess_rates_train6000_test1000_nodes400_iters4000_lr0p8\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m, label \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msuccess_rates_train6000_test1000_nodes100_iters1000_lr0.03\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#plt.plot(success_rates['success_rates_train6000_test1000_nodes300_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes300_iters1000_lr0.03')\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m#plt.plot(success_rates['success_rates_train6000_test1000_nodes784_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes784_iters1000_lr0.03')\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m#plt.plot(success_rates['success_rates_train6000_test1000_nodes1000_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes1000_iters1000_lr0.03')\u001b[39;00m\n\u001b[1;32m     10\u001b[0m plt\u001b[38;5;241m.\u001b[39mlegend()\n",
      "\u001b[0;31mKeyError\u001b[0m: 'success_rates_train6000_test1000_nodes400_iters4000_lr0p8'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 700x500 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here you can plot several training curves alongside each other \n",
    "# uncomment, delete, edit, copy-paste lines correspondingly\n",
    "# edit the NAME of the results to be plotted AND edit the LABEL, such that the legend is correct\n",
    "plt.figure(figsize = (7,5))\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes30_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes30_iters1000_lr0.03')\n",
    "plt.plot(success_rates['success_rates_train6000_test1000_nodes400_iters4000_lr0p8'], label = 'success_rates_train6000_test1000_nodes100_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes300_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes300_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes784_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes784_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes1000_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes1000_iters1000_lr0.03')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('success rate in %')\n",
    "plt.grid()\n",
    "plt.ylim([90,93]) # uncomment line to restrict the y-range for more detailed view on the late training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a41eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218b98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9baf391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises\n",
    "\n",
    "# explore the hyperparameters\n",
    "# - number of nodes\n",
    "# - learning rate\n",
    "# - size of traing data set\n",
    "#\n",
    "# keep size of test data set constant, e.g. 1000\n",
    "# \n",
    "# a strong increase in number of iterations beyond 1000 is  \n",
    "# recommended only AFTER exploring other prameters to some extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a484ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979e292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
