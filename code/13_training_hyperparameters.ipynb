{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "108c15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "981c1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a NumPy array of bytes:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the pixels into a matrix where each line is an image:\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "\n",
    "# 60000 images, each 784 elements (28 * 28 pixels)\n",
    "X_train = load_images(\"../data/mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 784 elements, with the same structure as X_train\n",
    "X_test = load_images(\"../data/mnist/t10k-images-idx3-ubyte.gz\")\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    n_labels = Y.shape[0]\n",
    "    n_classes = 10\n",
    "    encoded_Y = np.zeros((n_labels, n_classes))\n",
    "    for i in range(n_labels):\n",
    "        label = Y[i]\n",
    "        encoded_Y[i][label] = 1\n",
    "    return encoded_Y\n",
    "\n",
    "\n",
    "# !!! EDIT PATHS TO WHERE YOUR MNIST DATA IS !!!\n",
    "\n",
    "# 60K labels, each a single digit from 0 to 9\n",
    "Y_train_unencoded = load_labels(\"../data/mnist/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# 60K labels, each consisting of 10 one-hot encoded elements\n",
    "Y_train = one_hot_encode(Y_train_unencoded)\n",
    "\n",
    "# 10000 labels, each a single digit from 0 to 9\n",
    "Y_test = load_labels(\"../data/mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848b839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "0de8956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network implementation (almost the same as backpropagation.py,\n",
    "# except for a tiny refactoring in the back() function).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exponentials = np.exp(logits)\n",
    "    return exponentials / np.sum(exponentials, axis=1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def sigmoid_gradient(sigmoid):\n",
    "    return np.multiply(sigmoid, (1 - sigmoid))\n",
    "\n",
    "\n",
    "def loss(Y, y_hat):\n",
    "    return -np.sum(Y * np.log(y_hat)) / Y.shape[0]\n",
    "\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "def forward(X, w1, w2):\n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))\n",
    "    return (y_hat, h)\n",
    "\n",
    "\n",
    "def back(X, Y, y_hat, w2, h):\n",
    "    w2_gradient = np.matmul(prepend_bias(h).T, (y_hat - Y)) / X.shape[0]\n",
    "    w1_gradient = np.matmul(prepend_bias(X).T, np.matmul(y_hat - Y, w2[1:].T)\n",
    "                            * sigmoid_gradient(h)) / X.shape[0]\n",
    "    return (w1_gradient, w2_gradient)\n",
    "\n",
    "\n",
    "def classify(X, w1, w2):\n",
    "    y_hat, _ = forward(X, w1, w2)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def initialize_weights(n_input_variables, n_hidden_nodes, n_classes):\n",
    "    w1_rows = n_input_variables + 1\n",
    "    w1 = np.random.randn(w1_rows, n_hidden_nodes) * np.sqrt(1 / w1_rows)\n",
    "\n",
    "    w2_rows = n_hidden_nodes + 1\n",
    "    w2 = np.random.randn(w2_rows, n_classes) * np.sqrt(1 / w2_rows)\n",
    "\n",
    "    return (w1, w2)\n",
    "\n",
    "\n",
    "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat, _ = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(\"Iteration: %5d, Loss: %.8f, Accuracy: %.2f%%\" %\n",
    "          (iteration, training_loss, accuracy))\n",
    "    return accuracy\n",
    "\n",
    "def report_JR(iteration, X_train, Y_train, X_test, Y_test, w):\n",
    "    matches = np.count_nonzero(classify(X_test, w) == Y_test)\n",
    "    n_test_ex = Y_test.shape[0]\n",
    "    matches = matches * 100.0 / n_test_ex\n",
    "    training_loss = loss(X_train, Y_train, w)\n",
    "    print('iteration {} - loss: {:.2f}, matches: {:.2f}%'.format(iteration, training_loss, matches))\n",
    "    return matches\n",
    "\n",
    "    \n",
    "\n",
    "def train(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr):\n",
    "    success_rates = []\n",
    "    n_input_variables = X_train.shape[1]\n",
    "    n_classes = Y_train.shape[1]\n",
    "    w1, w2 = initialize_weights(n_input_variables, n_hidden_nodes, n_classes)\n",
    "    for iteration in range(iterations):\n",
    "        y_hat, h = forward(X_train, w1, w2)\n",
    "        w1_gradient, w2_gradient = back(X_train, Y_train, y_hat, w2, h)\n",
    "        w1 = w1 - (w1_gradient * lr)\n",
    "        w2 = w2 - (w2_gradient * lr)\n",
    "        accuracy = report(iteration, X_train, Y_train, X_test, Y_test, w1, w2)\n",
    "        success_rates.append(accuracy)\n",
    "    return (w1, w2, success_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f6912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f066d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "aa9a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store your success rates\n",
    "# !!! UNCOMMENT, EXECUTE, AND COMMENT THE LINE BELOW AGAIN !!!\n",
    "# !!! DON'T EXECUTE THE LINE AGAIN AFTER STARTING TO COLLECT RESULTS !!!\n",
    "\n",
    "#success_rates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "78a59014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:     0, Loss: 2.37164695, Accuracy: 15.70%\n",
      "Iteration:     1, Loss: 2.29462793, Accuracy: 32.30%\n",
      "Iteration:     2, Loss: 2.03039867, Accuracy: 22.90%\n",
      "Iteration:     3, Loss: 2.04196651, Accuracy: 50.70%\n",
      "Iteration:     4, Loss: 1.46653734, Accuracy: 47.30%\n",
      "Iteration:     5, Loss: 1.57082043, Accuracy: 55.50%\n",
      "Iteration:     6, Loss: 1.34825780, Accuracy: 46.80%\n",
      "Iteration:     7, Loss: 1.42520856, Accuracy: 53.10%\n",
      "Iteration:     8, Loss: 1.33148927, Accuracy: 51.80%\n",
      "Iteration:     9, Loss: 1.27331302, Accuracy: 63.20%\n",
      "Iteration:    10, Loss: 1.07850681, Accuracy: 58.80%\n",
      "Iteration:    11, Loss: 1.04265363, Accuracy: 63.20%\n",
      "Iteration:    12, Loss: 0.99841548, Accuracy: 59.00%\n",
      "Iteration:    13, Loss: 1.06728162, Accuracy: 66.20%\n",
      "Iteration:    14, Loss: 0.82660532, Accuracy: 70.70%\n",
      "Iteration:    15, Loss: 0.80988150, Accuracy: 69.50%\n",
      "Iteration:    16, Loss: 0.77379814, Accuracy: 69.70%\n",
      "Iteration:    17, Loss: 0.82533940, Accuracy: 70.00%\n",
      "Iteration:    18, Loss: 0.71785554, Accuracy: 72.80%\n",
      "Iteration:    19, Loss: 0.71851452, Accuracy: 71.00%\n",
      "Iteration:    20, Loss: 0.68590840, Accuracy: 73.30%\n",
      "Iteration:    21, Loss: 0.70633209, Accuracy: 71.50%\n",
      "Iteration:    22, Loss: 0.64317767, Accuracy: 74.50%\n",
      "Iteration:    23, Loss: 0.64685561, Accuracy: 72.70%\n",
      "Iteration:    24, Loss: 0.60314639, Accuracy: 76.00%\n",
      "Iteration:    25, Loss: 0.60499818, Accuracy: 74.20%\n",
      "Iteration:    26, Loss: 0.56456445, Accuracy: 77.80%\n",
      "Iteration:    27, Loss: 0.56209050, Accuracy: 76.00%\n",
      "Iteration:    28, Loss: 0.52915018, Accuracy: 78.30%\n",
      "Iteration:    29, Loss: 0.52391033, Accuracy: 78.30%\n",
      "Iteration:    30, Loss: 0.49789516, Accuracy: 79.60%\n",
      "Iteration:    31, Loss: 0.49094493, Accuracy: 79.60%\n",
      "Iteration:    32, Loss: 0.47090446, Accuracy: 80.60%\n",
      "Iteration:    33, Loss: 0.46314262, Accuracy: 80.50%\n",
      "Iteration:    34, Loss: 0.44795544, Accuracy: 81.40%\n",
      "Iteration:    35, Loss: 0.44011069, Accuracy: 82.60%\n",
      "Iteration:    36, Loss: 0.42864423, Accuracy: 82.10%\n",
      "Iteration:    37, Loss: 0.42124057, Accuracy: 82.70%\n",
      "Iteration:    38, Loss: 0.41244793, Accuracy: 82.80%\n",
      "Iteration:    39, Loss: 0.40575591, Accuracy: 83.10%\n",
      "Iteration:    40, Loss: 0.39877198, Accuracy: 83.00%\n",
      "Iteration:    41, Loss: 0.39283711, Accuracy: 83.60%\n",
      "Iteration:    42, Loss: 0.38702559, Accuracy: 83.30%\n",
      "Iteration:    43, Loss: 0.38175292, Accuracy: 83.90%\n",
      "Iteration:    44, Loss: 0.37669573, Accuracy: 83.50%\n",
      "Iteration:    45, Loss: 0.37194053, Accuracy: 84.00%\n",
      "Iteration:    46, Loss: 0.36737821, Accuracy: 83.80%\n",
      "Iteration:    47, Loss: 0.36300965, Accuracy: 84.60%\n",
      "Iteration:    48, Loss: 0.35879694, Accuracy: 84.40%\n",
      "Iteration:    49, Loss: 0.35472939, Accuracy: 84.50%\n",
      "Iteration:    50, Loss: 0.35079125, Accuracy: 84.50%\n",
      "Iteration:    51, Loss: 0.34697264, Accuracy: 84.80%\n",
      "Iteration:    52, Loss: 0.34326422, Accuracy: 84.70%\n",
      "Iteration:    53, Loss: 0.33965883, Accuracy: 84.80%\n",
      "Iteration:    54, Loss: 0.33615023, Accuracy: 84.80%\n",
      "Iteration:    55, Loss: 0.33273305, Accuracy: 84.90%\n",
      "Iteration:    56, Loss: 0.32940256, Accuracy: 84.90%\n",
      "Iteration:    57, Loss: 0.32615458, Accuracy: 85.00%\n",
      "Iteration:    58, Loss: 0.32298533, Accuracy: 85.00%\n",
      "Iteration:    59, Loss: 0.31989142, Accuracy: 85.00%\n",
      "Iteration:    60, Loss: 0.31686983, Accuracy: 85.10%\n",
      "Iteration:    61, Loss: 0.31391782, Accuracy: 85.40%\n",
      "Iteration:    62, Loss: 0.31103295, Accuracy: 85.40%\n",
      "Iteration:    63, Loss: 0.30821295, Accuracy: 85.50%\n",
      "Iteration:    64, Loss: 0.30545566, Accuracy: 85.50%\n",
      "Iteration:    65, Loss: 0.30275887, Accuracy: 85.50%\n",
      "Iteration:    66, Loss: 0.30012037, Accuracy: 85.50%\n",
      "Iteration:    67, Loss: 0.29753784, Accuracy: 85.60%\n",
      "Iteration:    68, Loss: 0.29500912, Accuracy: 85.70%\n",
      "Iteration:    69, Loss: 0.29253285, Accuracy: 85.70%\n",
      "Iteration:    70, Loss: 0.29010869, Accuracy: 85.80%\n",
      "Iteration:    71, Loss: 0.28773553, Accuracy: 85.90%\n",
      "Iteration:    72, Loss: 0.28541127, Accuracy: 85.90%\n",
      "Iteration:    73, Loss: 0.28313392, Accuracy: 85.90%\n",
      "Iteration:    74, Loss: 0.28090184, Accuracy: 85.90%\n",
      "Iteration:    75, Loss: 0.27871352, Accuracy: 85.90%\n",
      "Iteration:    76, Loss: 0.27656749, Accuracy: 85.90%\n",
      "Iteration:    77, Loss: 0.27446227, Accuracy: 85.90%\n",
      "Iteration:    78, Loss: 0.27239644, Accuracy: 85.90%\n",
      "Iteration:    79, Loss: 0.27036868, Accuracy: 85.80%\n",
      "Iteration:    80, Loss: 0.26837773, Accuracy: 85.80%\n",
      "Iteration:    81, Loss: 0.26642241, Accuracy: 85.80%\n",
      "Iteration:    82, Loss: 0.26450159, Accuracy: 85.70%\n",
      "Iteration:    83, Loss: 0.26261413, Accuracy: 85.80%\n",
      "Iteration:    84, Loss: 0.26075892, Accuracy: 86.00%\n",
      "Iteration:    85, Loss: 0.25893492, Accuracy: 86.00%\n",
      "Iteration:    86, Loss: 0.25714121, Accuracy: 86.00%\n",
      "Iteration:    87, Loss: 0.25537700, Accuracy: 86.00%\n",
      "Iteration:    88, Loss: 0.25364152, Accuracy: 86.00%\n",
      "Iteration:    89, Loss: 0.25193392, Accuracy: 86.00%\n",
      "Iteration:    90, Loss: 0.25025341, Accuracy: 86.00%\n",
      "Iteration:    91, Loss: 0.24859941, Accuracy: 86.00%\n",
      "Iteration:    92, Loss: 0.24697150, Accuracy: 86.00%\n",
      "Iteration:    93, Loss: 0.24536924, Accuracy: 86.00%\n",
      "Iteration:    94, Loss: 0.24379213, Accuracy: 86.00%\n",
      "Iteration:    95, Loss: 0.24223951, Accuracy: 86.10%\n",
      "Iteration:    96, Loss: 0.24071070, Accuracy: 86.10%\n",
      "Iteration:    97, Loss: 0.23920500, Accuracy: 86.10%\n",
      "Iteration:    98, Loss: 0.23772179, Accuracy: 86.20%\n",
      "Iteration:    99, Loss: 0.23626051, Accuracy: 86.20%\n",
      "Iteration:   100, Loss: 0.23482065, Accuracy: 86.20%\n",
      "Iteration:   101, Loss: 0.23340169, Accuracy: 86.20%\n",
      "Iteration:   102, Loss: 0.23200303, Accuracy: 86.20%\n",
      "Iteration:   103, Loss: 0.23062403, Accuracy: 86.30%\n",
      "Iteration:   104, Loss: 0.22926407, Accuracy: 86.40%\n",
      "Iteration:   105, Loss: 0.22792267, Accuracy: 86.40%\n",
      "Iteration:   106, Loss: 0.22659944, Accuracy: 86.40%\n",
      "Iteration:   107, Loss: 0.22529413, Accuracy: 86.50%\n",
      "Iteration:   108, Loss: 0.22400642, Accuracy: 86.50%\n",
      "Iteration:   109, Loss: 0.22273592, Accuracy: 86.60%\n",
      "Iteration:   110, Loss: 0.22148223, Accuracy: 86.70%\n",
      "Iteration:   111, Loss: 0.22024498, Accuracy: 86.70%\n",
      "Iteration:   112, Loss: 0.21902384, Accuracy: 86.70%\n",
      "Iteration:   113, Loss: 0.21781849, Accuracy: 86.70%\n",
      "Iteration:   114, Loss: 0.21662855, Accuracy: 86.70%\n",
      "Iteration:   115, Loss: 0.21545368, Accuracy: 86.80%\n",
      "Iteration:   116, Loss: 0.21429350, Accuracy: 86.80%\n",
      "Iteration:   117, Loss: 0.21314762, Accuracy: 86.90%\n",
      "Iteration:   118, Loss: 0.21201568, Accuracy: 86.90%\n",
      "Iteration:   119, Loss: 0.21089733, Accuracy: 86.90%\n",
      "Iteration:   120, Loss: 0.20979228, Accuracy: 87.00%\n",
      "Iteration:   121, Loss: 0.20870031, Accuracy: 87.00%\n",
      "Iteration:   122, Loss: 0.20762120, Accuracy: 87.00%\n",
      "Iteration:   123, Loss: 0.20655473, Accuracy: 87.00%\n",
      "Iteration:   124, Loss: 0.20550071, Accuracy: 87.00%\n",
      "Iteration:   125, Loss: 0.20445894, Accuracy: 87.10%\n",
      "Iteration:   126, Loss: 0.20342918, Accuracy: 87.10%\n",
      "Iteration:   127, Loss: 0.20241117, Accuracy: 87.00%\n",
      "Iteration:   128, Loss: 0.20140470, Accuracy: 87.10%\n",
      "Iteration:   129, Loss: 0.20040963, Accuracy: 87.10%\n",
      "Iteration:   130, Loss: 0.19942582, Accuracy: 87.10%\n",
      "Iteration:   131, Loss: 0.19845316, Accuracy: 87.10%\n",
      "Iteration:   132, Loss: 0.19749155, Accuracy: 87.00%\n",
      "Iteration:   133, Loss: 0.19654087, Accuracy: 87.10%\n",
      "Iteration:   134, Loss: 0.19560099, Accuracy: 87.10%\n",
      "Iteration:   135, Loss: 0.19467175, Accuracy: 87.10%\n",
      "Iteration:   136, Loss: 0.19375291, Accuracy: 87.10%\n",
      "Iteration:   137, Loss: 0.19284415, Accuracy: 87.10%\n",
      "Iteration:   138, Loss: 0.19194511, Accuracy: 87.10%\n",
      "Iteration:   139, Loss: 0.19105540, Accuracy: 87.10%\n",
      "Iteration:   140, Loss: 0.19017469, Accuracy: 87.10%\n",
      "Iteration:   141, Loss: 0.18930282, Accuracy: 87.10%\n",
      "Iteration:   142, Loss: 0.18843979, Accuracy: 87.10%\n",
      "Iteration:   143, Loss: 0.18758564, Accuracy: 87.20%\n",
      "Iteration:   144, Loss: 0.18674023, Accuracy: 87.20%\n",
      "Iteration:   145, Loss: 0.18590330, Accuracy: 87.20%\n",
      "Iteration:   146, Loss: 0.18507461, Accuracy: 87.20%\n",
      "Iteration:   147, Loss: 0.18425404, Accuracy: 87.20%\n",
      "Iteration:   148, Loss: 0.18344157, Accuracy: 87.20%\n",
      "Iteration:   149, Loss: 0.18263715, Accuracy: 87.20%\n",
      "Iteration:   150, Loss: 0.18184064, Accuracy: 87.20%\n",
      "Iteration:   151, Loss: 0.18105193, Accuracy: 87.20%\n",
      "Iteration:   152, Loss: 0.18027096, Accuracy: 87.30%\n",
      "Iteration:   153, Loss: 0.17949759, Accuracy: 87.30%\n",
      "Iteration:   154, Loss: 0.17873167, Accuracy: 87.30%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   155, Loss: 0.17797299, Accuracy: 87.40%\n",
      "Iteration:   156, Loss: 0.17722136, Accuracy: 87.30%\n",
      "Iteration:   157, Loss: 0.17647664, Accuracy: 87.30%\n",
      "Iteration:   158, Loss: 0.17573874, Accuracy: 87.30%\n",
      "Iteration:   159, Loss: 0.17500761, Accuracy: 87.30%\n",
      "Iteration:   160, Loss: 0.17428312, Accuracy: 87.30%\n",
      "Iteration:   161, Loss: 0.17356512, Accuracy: 87.40%\n",
      "Iteration:   162, Loss: 0.17285351, Accuracy: 87.40%\n",
      "Iteration:   163, Loss: 0.17214824, Accuracy: 87.40%\n",
      "Iteration:   164, Loss: 0.17144929, Accuracy: 87.40%\n",
      "Iteration:   165, Loss: 0.17075658, Accuracy: 87.40%\n",
      "Iteration:   166, Loss: 0.17006998, Accuracy: 87.40%\n",
      "Iteration:   167, Loss: 0.16938939, Accuracy: 87.50%\n",
      "Iteration:   168, Loss: 0.16871468, Accuracy: 87.50%\n",
      "Iteration:   169, Loss: 0.16804578, Accuracy: 87.50%\n",
      "Iteration:   170, Loss: 0.16738256, Accuracy: 87.50%\n",
      "Iteration:   171, Loss: 0.16672490, Accuracy: 87.50%\n",
      "Iteration:   172, Loss: 0.16607266, Accuracy: 87.50%\n",
      "Iteration:   173, Loss: 0.16542571, Accuracy: 87.50%\n",
      "Iteration:   174, Loss: 0.16478398, Accuracy: 87.50%\n",
      "Iteration:   175, Loss: 0.16414743, Accuracy: 87.50%\n",
      "Iteration:   176, Loss: 0.16351609, Accuracy: 87.50%\n",
      "Iteration:   177, Loss: 0.16288998, Accuracy: 87.50%\n",
      "Iteration:   178, Loss: 0.16226913, Accuracy: 87.50%\n",
      "Iteration:   179, Loss: 0.16165350, Accuracy: 87.50%\n",
      "Iteration:   180, Loss: 0.16104304, Accuracy: 87.50%\n",
      "Iteration:   181, Loss: 0.16043764, Accuracy: 87.50%\n",
      "Iteration:   182, Loss: 0.15983723, Accuracy: 87.50%\n",
      "Iteration:   183, Loss: 0.15924174, Accuracy: 87.60%\n",
      "Iteration:   184, Loss: 0.15865111, Accuracy: 87.60%\n",
      "Iteration:   185, Loss: 0.15806524, Accuracy: 87.70%\n",
      "Iteration:   186, Loss: 0.15748402, Accuracy: 87.70%\n",
      "Iteration:   187, Loss: 0.15690737, Accuracy: 87.70%\n",
      "Iteration:   188, Loss: 0.15633522, Accuracy: 87.70%\n",
      "Iteration:   189, Loss: 0.15576753, Accuracy: 87.80%\n",
      "Iteration:   190, Loss: 0.15520424, Accuracy: 87.80%\n",
      "Iteration:   191, Loss: 0.15464530, Accuracy: 87.80%\n",
      "Iteration:   192, Loss: 0.15409067, Accuracy: 87.80%\n",
      "Iteration:   193, Loss: 0.15354031, Accuracy: 87.80%\n",
      "Iteration:   194, Loss: 0.15299422, Accuracy: 87.80%\n",
      "Iteration:   195, Loss: 0.15245237, Accuracy: 87.80%\n",
      "Iteration:   196, Loss: 0.15191472, Accuracy: 87.80%\n",
      "Iteration:   197, Loss: 0.15138124, Accuracy: 87.80%\n",
      "Iteration:   198, Loss: 0.15085184, Accuracy: 87.80%\n",
      "Iteration:   199, Loss: 0.15032642, Accuracy: 87.80%\n",
      "Iteration:   200, Loss: 0.14980485, Accuracy: 87.80%\n",
      "Iteration:   201, Loss: 0.14928700, Accuracy: 87.80%\n",
      "Iteration:   202, Loss: 0.14877276, Accuracy: 87.80%\n",
      "Iteration:   203, Loss: 0.14826202, Accuracy: 87.80%\n",
      "Iteration:   204, Loss: 0.14775469, Accuracy: 87.80%\n",
      "Iteration:   205, Loss: 0.14725070, Accuracy: 87.80%\n",
      "Iteration:   206, Loss: 0.14675003, Accuracy: 87.90%\n",
      "Iteration:   207, Loss: 0.14625268, Accuracy: 87.90%\n",
      "Iteration:   208, Loss: 0.14575874, Accuracy: 87.90%\n",
      "Iteration:   209, Loss: 0.14526827, Accuracy: 87.90%\n",
      "Iteration:   210, Loss: 0.14478136, Accuracy: 88.00%\n",
      "Iteration:   211, Loss: 0.14429807, Accuracy: 88.00%\n",
      "Iteration:   212, Loss: 0.14381843, Accuracy: 88.00%\n",
      "Iteration:   213, Loss: 0.14334243, Accuracy: 88.00%\n",
      "Iteration:   214, Loss: 0.14287004, Accuracy: 88.00%\n",
      "Iteration:   215, Loss: 0.14240121, Accuracy: 88.00%\n",
      "Iteration:   216, Loss: 0.14193589, Accuracy: 88.00%\n",
      "Iteration:   217, Loss: 0.14147404, Accuracy: 88.00%\n",
      "Iteration:   218, Loss: 0.14101557, Accuracy: 88.00%\n",
      "Iteration:   219, Loss: 0.14056040, Accuracy: 88.00%\n",
      "Iteration:   220, Loss: 0.14010842, Accuracy: 88.00%\n",
      "Iteration:   221, Loss: 0.13965952, Accuracy: 88.00%\n",
      "Iteration:   222, Loss: 0.13921364, Accuracy: 88.00%\n",
      "Iteration:   223, Loss: 0.13877070, Accuracy: 88.00%\n",
      "Iteration:   224, Loss: 0.13833065, Accuracy: 88.00%\n",
      "Iteration:   225, Loss: 0.13789346, Accuracy: 88.00%\n",
      "Iteration:   226, Loss: 0.13745910, Accuracy: 88.00%\n",
      "Iteration:   227, Loss: 0.13702756, Accuracy: 88.00%\n",
      "Iteration:   228, Loss: 0.13659885, Accuracy: 88.00%\n",
      "Iteration:   229, Loss: 0.13617297, Accuracy: 88.00%\n",
      "Iteration:   230, Loss: 0.13574992, Accuracy: 88.00%\n",
      "Iteration:   231, Loss: 0.13532966, Accuracy: 88.10%\n",
      "Iteration:   232, Loss: 0.13491210, Accuracy: 88.10%\n",
      "Iteration:   233, Loss: 0.13449717, Accuracy: 88.10%\n",
      "Iteration:   234, Loss: 0.13408483, Accuracy: 88.10%\n",
      "Iteration:   235, Loss: 0.13367504, Accuracy: 88.20%\n",
      "Iteration:   236, Loss: 0.13326779, Accuracy: 88.20%\n",
      "Iteration:   237, Loss: 0.13286303, Accuracy: 88.20%\n",
      "Iteration:   238, Loss: 0.13246074, Accuracy: 88.20%\n",
      "Iteration:   239, Loss: 0.13206093, Accuracy: 88.20%\n",
      "Iteration:   240, Loss: 0.13166364, Accuracy: 88.20%\n",
      "Iteration:   241, Loss: 0.13126883, Accuracy: 88.30%\n",
      "Iteration:   242, Loss: 0.13087644, Accuracy: 88.30%\n",
      "Iteration:   243, Loss: 0.13048638, Accuracy: 88.30%\n",
      "Iteration:   244, Loss: 0.13009856, Accuracy: 88.30%\n",
      "Iteration:   245, Loss: 0.12971291, Accuracy: 88.30%\n",
      "Iteration:   246, Loss: 0.12932935, Accuracy: 88.30%\n",
      "Iteration:   247, Loss: 0.12894782, Accuracy: 88.30%\n",
      "Iteration:   248, Loss: 0.12856825, Accuracy: 88.30%\n",
      "Iteration:   249, Loss: 0.12819058, Accuracy: 88.40%\n",
      "Iteration:   250, Loss: 0.12781478, Accuracy: 88.40%\n",
      "Iteration:   251, Loss: 0.12744086, Accuracy: 88.40%\n",
      "Iteration:   252, Loss: 0.12706887, Accuracy: 88.40%\n",
      "Iteration:   253, Loss: 0.12669886, Accuracy: 88.40%\n",
      "Iteration:   254, Loss: 0.12633087, Accuracy: 88.40%\n",
      "Iteration:   255, Loss: 0.12596488, Accuracy: 88.40%\n",
      "Iteration:   256, Loss: 0.12560088, Accuracy: 88.40%\n",
      "Iteration:   257, Loss: 0.12523884, Accuracy: 88.30%\n",
      "Iteration:   258, Loss: 0.12487878, Accuracy: 88.40%\n",
      "Iteration:   259, Loss: 0.12452075, Accuracy: 88.50%\n",
      "Iteration:   260, Loss: 0.12416480, Accuracy: 88.50%\n",
      "Iteration:   261, Loss: 0.12381099, Accuracy: 88.60%\n",
      "Iteration:   262, Loss: 0.12345929, Accuracy: 88.60%\n",
      "Iteration:   263, Loss: 0.12310968, Accuracy: 88.60%\n",
      "Iteration:   264, Loss: 0.12276209, Accuracy: 88.60%\n",
      "Iteration:   265, Loss: 0.12241648, Accuracy: 88.60%\n",
      "Iteration:   266, Loss: 0.12207282, Accuracy: 88.60%\n",
      "Iteration:   267, Loss: 0.12173106, Accuracy: 88.60%\n",
      "Iteration:   268, Loss: 0.12139116, Accuracy: 88.60%\n",
      "Iteration:   269, Loss: 0.12105305, Accuracy: 88.60%\n",
      "Iteration:   270, Loss: 0.12071667, Accuracy: 88.60%\n",
      "Iteration:   271, Loss: 0.12038199, Accuracy: 88.80%\n",
      "Iteration:   272, Loss: 0.12004897, Accuracy: 88.80%\n",
      "Iteration:   273, Loss: 0.11971760, Accuracy: 88.80%\n",
      "Iteration:   274, Loss: 0.11938789, Accuracy: 88.80%\n",
      "Iteration:   275, Loss: 0.11905985, Accuracy: 88.80%\n",
      "Iteration:   276, Loss: 0.11873347, Accuracy: 88.80%\n",
      "Iteration:   277, Loss: 0.11840874, Accuracy: 88.80%\n",
      "Iteration:   278, Loss: 0.11808565, Accuracy: 88.80%\n",
      "Iteration:   279, Loss: 0.11776423, Accuracy: 88.80%\n",
      "Iteration:   280, Loss: 0.11744446, Accuracy: 88.80%\n",
      "Iteration:   281, Loss: 0.11712637, Accuracy: 88.80%\n",
      "Iteration:   282, Loss: 0.11680997, Accuracy: 88.80%\n",
      "Iteration:   283, Loss: 0.11649527, Accuracy: 88.80%\n",
      "Iteration:   284, Loss: 0.11618229, Accuracy: 88.80%\n",
      "Iteration:   285, Loss: 0.11587103, Accuracy: 88.80%\n",
      "Iteration:   286, Loss: 0.11556146, Accuracy: 88.80%\n",
      "Iteration:   287, Loss: 0.11525353, Accuracy: 88.80%\n",
      "Iteration:   288, Loss: 0.11494719, Accuracy: 88.80%\n",
      "Iteration:   289, Loss: 0.11464242, Accuracy: 88.80%\n",
      "Iteration:   290, Loss: 0.11433920, Accuracy: 88.80%\n",
      "Iteration:   291, Loss: 0.11403752, Accuracy: 88.80%\n",
      "Iteration:   292, Loss: 0.11373737, Accuracy: 88.80%\n",
      "Iteration:   293, Loss: 0.11343875, Accuracy: 88.90%\n",
      "Iteration:   294, Loss: 0.11314162, Accuracy: 88.90%\n",
      "Iteration:   295, Loss: 0.11284596, Accuracy: 88.90%\n",
      "Iteration:   296, Loss: 0.11255174, Accuracy: 88.90%\n",
      "Iteration:   297, Loss: 0.11225894, Accuracy: 88.90%\n",
      "Iteration:   298, Loss: 0.11196753, Accuracy: 88.90%\n",
      "Iteration:   299, Loss: 0.11167750, Accuracy: 88.90%\n",
      "Iteration:   300, Loss: 0.11138885, Accuracy: 88.90%\n",
      "Iteration:   301, Loss: 0.11110157, Accuracy: 88.90%\n",
      "Iteration:   302, Loss: 0.11081568, Accuracy: 88.90%\n",
      "Iteration:   303, Loss: 0.11053118, Accuracy: 88.90%\n",
      "Iteration:   304, Loss: 0.11024807, Accuracy: 88.90%\n",
      "Iteration:   305, Loss: 0.10996634, Accuracy: 88.90%\n",
      "Iteration:   306, Loss: 0.10968598, Accuracy: 88.90%\n",
      "Iteration:   307, Loss: 0.10940694, Accuracy: 88.90%\n",
      "Iteration:   308, Loss: 0.10912923, Accuracy: 88.90%\n",
      "Iteration:   309, Loss: 0.10885281, Accuracy: 88.90%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   310, Loss: 0.10857767, Accuracy: 88.90%\n",
      "Iteration:   311, Loss: 0.10830381, Accuracy: 88.90%\n",
      "Iteration:   312, Loss: 0.10803120, Accuracy: 88.90%\n",
      "Iteration:   313, Loss: 0.10775980, Accuracy: 88.90%\n",
      "Iteration:   314, Loss: 0.10748957, Accuracy: 88.90%\n",
      "Iteration:   315, Loss: 0.10722048, Accuracy: 88.90%\n",
      "Iteration:   316, Loss: 0.10695255, Accuracy: 88.90%\n",
      "Iteration:   317, Loss: 0.10668585, Accuracy: 88.90%\n",
      "Iteration:   318, Loss: 0.10642044, Accuracy: 88.90%\n",
      "Iteration:   319, Loss: 0.10615634, Accuracy: 88.90%\n",
      "Iteration:   320, Loss: 0.10589360, Accuracy: 88.90%\n",
      "Iteration:   321, Loss: 0.10563227, Accuracy: 88.90%\n",
      "Iteration:   322, Loss: 0.10537233, Accuracy: 88.90%\n",
      "Iteration:   323, Loss: 0.10511374, Accuracy: 88.90%\n",
      "Iteration:   324, Loss: 0.10485646, Accuracy: 88.90%\n",
      "Iteration:   325, Loss: 0.10460043, Accuracy: 88.90%\n",
      "Iteration:   326, Loss: 0.10434564, Accuracy: 88.90%\n",
      "Iteration:   327, Loss: 0.10409208, Accuracy: 88.90%\n",
      "Iteration:   328, Loss: 0.10383972, Accuracy: 88.90%\n",
      "Iteration:   329, Loss: 0.10358857, Accuracy: 88.90%\n",
      "Iteration:   330, Loss: 0.10333859, Accuracy: 88.90%\n",
      "Iteration:   331, Loss: 0.10308979, Accuracy: 88.90%\n",
      "Iteration:   332, Loss: 0.10284214, Accuracy: 88.90%\n",
      "Iteration:   333, Loss: 0.10259563, Accuracy: 88.90%\n",
      "Iteration:   334, Loss: 0.10235025, Accuracy: 88.90%\n",
      "Iteration:   335, Loss: 0.10210599, Accuracy: 88.90%\n",
      "Iteration:   336, Loss: 0.10186283, Accuracy: 88.90%\n",
      "Iteration:   337, Loss: 0.10162076, Accuracy: 88.90%\n",
      "Iteration:   338, Loss: 0.10137975, Accuracy: 89.00%\n",
      "Iteration:   339, Loss: 0.10113978, Accuracy: 89.00%\n",
      "Iteration:   340, Loss: 0.10090083, Accuracy: 89.00%\n",
      "Iteration:   341, Loss: 0.10066288, Accuracy: 89.00%\n",
      "Iteration:   342, Loss: 0.10042590, Accuracy: 89.00%\n",
      "Iteration:   343, Loss: 0.10018988, Accuracy: 89.00%\n",
      "Iteration:   344, Loss: 0.09995481, Accuracy: 89.10%\n",
      "Iteration:   345, Loss: 0.09972069, Accuracy: 89.10%\n",
      "Iteration:   346, Loss: 0.09948749, Accuracy: 89.10%\n",
      "Iteration:   347, Loss: 0.09925521, Accuracy: 89.10%\n",
      "Iteration:   348, Loss: 0.09902383, Accuracy: 89.10%\n",
      "Iteration:   349, Loss: 0.09879332, Accuracy: 89.10%\n",
      "Iteration:   350, Loss: 0.09856367, Accuracy: 89.10%\n",
      "Iteration:   351, Loss: 0.09833486, Accuracy: 89.10%\n",
      "Iteration:   352, Loss: 0.09810689, Accuracy: 89.10%\n",
      "Iteration:   353, Loss: 0.09787977, Accuracy: 89.10%\n",
      "Iteration:   354, Loss: 0.09765347, Accuracy: 89.10%\n",
      "Iteration:   355, Loss: 0.09742802, Accuracy: 89.10%\n",
      "Iteration:   356, Loss: 0.09720347, Accuracy: 89.10%\n",
      "Iteration:   357, Loss: 0.09697997, Accuracy: 89.10%\n",
      "Iteration:   358, Loss: 0.09675759, Accuracy: 89.10%\n",
      "Iteration:   359, Loss: 0.09653633, Accuracy: 89.10%\n",
      "Iteration:   360, Loss: 0.09631607, Accuracy: 89.10%\n",
      "Iteration:   361, Loss: 0.09609675, Accuracy: 89.10%\n",
      "Iteration:   362, Loss: 0.09587830, Accuracy: 89.10%\n",
      "Iteration:   363, Loss: 0.09566071, Accuracy: 89.10%\n",
      "Iteration:   364, Loss: 0.09544395, Accuracy: 89.10%\n",
      "Iteration:   365, Loss: 0.09522802, Accuracy: 89.10%\n",
      "Iteration:   366, Loss: 0.09501291, Accuracy: 89.10%\n",
      "Iteration:   367, Loss: 0.09479864, Accuracy: 89.10%\n",
      "Iteration:   368, Loss: 0.09458521, Accuracy: 89.10%\n",
      "Iteration:   369, Loss: 0.09437264, Accuracy: 89.10%\n",
      "Iteration:   370, Loss: 0.09416093, Accuracy: 89.10%\n",
      "Iteration:   371, Loss: 0.09395007, Accuracy: 89.10%\n",
      "Iteration:   372, Loss: 0.09374007, Accuracy: 89.10%\n",
      "Iteration:   373, Loss: 0.09353091, Accuracy: 89.10%\n",
      "Iteration:   374, Loss: 0.09332259, Accuracy: 89.10%\n",
      "Iteration:   375, Loss: 0.09311508, Accuracy: 89.10%\n",
      "Iteration:   376, Loss: 0.09290835, Accuracy: 89.10%\n",
      "Iteration:   377, Loss: 0.09270241, Accuracy: 89.10%\n",
      "Iteration:   378, Loss: 0.09249727, Accuracy: 89.10%\n",
      "Iteration:   379, Loss: 0.09229296, Accuracy: 89.10%\n",
      "Iteration:   380, Loss: 0.09208952, Accuracy: 89.10%\n",
      "Iteration:   381, Loss: 0.09188697, Accuracy: 89.10%\n",
      "Iteration:   382, Loss: 0.09168530, Accuracy: 89.10%\n",
      "Iteration:   383, Loss: 0.09148450, Accuracy: 89.10%\n",
      "Iteration:   384, Loss: 0.09128457, Accuracy: 89.10%\n",
      "Iteration:   385, Loss: 0.09108549, Accuracy: 89.10%\n",
      "Iteration:   386, Loss: 0.09088724, Accuracy: 89.10%\n",
      "Iteration:   387, Loss: 0.09068980, Accuracy: 89.10%\n",
      "Iteration:   388, Loss: 0.09049316, Accuracy: 89.10%\n",
      "Iteration:   389, Loss: 0.09029730, Accuracy: 89.10%\n",
      "Iteration:   390, Loss: 0.09010224, Accuracy: 89.10%\n",
      "Iteration:   391, Loss: 0.08990801, Accuracy: 89.10%\n",
      "Iteration:   392, Loss: 0.08971460, Accuracy: 89.10%\n",
      "Iteration:   393, Loss: 0.08952202, Accuracy: 89.10%\n",
      "Iteration:   394, Loss: 0.08933024, Accuracy: 89.10%\n",
      "Iteration:   395, Loss: 0.08913926, Accuracy: 89.10%\n",
      "Iteration:   396, Loss: 0.08894906, Accuracy: 89.10%\n",
      "Iteration:   397, Loss: 0.08875965, Accuracy: 89.10%\n",
      "Iteration:   398, Loss: 0.08857102, Accuracy: 89.10%\n",
      "Iteration:   399, Loss: 0.08838318, Accuracy: 89.10%\n",
      "Iteration:   400, Loss: 0.08819611, Accuracy: 89.10%\n",
      "Iteration:   401, Loss: 0.08800979, Accuracy: 89.10%\n",
      "Iteration:   402, Loss: 0.08782418, Accuracy: 89.10%\n",
      "Iteration:   403, Loss: 0.08763925, Accuracy: 89.10%\n",
      "Iteration:   404, Loss: 0.08745494, Accuracy: 89.10%\n",
      "Iteration:   405, Loss: 0.08727122, Accuracy: 89.10%\n",
      "Iteration:   406, Loss: 0.08708802, Accuracy: 89.20%\n",
      "Iteration:   407, Loss: 0.08690532, Accuracy: 89.20%\n",
      "Iteration:   408, Loss: 0.08672311, Accuracy: 89.20%\n",
      "Iteration:   409, Loss: 0.08654143, Accuracy: 89.20%\n",
      "Iteration:   410, Loss: 0.08636031, Accuracy: 89.20%\n",
      "Iteration:   411, Loss: 0.08617975, Accuracy: 89.20%\n",
      "Iteration:   412, Loss: 0.08599978, Accuracy: 89.20%\n",
      "Iteration:   413, Loss: 0.08582043, Accuracy: 89.30%\n",
      "Iteration:   414, Loss: 0.08564175, Accuracy: 89.30%\n",
      "Iteration:   415, Loss: 0.08546374, Accuracy: 89.30%\n",
      "Iteration:   416, Loss: 0.08528636, Accuracy: 89.20%\n",
      "Iteration:   417, Loss: 0.08510957, Accuracy: 89.20%\n",
      "Iteration:   418, Loss: 0.08493332, Accuracy: 89.20%\n",
      "Iteration:   419, Loss: 0.08475758, Accuracy: 89.20%\n",
      "Iteration:   420, Loss: 0.08458229, Accuracy: 89.20%\n",
      "Iteration:   421, Loss: 0.08440744, Accuracy: 89.20%\n",
      "Iteration:   422, Loss: 0.08423299, Accuracy: 89.20%\n",
      "Iteration:   423, Loss: 0.08405890, Accuracy: 89.20%\n",
      "Iteration:   424, Loss: 0.08388522, Accuracy: 89.20%\n",
      "Iteration:   425, Loss: 0.08371209, Accuracy: 89.20%\n",
      "Iteration:   426, Loss: 0.08353978, Accuracy: 89.20%\n",
      "Iteration:   427, Loss: 0.08336843, Accuracy: 89.20%\n",
      "Iteration:   428, Loss: 0.08319802, Accuracy: 89.20%\n",
      "Iteration:   429, Loss: 0.08302846, Accuracy: 89.20%\n",
      "Iteration:   430, Loss: 0.08285966, Accuracy: 89.20%\n",
      "Iteration:   431, Loss: 0.08269158, Accuracy: 89.20%\n",
      "Iteration:   432, Loss: 0.08252415, Accuracy: 89.30%\n",
      "Iteration:   433, Loss: 0.08235730, Accuracy: 89.30%\n",
      "Iteration:   434, Loss: 0.08219099, Accuracy: 89.30%\n",
      "Iteration:   435, Loss: 0.08202516, Accuracy: 89.30%\n",
      "Iteration:   436, Loss: 0.08185977, Accuracy: 89.30%\n",
      "Iteration:   437, Loss: 0.08169483, Accuracy: 89.30%\n",
      "Iteration:   438, Loss: 0.08153041, Accuracy: 89.30%\n",
      "Iteration:   439, Loss: 0.08136667, Accuracy: 89.30%\n",
      "Iteration:   440, Loss: 0.08120368, Accuracy: 89.30%\n",
      "Iteration:   441, Loss: 0.08104143, Accuracy: 89.30%\n",
      "Iteration:   442, Loss: 0.08087989, Accuracy: 89.30%\n",
      "Iteration:   443, Loss: 0.08071903, Accuracy: 89.30%\n",
      "Iteration:   444, Loss: 0.08055884, Accuracy: 89.30%\n",
      "Iteration:   445, Loss: 0.08039931, Accuracy: 89.30%\n",
      "Iteration:   446, Loss: 0.08024046, Accuracy: 89.30%\n",
      "Iteration:   447, Loss: 0.08008231, Accuracy: 89.30%\n",
      "Iteration:   448, Loss: 0.07992488, Accuracy: 89.30%\n",
      "Iteration:   449, Loss: 0.07976817, Accuracy: 89.30%\n",
      "Iteration:   450, Loss: 0.07961217, Accuracy: 89.30%\n",
      "Iteration:   451, Loss: 0.07945685, Accuracy: 89.30%\n",
      "Iteration:   452, Loss: 0.07930217, Accuracy: 89.30%\n",
      "Iteration:   453, Loss: 0.07914811, Accuracy: 89.30%\n",
      "Iteration:   454, Loss: 0.07899464, Accuracy: 89.30%\n",
      "Iteration:   455, Loss: 0.07884174, Accuracy: 89.30%\n",
      "Iteration:   456, Loss: 0.07868938, Accuracy: 89.30%\n",
      "Iteration:   457, Loss: 0.07853756, Accuracy: 89.30%\n",
      "Iteration:   458, Loss: 0.07838627, Accuracy: 89.30%\n",
      "Iteration:   459, Loss: 0.07823548, Accuracy: 89.30%\n",
      "Iteration:   460, Loss: 0.07808520, Accuracy: 89.30%\n",
      "Iteration:   461, Loss: 0.07793542, Accuracy: 89.30%\n",
      "Iteration:   462, Loss: 0.07778615, Accuracy: 89.30%\n",
      "Iteration:   463, Loss: 0.07763737, Accuracy: 89.40%\n",
      "Iteration:   464, Loss: 0.07748910, Accuracy: 89.40%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   465, Loss: 0.07734131, Accuracy: 89.40%\n",
      "Iteration:   466, Loss: 0.07719400, Accuracy: 89.40%\n",
      "Iteration:   467, Loss: 0.07704717, Accuracy: 89.40%\n",
      "Iteration:   468, Loss: 0.07690080, Accuracy: 89.40%\n",
      "Iteration:   469, Loss: 0.07675489, Accuracy: 89.40%\n",
      "Iteration:   470, Loss: 0.07660942, Accuracy: 89.50%\n",
      "Iteration:   471, Loss: 0.07646440, Accuracy: 89.50%\n",
      "Iteration:   472, Loss: 0.07631980, Accuracy: 89.50%\n",
      "Iteration:   473, Loss: 0.07617561, Accuracy: 89.60%\n",
      "Iteration:   474, Loss: 0.07603182, Accuracy: 89.60%\n",
      "Iteration:   475, Loss: 0.07588840, Accuracy: 89.60%\n",
      "Iteration:   476, Loss: 0.07574535, Accuracy: 89.60%\n",
      "Iteration:   477, Loss: 0.07560266, Accuracy: 89.60%\n",
      "Iteration:   478, Loss: 0.07546031, Accuracy: 89.60%\n",
      "Iteration:   479, Loss: 0.07531831, Accuracy: 89.60%\n",
      "Iteration:   480, Loss: 0.07517665, Accuracy: 89.60%\n",
      "Iteration:   481, Loss: 0.07503531, Accuracy: 89.60%\n",
      "Iteration:   482, Loss: 0.07489429, Accuracy: 89.60%\n",
      "Iteration:   483, Loss: 0.07475361, Accuracy: 89.60%\n",
      "Iteration:   484, Loss: 0.07461329, Accuracy: 89.60%\n",
      "Iteration:   485, Loss: 0.07447339, Accuracy: 89.60%\n",
      "Iteration:   486, Loss: 0.07433393, Accuracy: 89.60%\n",
      "Iteration:   487, Loss: 0.07419492, Accuracy: 89.60%\n",
      "Iteration:   488, Loss: 0.07405638, Accuracy: 89.70%\n",
      "Iteration:   489, Loss: 0.07391829, Accuracy: 89.70%\n",
      "Iteration:   490, Loss: 0.07378065, Accuracy: 89.70%\n",
      "Iteration:   491, Loss: 0.07364349, Accuracy: 89.70%\n",
      "Iteration:   492, Loss: 0.07350682, Accuracy: 89.70%\n",
      "Iteration:   493, Loss: 0.07337066, Accuracy: 89.70%\n",
      "Iteration:   494, Loss: 0.07323503, Accuracy: 89.70%\n",
      "Iteration:   495, Loss: 0.07309993, Accuracy: 89.70%\n",
      "Iteration:   496, Loss: 0.07296536, Accuracy: 89.70%\n",
      "Iteration:   497, Loss: 0.07283130, Accuracy: 89.70%\n",
      "Iteration:   498, Loss: 0.07269773, Accuracy: 89.70%\n",
      "Iteration:   499, Loss: 0.07256465, Accuracy: 89.70%\n",
      "Iteration:   500, Loss: 0.07243202, Accuracy: 89.70%\n",
      "Iteration:   501, Loss: 0.07229984, Accuracy: 89.70%\n",
      "Iteration:   502, Loss: 0.07216807, Accuracy: 89.70%\n",
      "Iteration:   503, Loss: 0.07203669, Accuracy: 89.70%\n",
      "Iteration:   504, Loss: 0.07190567, Accuracy: 89.70%\n",
      "Iteration:   505, Loss: 0.07177499, Accuracy: 89.70%\n",
      "Iteration:   506, Loss: 0.07164460, Accuracy: 89.70%\n",
      "Iteration:   507, Loss: 0.07151450, Accuracy: 89.70%\n",
      "Iteration:   508, Loss: 0.07138466, Accuracy: 89.70%\n",
      "Iteration:   509, Loss: 0.07125511, Accuracy: 89.70%\n",
      "Iteration:   510, Loss: 0.07112595, Accuracy: 89.70%\n",
      "Iteration:   511, Loss: 0.07099736, Accuracy: 89.70%\n",
      "Iteration:   512, Loss: 0.07086942, Accuracy: 89.70%\n",
      "Iteration:   513, Loss: 0.07074212, Accuracy: 89.70%\n",
      "Iteration:   514, Loss: 0.07061540, Accuracy: 89.70%\n",
      "Iteration:   515, Loss: 0.07048923, Accuracy: 89.70%\n",
      "Iteration:   516, Loss: 0.07036356, Accuracy: 89.70%\n",
      "Iteration:   517, Loss: 0.07023837, Accuracy: 89.70%\n",
      "Iteration:   518, Loss: 0.07011364, Accuracy: 89.70%\n",
      "Iteration:   519, Loss: 0.06998937, Accuracy: 89.70%\n",
      "Iteration:   520, Loss: 0.06986554, Accuracy: 89.70%\n",
      "Iteration:   521, Loss: 0.06974215, Accuracy: 89.70%\n",
      "Iteration:   522, Loss: 0.06961919, Accuracy: 89.80%\n",
      "Iteration:   523, Loss: 0.06949666, Accuracy: 89.80%\n",
      "Iteration:   524, Loss: 0.06937455, Accuracy: 89.80%\n",
      "Iteration:   525, Loss: 0.06925286, Accuracy: 89.80%\n",
      "Iteration:   526, Loss: 0.06913158, Accuracy: 89.80%\n",
      "Iteration:   527, Loss: 0.06901070, Accuracy: 89.80%\n",
      "Iteration:   528, Loss: 0.06889023, Accuracy: 89.80%\n",
      "Iteration:   529, Loss: 0.06877016, Accuracy: 89.80%\n",
      "Iteration:   530, Loss: 0.06865047, Accuracy: 89.80%\n",
      "Iteration:   531, Loss: 0.06853116, Accuracy: 89.80%\n",
      "Iteration:   532, Loss: 0.06841220, Accuracy: 89.80%\n",
      "Iteration:   533, Loss: 0.06829358, Accuracy: 89.80%\n",
      "Iteration:   534, Loss: 0.06817529, Accuracy: 89.80%\n",
      "Iteration:   535, Loss: 0.06805729, Accuracy: 89.80%\n",
      "Iteration:   536, Loss: 0.06793957, Accuracy: 89.80%\n",
      "Iteration:   537, Loss: 0.06782210, Accuracy: 89.80%\n",
      "Iteration:   538, Loss: 0.06770486, Accuracy: 89.80%\n",
      "Iteration:   539, Loss: 0.06758782, Accuracy: 89.90%\n",
      "Iteration:   540, Loss: 0.06747096, Accuracy: 89.90%\n",
      "Iteration:   541, Loss: 0.06735428, Accuracy: 89.90%\n",
      "Iteration:   542, Loss: 0.06723776, Accuracy: 90.00%\n",
      "Iteration:   543, Loss: 0.06712141, Accuracy: 90.00%\n",
      "Iteration:   544, Loss: 0.06700529, Accuracy: 90.00%\n",
      "Iteration:   545, Loss: 0.06688942, Accuracy: 90.00%\n",
      "Iteration:   546, Loss: 0.06677384, Accuracy: 90.00%\n",
      "Iteration:   547, Loss: 0.06665859, Accuracy: 90.00%\n",
      "Iteration:   548, Loss: 0.06654366, Accuracy: 90.00%\n",
      "Iteration:   549, Loss: 0.06642905, Accuracy: 90.00%\n",
      "Iteration:   550, Loss: 0.06631476, Accuracy: 90.00%\n",
      "Iteration:   551, Loss: 0.06620079, Accuracy: 90.00%\n",
      "Iteration:   552, Loss: 0.06608712, Accuracy: 90.00%\n",
      "Iteration:   553, Loss: 0.06597379, Accuracy: 90.00%\n",
      "Iteration:   554, Loss: 0.06586077, Accuracy: 90.00%\n",
      "Iteration:   555, Loss: 0.06574808, Accuracy: 90.00%\n",
      "Iteration:   556, Loss: 0.06563572, Accuracy: 90.00%\n",
      "Iteration:   557, Loss: 0.06552367, Accuracy: 90.00%\n",
      "Iteration:   558, Loss: 0.06541194, Accuracy: 90.00%\n",
      "Iteration:   559, Loss: 0.06530052, Accuracy: 90.00%\n",
      "Iteration:   560, Loss: 0.06518942, Accuracy: 90.00%\n",
      "Iteration:   561, Loss: 0.06507862, Accuracy: 90.00%\n",
      "Iteration:   562, Loss: 0.06496813, Accuracy: 90.00%\n",
      "Iteration:   563, Loss: 0.06485792, Accuracy: 90.00%\n",
      "Iteration:   564, Loss: 0.06474799, Accuracy: 90.00%\n",
      "Iteration:   565, Loss: 0.06463832, Accuracy: 90.00%\n",
      "Iteration:   566, Loss: 0.06452890, Accuracy: 90.00%\n",
      "Iteration:   567, Loss: 0.06441974, Accuracy: 90.00%\n",
      "Iteration:   568, Loss: 0.06431084, Accuracy: 90.00%\n",
      "Iteration:   569, Loss: 0.06420221, Accuracy: 90.00%\n",
      "Iteration:   570, Loss: 0.06409386, Accuracy: 90.00%\n",
      "Iteration:   571, Loss: 0.06398580, Accuracy: 89.90%\n",
      "Iteration:   572, Loss: 0.06387801, Accuracy: 89.90%\n",
      "Iteration:   573, Loss: 0.06377049, Accuracy: 89.90%\n",
      "Iteration:   574, Loss: 0.06366324, Accuracy: 90.00%\n",
      "Iteration:   575, Loss: 0.06355624, Accuracy: 90.00%\n",
      "Iteration:   576, Loss: 0.06344948, Accuracy: 90.00%\n",
      "Iteration:   577, Loss: 0.06334295, Accuracy: 90.00%\n",
      "Iteration:   578, Loss: 0.06323666, Accuracy: 90.00%\n",
      "Iteration:   579, Loss: 0.06313061, Accuracy: 90.00%\n",
      "Iteration:   580, Loss: 0.06302484, Accuracy: 90.00%\n",
      "Iteration:   581, Loss: 0.06291937, Accuracy: 90.00%\n",
      "Iteration:   582, Loss: 0.06281425, Accuracy: 90.00%\n",
      "Iteration:   583, Loss: 0.06270954, Accuracy: 90.00%\n",
      "Iteration:   584, Loss: 0.06260526, Accuracy: 90.00%\n",
      "Iteration:   585, Loss: 0.06250144, Accuracy: 90.00%\n",
      "Iteration:   586, Loss: 0.06239805, Accuracy: 90.00%\n",
      "Iteration:   587, Loss: 0.06229507, Accuracy: 90.00%\n",
      "Iteration:   588, Loss: 0.06219247, Accuracy: 90.00%\n",
      "Iteration:   589, Loss: 0.06209025, Accuracy: 90.00%\n",
      "Iteration:   590, Loss: 0.06198839, Accuracy: 90.00%\n",
      "Iteration:   591, Loss: 0.06188688, Accuracy: 90.00%\n",
      "Iteration:   592, Loss: 0.06178570, Accuracy: 90.00%\n",
      "Iteration:   593, Loss: 0.06168485, Accuracy: 90.00%\n",
      "Iteration:   594, Loss: 0.06158433, Accuracy: 90.00%\n",
      "Iteration:   595, Loss: 0.06148411, Accuracy: 90.00%\n",
      "Iteration:   596, Loss: 0.06138420, Accuracy: 90.00%\n",
      "Iteration:   597, Loss: 0.06128460, Accuracy: 90.00%\n",
      "Iteration:   598, Loss: 0.06118529, Accuracy: 90.00%\n",
      "Iteration:   599, Loss: 0.06108628, Accuracy: 90.00%\n",
      "Iteration:   600, Loss: 0.06098755, Accuracy: 90.00%\n",
      "Iteration:   601, Loss: 0.06088911, Accuracy: 90.00%\n",
      "Iteration:   602, Loss: 0.06079095, Accuracy: 90.00%\n",
      "Iteration:   603, Loss: 0.06069307, Accuracy: 90.00%\n",
      "Iteration:   604, Loss: 0.06059547, Accuracy: 90.00%\n",
      "Iteration:   605, Loss: 0.06049813, Accuracy: 90.00%\n",
      "Iteration:   606, Loss: 0.06040107, Accuracy: 90.00%\n",
      "Iteration:   607, Loss: 0.06030427, Accuracy: 89.90%\n",
      "Iteration:   608, Loss: 0.06020774, Accuracy: 89.90%\n",
      "Iteration:   609, Loss: 0.06011147, Accuracy: 89.90%\n",
      "Iteration:   610, Loss: 0.06001546, Accuracy: 89.90%\n",
      "Iteration:   611, Loss: 0.05991972, Accuracy: 89.90%\n",
      "Iteration:   612, Loss: 0.05982422, Accuracy: 89.90%\n",
      "Iteration:   613, Loss: 0.05972899, Accuracy: 89.90%\n",
      "Iteration:   614, Loss: 0.05963400, Accuracy: 89.90%\n",
      "Iteration:   615, Loss: 0.05953927, Accuracy: 89.90%\n",
      "Iteration:   616, Loss: 0.05944479, Accuracy: 89.90%\n",
      "Iteration:   617, Loss: 0.05935057, Accuracy: 89.90%\n",
      "Iteration:   618, Loss: 0.05925661, Accuracy: 90.00%\n",
      "Iteration:   619, Loss: 0.05916292, Accuracy: 90.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   620, Loss: 0.05906952, Accuracy: 90.00%\n",
      "Iteration:   621, Loss: 0.05897641, Accuracy: 90.00%\n",
      "Iteration:   622, Loss: 0.05888360, Accuracy: 90.00%\n",
      "Iteration:   623, Loss: 0.05879109, Accuracy: 90.00%\n",
      "Iteration:   624, Loss: 0.05869886, Accuracy: 90.00%\n",
      "Iteration:   625, Loss: 0.05860692, Accuracy: 90.00%\n",
      "Iteration:   626, Loss: 0.05851525, Accuracy: 90.00%\n",
      "Iteration:   627, Loss: 0.05842385, Accuracy: 90.00%\n",
      "Iteration:   628, Loss: 0.05833270, Accuracy: 90.00%\n",
      "Iteration:   629, Loss: 0.05824179, Accuracy: 90.00%\n",
      "Iteration:   630, Loss: 0.05815113, Accuracy: 90.00%\n",
      "Iteration:   631, Loss: 0.05806069, Accuracy: 90.00%\n",
      "Iteration:   632, Loss: 0.05797049, Accuracy: 90.00%\n",
      "Iteration:   633, Loss: 0.05788050, Accuracy: 90.00%\n",
      "Iteration:   634, Loss: 0.05779073, Accuracy: 90.00%\n",
      "Iteration:   635, Loss: 0.05770116, Accuracy: 90.00%\n",
      "Iteration:   636, Loss: 0.05761180, Accuracy: 90.00%\n",
      "Iteration:   637, Loss: 0.05752264, Accuracy: 90.00%\n",
      "Iteration:   638, Loss: 0.05743367, Accuracy: 90.00%\n",
      "Iteration:   639, Loss: 0.05734490, Accuracy: 90.00%\n",
      "Iteration:   640, Loss: 0.05725632, Accuracy: 90.00%\n",
      "Iteration:   641, Loss: 0.05716791, Accuracy: 90.00%\n",
      "Iteration:   642, Loss: 0.05707969, Accuracy: 90.00%\n",
      "Iteration:   643, Loss: 0.05699165, Accuracy: 90.00%\n",
      "Iteration:   644, Loss: 0.05690377, Accuracy: 90.00%\n",
      "Iteration:   645, Loss: 0.05681607, Accuracy: 90.00%\n",
      "Iteration:   646, Loss: 0.05672853, Accuracy: 90.00%\n",
      "Iteration:   647, Loss: 0.05664115, Accuracy: 90.00%\n",
      "Iteration:   648, Loss: 0.05655395, Accuracy: 90.00%\n",
      "Iteration:   649, Loss: 0.05646692, Accuracy: 90.00%\n",
      "Iteration:   650, Loss: 0.05638006, Accuracy: 90.00%\n",
      "Iteration:   651, Loss: 0.05629338, Accuracy: 90.00%\n",
      "Iteration:   652, Loss: 0.05620689, Accuracy: 90.00%\n",
      "Iteration:   653, Loss: 0.05612060, Accuracy: 90.00%\n",
      "Iteration:   654, Loss: 0.05603449, Accuracy: 90.00%\n",
      "Iteration:   655, Loss: 0.05594859, Accuracy: 90.00%\n",
      "Iteration:   656, Loss: 0.05586289, Accuracy: 90.00%\n",
      "Iteration:   657, Loss: 0.05577739, Accuracy: 90.00%\n",
      "Iteration:   658, Loss: 0.05569209, Accuracy: 90.00%\n",
      "Iteration:   659, Loss: 0.05560697, Accuracy: 90.00%\n",
      "Iteration:   660, Loss: 0.05552203, Accuracy: 90.00%\n",
      "Iteration:   661, Loss: 0.05543725, Accuracy: 90.00%\n",
      "Iteration:   662, Loss: 0.05535269, Accuracy: 90.00%\n",
      "Iteration:   663, Loss: 0.05526843, Accuracy: 90.00%\n",
      "Iteration:   664, Loss: 0.05518450, Accuracy: 90.00%\n",
      "Iteration:   665, Loss: 0.05510091, Accuracy: 90.00%\n",
      "Iteration:   666, Loss: 0.05501761, Accuracy: 90.00%\n",
      "Iteration:   667, Loss: 0.05493459, Accuracy: 90.00%\n",
      "Iteration:   668, Loss: 0.05485181, Accuracy: 90.00%\n",
      "Iteration:   669, Loss: 0.05476928, Accuracy: 90.00%\n",
      "Iteration:   670, Loss: 0.05468698, Accuracy: 90.00%\n",
      "Iteration:   671, Loss: 0.05460493, Accuracy: 90.00%\n",
      "Iteration:   672, Loss: 0.05452310, Accuracy: 90.00%\n",
      "Iteration:   673, Loss: 0.05444151, Accuracy: 90.00%\n",
      "Iteration:   674, Loss: 0.05436013, Accuracy: 90.00%\n",
      "Iteration:   675, Loss: 0.05427896, Accuracy: 90.00%\n",
      "Iteration:   676, Loss: 0.05419799, Accuracy: 90.00%\n",
      "Iteration:   677, Loss: 0.05411722, Accuracy: 90.00%\n",
      "Iteration:   678, Loss: 0.05403665, Accuracy: 90.00%\n",
      "Iteration:   679, Loss: 0.05395627, Accuracy: 90.00%\n",
      "Iteration:   680, Loss: 0.05387610, Accuracy: 90.00%\n",
      "Iteration:   681, Loss: 0.05379614, Accuracy: 90.00%\n",
      "Iteration:   682, Loss: 0.05371640, Accuracy: 90.00%\n",
      "Iteration:   683, Loss: 0.05363689, Accuracy: 90.00%\n",
      "Iteration:   684, Loss: 0.05355761, Accuracy: 90.00%\n",
      "Iteration:   685, Loss: 0.05347856, Accuracy: 90.00%\n",
      "Iteration:   686, Loss: 0.05339974, Accuracy: 90.00%\n",
      "Iteration:   687, Loss: 0.05332115, Accuracy: 90.00%\n",
      "Iteration:   688, Loss: 0.05324278, Accuracy: 90.00%\n",
      "Iteration:   689, Loss: 0.05316464, Accuracy: 90.00%\n",
      "Iteration:   690, Loss: 0.05308672, Accuracy: 90.00%\n",
      "Iteration:   691, Loss: 0.05300900, Accuracy: 90.00%\n",
      "Iteration:   692, Loss: 0.05293148, Accuracy: 90.00%\n",
      "Iteration:   693, Loss: 0.05285414, Accuracy: 90.00%\n",
      "Iteration:   694, Loss: 0.05277698, Accuracy: 90.00%\n",
      "Iteration:   695, Loss: 0.05269997, Accuracy: 90.00%\n",
      "Iteration:   696, Loss: 0.05262310, Accuracy: 90.00%\n",
      "Iteration:   697, Loss: 0.05254636, Accuracy: 89.90%\n",
      "Iteration:   698, Loss: 0.05246974, Accuracy: 89.90%\n",
      "Iteration:   699, Loss: 0.05239325, Accuracy: 89.90%\n",
      "Iteration:   700, Loss: 0.05231689, Accuracy: 89.90%\n",
      "Iteration:   701, Loss: 0.05224068, Accuracy: 89.90%\n",
      "Iteration:   702, Loss: 0.05216463, Accuracy: 89.90%\n",
      "Iteration:   703, Loss: 0.05208873, Accuracy: 89.90%\n",
      "Iteration:   704, Loss: 0.05201300, Accuracy: 89.90%\n",
      "Iteration:   705, Loss: 0.05193743, Accuracy: 89.90%\n",
      "Iteration:   706, Loss: 0.05186203, Accuracy: 89.90%\n",
      "Iteration:   707, Loss: 0.05178683, Accuracy: 89.90%\n",
      "Iteration:   708, Loss: 0.05171185, Accuracy: 89.90%\n",
      "Iteration:   709, Loss: 0.05163714, Accuracy: 89.90%\n",
      "Iteration:   710, Loss: 0.05156270, Accuracy: 89.90%\n",
      "Iteration:   711, Loss: 0.05148853, Accuracy: 89.90%\n",
      "Iteration:   712, Loss: 0.05141462, Accuracy: 89.90%\n",
      "Iteration:   713, Loss: 0.05134096, Accuracy: 89.90%\n",
      "Iteration:   714, Loss: 0.05126752, Accuracy: 89.90%\n",
      "Iteration:   715, Loss: 0.05119429, Accuracy: 89.90%\n",
      "Iteration:   716, Loss: 0.05112127, Accuracy: 89.90%\n",
      "Iteration:   717, Loss: 0.05104844, Accuracy: 89.90%\n",
      "Iteration:   718, Loss: 0.05097579, Accuracy: 89.90%\n",
      "Iteration:   719, Loss: 0.05090331, Accuracy: 89.90%\n",
      "Iteration:   720, Loss: 0.05083100, Accuracy: 89.90%\n",
      "Iteration:   721, Loss: 0.05075884, Accuracy: 89.90%\n",
      "Iteration:   722, Loss: 0.05068683, Accuracy: 89.90%\n",
      "Iteration:   723, Loss: 0.05061497, Accuracy: 89.90%\n",
      "Iteration:   724, Loss: 0.05054326, Accuracy: 89.90%\n",
      "Iteration:   725, Loss: 0.05047169, Accuracy: 89.90%\n",
      "Iteration:   726, Loss: 0.05040028, Accuracy: 89.90%\n",
      "Iteration:   727, Loss: 0.05032903, Accuracy: 89.90%\n",
      "Iteration:   728, Loss: 0.05025795, Accuracy: 89.90%\n",
      "Iteration:   729, Loss: 0.05018703, Accuracy: 89.90%\n",
      "Iteration:   730, Loss: 0.05011628, Accuracy: 89.90%\n",
      "Iteration:   731, Loss: 0.05004568, Accuracy: 89.90%\n",
      "Iteration:   732, Loss: 0.04997520, Accuracy: 89.90%\n",
      "Iteration:   733, Loss: 0.04990484, Accuracy: 89.90%\n",
      "Iteration:   734, Loss: 0.04983457, Accuracy: 89.90%\n",
      "Iteration:   735, Loss: 0.04976437, Accuracy: 89.90%\n",
      "Iteration:   736, Loss: 0.04969426, Accuracy: 89.90%\n",
      "Iteration:   737, Loss: 0.04962425, Accuracy: 89.90%\n",
      "Iteration:   738, Loss: 0.04955438, Accuracy: 89.90%\n",
      "Iteration:   739, Loss: 0.04948467, Accuracy: 89.90%\n",
      "Iteration:   740, Loss: 0.04941515, Accuracy: 89.90%\n",
      "Iteration:   741, Loss: 0.04934580, Accuracy: 89.90%\n",
      "Iteration:   742, Loss: 0.04927661, Accuracy: 89.90%\n",
      "Iteration:   743, Loss: 0.04920758, Accuracy: 89.90%\n",
      "Iteration:   744, Loss: 0.04913870, Accuracy: 89.90%\n",
      "Iteration:   745, Loss: 0.04906999, Accuracy: 89.90%\n",
      "Iteration:   746, Loss: 0.04900146, Accuracy: 89.90%\n",
      "Iteration:   747, Loss: 0.04893311, Accuracy: 89.90%\n",
      "Iteration:   748, Loss: 0.04886494, Accuracy: 89.90%\n",
      "Iteration:   749, Loss: 0.04879695, Accuracy: 89.90%\n",
      "Iteration:   750, Loss: 0.04872915, Accuracy: 89.90%\n",
      "Iteration:   751, Loss: 0.04866153, Accuracy: 89.90%\n",
      "Iteration:   752, Loss: 0.04859409, Accuracy: 89.90%\n",
      "Iteration:   753, Loss: 0.04852683, Accuracy: 89.90%\n",
      "Iteration:   754, Loss: 0.04845974, Accuracy: 89.90%\n",
      "Iteration:   755, Loss: 0.04839282, Accuracy: 89.90%\n",
      "Iteration:   756, Loss: 0.04832607, Accuracy: 89.90%\n",
      "Iteration:   757, Loss: 0.04825949, Accuracy: 89.90%\n",
      "Iteration:   758, Loss: 0.04819308, Accuracy: 89.90%\n",
      "Iteration:   759, Loss: 0.04812683, Accuracy: 89.90%\n",
      "Iteration:   760, Loss: 0.04806075, Accuracy: 89.90%\n",
      "Iteration:   761, Loss: 0.04799484, Accuracy: 89.80%\n",
      "Iteration:   762, Loss: 0.04792910, Accuracy: 89.80%\n",
      "Iteration:   763, Loss: 0.04786353, Accuracy: 89.80%\n",
      "Iteration:   764, Loss: 0.04779814, Accuracy: 89.80%\n",
      "Iteration:   765, Loss: 0.04773293, Accuracy: 89.80%\n",
      "Iteration:   766, Loss: 0.04766790, Accuracy: 89.80%\n",
      "Iteration:   767, Loss: 0.04760305, Accuracy: 89.80%\n",
      "Iteration:   768, Loss: 0.04753837, Accuracy: 89.80%\n",
      "Iteration:   769, Loss: 0.04747386, Accuracy: 89.80%\n",
      "Iteration:   770, Loss: 0.04740952, Accuracy: 89.80%\n",
      "Iteration:   771, Loss: 0.04734534, Accuracy: 89.80%\n",
      "Iteration:   772, Loss: 0.04728133, Accuracy: 89.80%\n",
      "Iteration:   773, Loss: 0.04721747, Accuracy: 89.80%\n",
      "Iteration:   774, Loss: 0.04715376, Accuracy: 89.80%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   775, Loss: 0.04709021, Accuracy: 89.80%\n",
      "Iteration:   776, Loss: 0.04702681, Accuracy: 89.80%\n",
      "Iteration:   777, Loss: 0.04696355, Accuracy: 89.80%\n",
      "Iteration:   778, Loss: 0.04690045, Accuracy: 89.80%\n",
      "Iteration:   779, Loss: 0.04683748, Accuracy: 89.80%\n",
      "Iteration:   780, Loss: 0.04677466, Accuracy: 89.80%\n",
      "Iteration:   781, Loss: 0.04671198, Accuracy: 89.80%\n",
      "Iteration:   782, Loss: 0.04664943, Accuracy: 89.80%\n",
      "Iteration:   783, Loss: 0.04658702, Accuracy: 89.80%\n",
      "Iteration:   784, Loss: 0.04652475, Accuracy: 89.80%\n",
      "Iteration:   785, Loss: 0.04646260, Accuracy: 89.80%\n",
      "Iteration:   786, Loss: 0.04640058, Accuracy: 89.80%\n",
      "Iteration:   787, Loss: 0.04633869, Accuracy: 89.80%\n",
      "Iteration:   788, Loss: 0.04627692, Accuracy: 89.80%\n",
      "Iteration:   789, Loss: 0.04621528, Accuracy: 89.80%\n",
      "Iteration:   790, Loss: 0.04615375, Accuracy: 89.80%\n",
      "Iteration:   791, Loss: 0.04609234, Accuracy: 89.80%\n",
      "Iteration:   792, Loss: 0.04603104, Accuracy: 89.80%\n",
      "Iteration:   793, Loss: 0.04596985, Accuracy: 89.80%\n",
      "Iteration:   794, Loss: 0.04590877, Accuracy: 89.90%\n",
      "Iteration:   795, Loss: 0.04584778, Accuracy: 89.90%\n",
      "Iteration:   796, Loss: 0.04578690, Accuracy: 89.90%\n",
      "Iteration:   797, Loss: 0.04572612, Accuracy: 89.90%\n",
      "Iteration:   798, Loss: 0.04566545, Accuracy: 89.90%\n",
      "Iteration:   799, Loss: 0.04560491, Accuracy: 89.90%\n",
      "Iteration:   800, Loss: 0.04554449, Accuracy: 89.90%\n",
      "Iteration:   801, Loss: 0.04548420, Accuracy: 89.90%\n",
      "Iteration:   802, Loss: 0.04542404, Accuracy: 89.90%\n",
      "Iteration:   803, Loss: 0.04536400, Accuracy: 89.90%\n",
      "Iteration:   804, Loss: 0.04530407, Accuracy: 89.90%\n",
      "Iteration:   805, Loss: 0.04524424, Accuracy: 89.90%\n",
      "Iteration:   806, Loss: 0.04518451, Accuracy: 89.90%\n",
      "Iteration:   807, Loss: 0.04512486, Accuracy: 89.90%\n",
      "Iteration:   808, Loss: 0.04506530, Accuracy: 89.90%\n",
      "Iteration:   809, Loss: 0.04500582, Accuracy: 89.90%\n",
      "Iteration:   810, Loss: 0.04494645, Accuracy: 89.90%\n",
      "Iteration:   811, Loss: 0.04488721, Accuracy: 89.90%\n",
      "Iteration:   812, Loss: 0.04482812, Accuracy: 89.90%\n",
      "Iteration:   813, Loss: 0.04476919, Accuracy: 89.90%\n",
      "Iteration:   814, Loss: 0.04471040, Accuracy: 89.90%\n",
      "Iteration:   815, Loss: 0.04465177, Accuracy: 89.90%\n",
      "Iteration:   816, Loss: 0.04459327, Accuracy: 89.90%\n",
      "Iteration:   817, Loss: 0.04453490, Accuracy: 89.90%\n",
      "Iteration:   818, Loss: 0.04447664, Accuracy: 89.90%\n",
      "Iteration:   819, Loss: 0.04441849, Accuracy: 89.90%\n",
      "Iteration:   820, Loss: 0.04436044, Accuracy: 89.90%\n",
      "Iteration:   821, Loss: 0.04430247, Accuracy: 89.90%\n",
      "Iteration:   822, Loss: 0.04424459, Accuracy: 89.90%\n",
      "Iteration:   823, Loss: 0.04418679, Accuracy: 89.90%\n",
      "Iteration:   824, Loss: 0.04412911, Accuracy: 89.90%\n",
      "Iteration:   825, Loss: 0.04407158, Accuracy: 89.90%\n",
      "Iteration:   826, Loss: 0.04401423, Accuracy: 89.90%\n",
      "Iteration:   827, Loss: 0.04395707, Accuracy: 89.90%\n",
      "Iteration:   828, Loss: 0.04390010, Accuracy: 89.90%\n",
      "Iteration:   829, Loss: 0.04384331, Accuracy: 89.90%\n",
      "Iteration:   830, Loss: 0.04378668, Accuracy: 89.90%\n",
      "Iteration:   831, Loss: 0.04373020, Accuracy: 89.90%\n",
      "Iteration:   832, Loss: 0.04367384, Accuracy: 89.90%\n",
      "Iteration:   833, Loss: 0.04361761, Accuracy: 89.90%\n",
      "Iteration:   834, Loss: 0.04356148, Accuracy: 89.90%\n",
      "Iteration:   835, Loss: 0.04350547, Accuracy: 89.90%\n",
      "Iteration:   836, Loss: 0.04344957, Accuracy: 89.90%\n",
      "Iteration:   837, Loss: 0.04339378, Accuracy: 89.90%\n",
      "Iteration:   838, Loss: 0.04333810, Accuracy: 89.90%\n",
      "Iteration:   839, Loss: 0.04328253, Accuracy: 89.90%\n",
      "Iteration:   840, Loss: 0.04322707, Accuracy: 89.90%\n",
      "Iteration:   841, Loss: 0.04317170, Accuracy: 89.90%\n",
      "Iteration:   842, Loss: 0.04311643, Accuracy: 89.90%\n",
      "Iteration:   843, Loss: 0.04306125, Accuracy: 89.90%\n",
      "Iteration:   844, Loss: 0.04300616, Accuracy: 89.90%\n",
      "Iteration:   845, Loss: 0.04295116, Accuracy: 89.90%\n",
      "Iteration:   846, Loss: 0.04289624, Accuracy: 89.90%\n",
      "Iteration:   847, Loss: 0.04284141, Accuracy: 89.90%\n",
      "Iteration:   848, Loss: 0.04278667, Accuracy: 89.90%\n",
      "Iteration:   849, Loss: 0.04273203, Accuracy: 89.90%\n",
      "Iteration:   850, Loss: 0.04267749, Accuracy: 89.90%\n",
      "Iteration:   851, Loss: 0.04262306, Accuracy: 89.90%\n",
      "Iteration:   852, Loss: 0.04256875, Accuracy: 89.90%\n",
      "Iteration:   853, Loss: 0.04251455, Accuracy: 89.90%\n",
      "Iteration:   854, Loss: 0.04246045, Accuracy: 89.90%\n",
      "Iteration:   855, Loss: 0.04240641, Accuracy: 89.90%\n",
      "Iteration:   856, Loss: 0.04235239, Accuracy: 89.90%\n",
      "Iteration:   857, Loss: 0.04229829, Accuracy: 89.90%\n",
      "Iteration:   858, Loss: 0.04224394, Accuracy: 89.90%\n",
      "Iteration:   859, Loss: 0.04218908, Accuracy: 89.90%\n",
      "Iteration:   860, Loss: 0.04213347, Accuracy: 89.90%\n",
      "Iteration:   861, Loss: 0.04207766, Accuracy: 89.90%\n",
      "Iteration:   862, Loss: 0.04202301, Accuracy: 89.90%\n",
      "Iteration:   863, Loss: 0.04196948, Accuracy: 89.90%\n",
      "Iteration:   864, Loss: 0.04191644, Accuracy: 89.90%\n",
      "Iteration:   865, Loss: 0.04186364, Accuracy: 89.90%\n",
      "Iteration:   866, Loss: 0.04181099, Accuracy: 89.90%\n",
      "Iteration:   867, Loss: 0.04175847, Accuracy: 89.90%\n",
      "Iteration:   868, Loss: 0.04170606, Accuracy: 89.90%\n",
      "Iteration:   869, Loss: 0.04165376, Accuracy: 89.90%\n",
      "Iteration:   870, Loss: 0.04160156, Accuracy: 89.90%\n",
      "Iteration:   871, Loss: 0.04154946, Accuracy: 89.90%\n",
      "Iteration:   872, Loss: 0.04149748, Accuracy: 89.90%\n",
      "Iteration:   873, Loss: 0.04144561, Accuracy: 89.90%\n",
      "Iteration:   874, Loss: 0.04139387, Accuracy: 89.90%\n",
      "Iteration:   875, Loss: 0.04134226, Accuracy: 89.90%\n",
      "Iteration:   876, Loss: 0.04129078, Accuracy: 89.90%\n",
      "Iteration:   877, Loss: 0.04123943, Accuracy: 89.90%\n",
      "Iteration:   878, Loss: 0.04118821, Accuracy: 89.90%\n",
      "Iteration:   879, Loss: 0.04113711, Accuracy: 89.90%\n",
      "Iteration:   880, Loss: 0.04108612, Accuracy: 89.90%\n",
      "Iteration:   881, Loss: 0.04103522, Accuracy: 89.90%\n",
      "Iteration:   882, Loss: 0.04098441, Accuracy: 89.90%\n",
      "Iteration:   883, Loss: 0.04093369, Accuracy: 89.90%\n",
      "Iteration:   884, Loss: 0.04088307, Accuracy: 89.90%\n",
      "Iteration:   885, Loss: 0.04083255, Accuracy: 89.90%\n",
      "Iteration:   886, Loss: 0.04078216, Accuracy: 89.90%\n",
      "Iteration:   887, Loss: 0.04073190, Accuracy: 89.90%\n",
      "Iteration:   888, Loss: 0.04068178, Accuracy: 89.90%\n",
      "Iteration:   889, Loss: 0.04063177, Accuracy: 89.90%\n",
      "Iteration:   890, Loss: 0.04058188, Accuracy: 89.90%\n",
      "Iteration:   891, Loss: 0.04053208, Accuracy: 89.90%\n",
      "Iteration:   892, Loss: 0.04048236, Accuracy: 89.90%\n",
      "Iteration:   893, Loss: 0.04043270, Accuracy: 89.90%\n",
      "Iteration:   894, Loss: 0.04038310, Accuracy: 89.90%\n",
      "Iteration:   895, Loss: 0.04033352, Accuracy: 89.90%\n",
      "Iteration:   896, Loss: 0.04028394, Accuracy: 89.90%\n",
      "Iteration:   897, Loss: 0.04023431, Accuracy: 89.90%\n",
      "Iteration:   898, Loss: 0.04018460, Accuracy: 89.90%\n",
      "Iteration:   899, Loss: 0.04013480, Accuracy: 89.90%\n",
      "Iteration:   900, Loss: 0.04008495, Accuracy: 90.00%\n",
      "Iteration:   901, Loss: 0.04003520, Accuracy: 90.00%\n",
      "Iteration:   902, Loss: 0.03998569, Accuracy: 90.00%\n",
      "Iteration:   903, Loss: 0.03993645, Accuracy: 90.00%\n",
      "Iteration:   904, Loss: 0.03988745, Accuracy: 90.00%\n",
      "Iteration:   905, Loss: 0.03983864, Accuracy: 90.00%\n",
      "Iteration:   906, Loss: 0.03978999, Accuracy: 90.00%\n",
      "Iteration:   907, Loss: 0.03974148, Accuracy: 90.00%\n",
      "Iteration:   908, Loss: 0.03969310, Accuracy: 90.10%\n",
      "Iteration:   909, Loss: 0.03964485, Accuracy: 90.10%\n",
      "Iteration:   910, Loss: 0.03959674, Accuracy: 90.10%\n",
      "Iteration:   911, Loss: 0.03954877, Accuracy: 90.10%\n",
      "Iteration:   912, Loss: 0.03950093, Accuracy: 90.10%\n",
      "Iteration:   913, Loss: 0.03945323, Accuracy: 90.10%\n",
      "Iteration:   914, Loss: 0.03940567, Accuracy: 90.10%\n",
      "Iteration:   915, Loss: 0.03935823, Accuracy: 90.10%\n",
      "Iteration:   916, Loss: 0.03931091, Accuracy: 90.10%\n",
      "Iteration:   917, Loss: 0.03926371, Accuracy: 90.10%\n",
      "Iteration:   918, Loss: 0.03921662, Accuracy: 90.10%\n",
      "Iteration:   919, Loss: 0.03916963, Accuracy: 90.10%\n",
      "Iteration:   920, Loss: 0.03912274, Accuracy: 90.10%\n",
      "Iteration:   921, Loss: 0.03907594, Accuracy: 90.10%\n",
      "Iteration:   922, Loss: 0.03902921, Accuracy: 90.10%\n",
      "Iteration:   923, Loss: 0.03898255, Accuracy: 90.10%\n",
      "Iteration:   924, Loss: 0.03893594, Accuracy: 90.10%\n",
      "Iteration:   925, Loss: 0.03888936, Accuracy: 90.10%\n",
      "Iteration:   926, Loss: 0.03884282, Accuracy: 90.10%\n",
      "Iteration:   927, Loss: 0.03879630, Accuracy: 90.10%\n",
      "Iteration:   928, Loss: 0.03874981, Accuracy: 90.00%\n",
      "Iteration:   929, Loss: 0.03870338, Accuracy: 90.00%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:   930, Loss: 0.03865704, Accuracy: 90.00%\n",
      "Iteration:   931, Loss: 0.03861083, Accuracy: 90.00%\n",
      "Iteration:   932, Loss: 0.03856476, Accuracy: 90.00%\n",
      "Iteration:   933, Loss: 0.03851886, Accuracy: 90.00%\n",
      "Iteration:   934, Loss: 0.03847312, Accuracy: 90.00%\n",
      "Iteration:   935, Loss: 0.03842754, Accuracy: 90.00%\n",
      "Iteration:   936, Loss: 0.03838210, Accuracy: 90.00%\n",
      "Iteration:   937, Loss: 0.03833680, Accuracy: 90.00%\n",
      "Iteration:   938, Loss: 0.03829161, Accuracy: 90.00%\n",
      "Iteration:   939, Loss: 0.03824654, Accuracy: 90.00%\n",
      "Iteration:   940, Loss: 0.03820156, Accuracy: 90.00%\n",
      "Iteration:   941, Loss: 0.03815668, Accuracy: 90.00%\n",
      "Iteration:   942, Loss: 0.03811187, Accuracy: 90.00%\n",
      "Iteration:   943, Loss: 0.03806713, Accuracy: 90.00%\n",
      "Iteration:   944, Loss: 0.03802245, Accuracy: 90.00%\n",
      "Iteration:   945, Loss: 0.03797783, Accuracy: 90.00%\n",
      "Iteration:   946, Loss: 0.03793327, Accuracy: 90.00%\n",
      "Iteration:   947, Loss: 0.03788876, Accuracy: 90.00%\n",
      "Iteration:   948, Loss: 0.03784433, Accuracy: 90.00%\n",
      "Iteration:   949, Loss: 0.03779998, Accuracy: 90.00%\n",
      "Iteration:   950, Loss: 0.03775573, Accuracy: 90.00%\n",
      "Iteration:   951, Loss: 0.03771161, Accuracy: 90.00%\n",
      "Iteration:   952, Loss: 0.03766762, Accuracy: 90.00%\n",
      "Iteration:   953, Loss: 0.03762376, Accuracy: 90.00%\n",
      "Iteration:   954, Loss: 0.03758004, Accuracy: 90.00%\n",
      "Iteration:   955, Loss: 0.03753645, Accuracy: 90.00%\n",
      "Iteration:   956, Loss: 0.03749298, Accuracy: 90.00%\n",
      "Iteration:   957, Loss: 0.03744963, Accuracy: 90.00%\n",
      "Iteration:   958, Loss: 0.03740639, Accuracy: 90.00%\n",
      "Iteration:   959, Loss: 0.03736325, Accuracy: 90.00%\n",
      "Iteration:   960, Loss: 0.03732022, Accuracy: 90.00%\n",
      "Iteration:   961, Loss: 0.03727729, Accuracy: 90.00%\n",
      "Iteration:   962, Loss: 0.03723446, Accuracy: 90.00%\n",
      "Iteration:   963, Loss: 0.03719172, Accuracy: 90.00%\n",
      "Iteration:   964, Loss: 0.03714908, Accuracy: 90.00%\n",
      "Iteration:   965, Loss: 0.03710654, Accuracy: 90.00%\n",
      "Iteration:   966, Loss: 0.03706409, Accuracy: 90.00%\n",
      "Iteration:   967, Loss: 0.03702174, Accuracy: 90.00%\n",
      "Iteration:   968, Loss: 0.03697948, Accuracy: 90.00%\n",
      "Iteration:   969, Loss: 0.03693732, Accuracy: 90.00%\n",
      "Iteration:   970, Loss: 0.03689525, Accuracy: 90.00%\n",
      "Iteration:   971, Loss: 0.03685327, Accuracy: 90.00%\n",
      "Iteration:   972, Loss: 0.03681139, Accuracy: 90.00%\n",
      "Iteration:   973, Loss: 0.03676960, Accuracy: 90.00%\n",
      "Iteration:   974, Loss: 0.03672790, Accuracy: 90.00%\n",
      "Iteration:   975, Loss: 0.03668629, Accuracy: 90.00%\n",
      "Iteration:   976, Loss: 0.03664476, Accuracy: 90.00%\n",
      "Iteration:   977, Loss: 0.03660332, Accuracy: 90.00%\n",
      "Iteration:   978, Loss: 0.03656197, Accuracy: 90.00%\n",
      "Iteration:   979, Loss: 0.03652070, Accuracy: 90.00%\n",
      "Iteration:   980, Loss: 0.03647950, Accuracy: 90.00%\n",
      "Iteration:   981, Loss: 0.03643839, Accuracy: 90.10%\n",
      "Iteration:   982, Loss: 0.03639734, Accuracy: 90.10%\n",
      "Iteration:   983, Loss: 0.03635636, Accuracy: 90.10%\n",
      "Iteration:   984, Loss: 0.03631544, Accuracy: 90.10%\n",
      "Iteration:   985, Loss: 0.03627458, Accuracy: 90.10%\n",
      "Iteration:   986, Loss: 0.03623377, Accuracy: 90.10%\n",
      "Iteration:   987, Loss: 0.03619302, Accuracy: 90.10%\n",
      "Iteration:   988, Loss: 0.03615231, Accuracy: 90.10%\n",
      "Iteration:   989, Loss: 0.03611165, Accuracy: 90.10%\n",
      "Iteration:   990, Loss: 0.03607104, Accuracy: 90.10%\n",
      "Iteration:   991, Loss: 0.03603049, Accuracy: 90.10%\n",
      "Iteration:   992, Loss: 0.03598998, Accuracy: 90.10%\n",
      "Iteration:   993, Loss: 0.03594954, Accuracy: 90.10%\n",
      "Iteration:   994, Loss: 0.03590916, Accuracy: 90.10%\n",
      "Iteration:   995, Loss: 0.03586885, Accuracy: 90.10%\n",
      "Iteration:   996, Loss: 0.03582862, Accuracy: 90.10%\n",
      "Iteration:   997, Loss: 0.03578847, Accuracy: 90.10%\n",
      "Iteration:   998, Loss: 0.03574841, Accuracy: 90.10%\n",
      "Iteration:   999, Loss: 0.03570843, Accuracy: 90.10%\n"
     ]
    }
   ],
   "source": [
    "n_train = 6000 # number of examples in training data (max is 60,000)\n",
    "n_test  = 1000 # number of examples in training data (max is 10,000)\n",
    "n_nodes = 3000 # number of hidden nodes\n",
    "n_iters = 1000 # number of iterations\n",
    "lr      = 0.03\n",
    "\n",
    "# create a string with the parameter values\n",
    "lr_str = str(lr).replace('.', 'p')\n",
    "name = 'success_rates_train{}_test{}_nodes{}_iters{}_lr'.format(n_train, n_test, n_nodes, n_iters)+lr_str\n",
    "\n",
    "\n",
    "(w1, w2, sr) = train(X_train[:n_train], Y_train[:n_train], \n",
    "               X_test[:n_test], Y_test[:n_test], \n",
    "               n_hidden_nodes=n_nodes, iterations=n_iters, lr=lr)\n",
    "# store the corrent success rates in the dictionary named success rates\n",
    "success_rates[name] = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "f64a9f16",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 504x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here you can plot several training curves alongside each other \n",
    "# uncomment, delete, edit, copy-paste lines correspondingly\n",
    "# edit the NAME of the results to be plotted AND edit the LABEL, such that the legend is correct\n",
    "plt.figure(figsize = (7,5))\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes30_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes30_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes100_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes100_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes300_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes300_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes784_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes784_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes1000_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes1000_iters1000_lr0.03')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('success rate in %')\n",
    "plt.grid()\n",
    "# plt.ylim([80,95]) # uncomment line to restrict the y-range for more detailed view on the late training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a41eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218b98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9baf391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises\n",
    "\n",
    "# explore the hyperparameters\n",
    "# - number of nodes\n",
    "# - learning rate\n",
    "# - size of traing data set\n",
    "#\n",
    "# keep size of test data set constant, e.g. 1000\n",
    "# \n",
    "# a strong increase in number of iterations beyond 1000 is  \n",
    "# recommended only AFTER exploring other prameters to some extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a484ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979e292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
