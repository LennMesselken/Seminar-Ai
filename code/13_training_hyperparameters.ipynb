{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "108c15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib \n",
    "from matplotlib import pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "981c1be2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load MNIST data\n",
    "\n",
    "import numpy as np\n",
    "import gzip\n",
    "import struct\n",
    "\n",
    "\n",
    "def load_images(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Read the header information into a bunch of variables:\n",
    "        _ignored, n_images, columns, rows = struct.unpack('>IIII', f.read(16))\n",
    "        # Read all the pixels into a NumPy array of bytes:\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        # Reshape the pixels into a matrix where each line is an image:\n",
    "        return all_pixels.reshape(n_images, columns * rows)\n",
    "\n",
    "\n",
    "# 60000 images, each 784 elements (28 * 28 pixels)\n",
    "X_train = load_images(\"../data/mnist/train-images-idx3-ubyte.gz\")\n",
    "\n",
    "# 10000 images, each 784 elements, with the same structure as X_train\n",
    "X_test = load_images(\"../data/mnist/t10k-images-idx3-ubyte.gz\")\n",
    "\n",
    "\n",
    "def load_labels(filename):\n",
    "    # Open and unzip the file of images:\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        # Skip the header bytes:\n",
    "        f.read(8)\n",
    "        # Read all the labels into a list:\n",
    "        all_labels = f.read()\n",
    "        # Reshape the list of labels into a one-column matrix:\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def one_hot_encode(Y):\n",
    "    n_labels = Y.shape[0]\n",
    "    n_classes = 10\n",
    "    encoded_Y = np.zeros((n_labels, n_classes))\n",
    "    for i in range(n_labels):\n",
    "        label = Y[i]\n",
    "        encoded_Y[i][label] = 1\n",
    "    return encoded_Y\n",
    "\n",
    "\n",
    "# !!! EDIT PATHS TO WHERE YOUR MNIST DATA IS !!!\n",
    "\n",
    "# 60K labels, each a single digit from 0 to 9\n",
    "Y_train_unencoded = load_labels(\"../data/mnist/train-labels-idx1-ubyte.gz\")\n",
    "\n",
    "# 60K labels, each consisting of 10 one-hot encoded elements\n",
    "Y_train = one_hot_encode(Y_train_unencoded)\n",
    "\n",
    "# 10000 labels, each a single digit from 0 to 9\n",
    "Y_test = load_labels(\"../data/mnist/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0848b839",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "0de8956a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A neural network implementation (almost the same as backpropagation.py,\n",
    "# except for a tiny refactoring in the back() function).\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def sigmoid(z):\n",
    "    return 1 / (1 + np.exp(-z))\n",
    "\n",
    "\n",
    "def softmax(logits):\n",
    "    exponentials = np.exp(logits)\n",
    "    return exponentials / np.sum(exponentials, axis=1).reshape(-1, 1)\n",
    "\n",
    "\n",
    "def sigmoid_gradient(sigmoid):\n",
    "    return np.multiply(sigmoid, (1 - sigmoid))\n",
    "\n",
    "\n",
    "def loss(Y, y_hat):\n",
    "    return -np.sum(Y * np.log(y_hat)) / Y.shape[0]\n",
    "\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "\n",
    "def forward(X, w1, w2):\n",
    "    h = sigmoid(np.matmul(prepend_bias(X), w1))\n",
    "    y_hat = softmax(np.matmul(prepend_bias(h), w2))\n",
    "    return (y_hat, h)\n",
    "\n",
    "\n",
    "def back(X, Y, y_hat, w2, h):\n",
    "    w2_gradient = np.matmul(prepend_bias(h).T, (y_hat - Y)) / X.shape[0]\n",
    "    w1_gradient = np.matmul(prepend_bias(X).T, np.matmul(y_hat - Y, w2[1:].T)\n",
    "                            * sigmoid_gradient(h)) / X.shape[0]\n",
    "    return (w1_gradient, w2_gradient)\n",
    "\n",
    "\n",
    "def classify(X, w1, w2):\n",
    "    y_hat, _ = forward(X, w1, w2)\n",
    "    labels = np.argmax(y_hat, axis=1)\n",
    "    return labels.reshape(-1, 1)\n",
    "\n",
    "\n",
    "def initialize_weights(n_input_variables, n_hidden_nodes, n_classes):\n",
    "    w1_rows = n_input_variables + 1\n",
    "    w1 = np.random.randn(w1_rows, n_hidden_nodes) * np.sqrt(1 / w1_rows)\n",
    "\n",
    "    w2_rows = n_hidden_nodes + 1\n",
    "    w2 = np.random.randn(w2_rows, n_classes) * np.sqrt(1 / w2_rows)\n",
    "\n",
    "    return (w1, w2)\n",
    "\n",
    "\n",
    "def report(iteration, X_train, Y_train, X_test, Y_test, w1, w2):\n",
    "    y_hat, _ = forward(X_train, w1, w2)\n",
    "    training_loss = loss(Y_train, y_hat)\n",
    "    classifications = classify(X_test, w1, w2)\n",
    "    accuracy = np.average(classifications == Y_test) * 100.0\n",
    "    print(\"Iteration: %5d, Loss: %.8f, Accuracy: %.2f%%\" %\n",
    "          (iteration, training_loss, accuracy))\n",
    "    return accuracy\n",
    "\n",
    "def report_JR(iteration, X_train, Y_train, X_test, Y_test, w):\n",
    "    matches = np.count_nonzero(classify(X_test, w) == Y_test)\n",
    "    n_test_ex = Y_test.shape[0]\n",
    "    matches = matches * 100.0 / n_test_ex\n",
    "    training_loss = loss(X_train, Y_train, w)\n",
    "    print('iteration {} - loss: {:.2f}, matches: {:.2f}%'.format(iteration, training_loss, matches))\n",
    "    return matches\n",
    "\n",
    "    \n",
    "\n",
    "def train(X_train, Y_train, X_test, Y_test, n_hidden_nodes, iterations, lr):\n",
    "    success_rates = []\n",
    "    n_input_variables = X_train.shape[1]\n",
    "    n_classes = Y_train.shape[1]\n",
    "    w1, w2 = initialize_weights(n_input_variables, n_hidden_nodes, n_classes)\n",
    "    for iteration in range(iterations):\n",
    "        y_hat, h = forward(X_train, w1, w2)\n",
    "        w1_gradient, w2_gradient = back(X_train, Y_train, y_hat, w2, h)\n",
    "        w1 = w1 - (w1_gradient * lr)\n",
    "        w2 = w2 - (w2_gradient * lr)\n",
    "        accuracy = report(iteration, X_train, Y_train, X_test, Y_test, w1, w2)\n",
    "        success_rates.append(accuracy)\n",
    "    return (w1, w2, success_rates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d1f6912",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5d4207e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "319f066d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "aa9a32b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a dictionary to store your success rates\n",
    "# !!! UNCOMMENT, EXECUTE, AND COMMENT THE LINE BELOW AGAIN !!!\n",
    "# !!! DON'T EXECUTE THE LINE AGAIN AFTER STARTING TO COLLECT RESULTS !!!\n",
    "\n",
    "#success_rates = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "78a59014",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/nb/r7d1zvcs4g5_lpdwx6rf5tgr0000gn/T/ipykernel_22124/2216911367.py:8: RuntimeWarning: overflow encountered in exp\n",
      "  return 1 / (1 + np.exp(-z))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration:     0, Loss: 5.66571201, Accuracy: 18.40%\n",
      "Iteration:     1, Loss: 8.38055444, Accuracy: 25.40%\n",
      "Iteration:     2, Loss: 5.44229088, Accuracy: 11.60%\n",
      "Iteration:     3, Loss: 3.33692981, Accuracy: 24.80%\n",
      "Iteration:     4, Loss: 2.57990879, Accuracy: 25.10%\n",
      "Iteration:     5, Loss: 2.20751892, Accuracy: 21.00%\n",
      "Iteration:     6, Loss: 1.80909260, Accuracy: 34.00%\n",
      "Iteration:     7, Loss: 1.67475712, Accuracy: 39.80%\n",
      "Iteration:     8, Loss: 1.48411859, Accuracy: 43.10%\n",
      "Iteration:     9, Loss: 1.52339969, Accuracy: 48.50%\n",
      "Iteration:    10, Loss: 1.30218938, Accuracy: 52.20%\n",
      "Iteration:    11, Loss: 1.20283218, Accuracy: 56.30%\n",
      "Iteration:    12, Loss: 1.01492351, Accuracy: 63.00%\n",
      "Iteration:    13, Loss: 0.95540001, Accuracy: 63.70%\n",
      "Iteration:    14, Loss: 0.96877443, Accuracy: 60.80%\n",
      "Iteration:    15, Loss: 0.95803558, Accuracy: 62.40%\n",
      "Iteration:    16, Loss: 0.90084630, Accuracy: 65.20%\n",
      "Iteration:    17, Loss: 0.77960553, Accuracy: 70.80%\n",
      "Iteration:    18, Loss: 0.72153488, Accuracy: 72.40%\n",
      "Iteration:    19, Loss: 0.67350786, Accuracy: 74.60%\n",
      "Iteration:    20, Loss: 0.66865440, Accuracy: 72.10%\n",
      "Iteration:    21, Loss: 0.70404254, Accuracy: 73.90%\n",
      "Iteration:    22, Loss: 0.73073308, Accuracy: 69.80%\n",
      "Iteration:    23, Loss: 0.89188296, Accuracy: 66.50%\n",
      "Iteration:    24, Loss: 0.75289730, Accuracy: 71.80%\n",
      "Iteration:    25, Loss: 0.70287733, Accuracy: 71.90%\n",
      "Iteration:    26, Loss: 0.58803917, Accuracy: 76.80%\n",
      "Iteration:    27, Loss: 0.55238620, Accuracy: 76.30%\n",
      "Iteration:    28, Loss: 0.55176565, Accuracy: 77.00%\n",
      "Iteration:    29, Loss: 0.52762007, Accuracy: 75.60%\n",
      "Iteration:    30, Loss: 0.57054597, Accuracy: 78.00%\n",
      "Iteration:    31, Loss: 0.55304925, Accuracy: 74.50%\n",
      "Iteration:    32, Loss: 0.54476530, Accuracy: 77.70%\n",
      "Iteration:    33, Loss: 0.45786343, Accuracy: 80.00%\n",
      "Iteration:    34, Loss: 0.41826258, Accuracy: 84.10%\n",
      "Iteration:    35, Loss: 0.37873420, Accuracy: 84.60%\n",
      "Iteration:    36, Loss: 0.35591514, Accuracy: 86.30%\n",
      "Iteration:    37, Loss: 0.35003989, Accuracy: 84.80%\n",
      "Iteration:    38, Loss: 0.33752767, Accuracy: 87.40%\n",
      "Iteration:    39, Loss: 0.32288474, Accuracy: 86.90%\n",
      "Iteration:    40, Loss: 0.32105349, Accuracy: 87.10%\n",
      "Iteration:    41, Loss: 0.31482398, Accuracy: 86.20%\n",
      "Iteration:    42, Loss: 0.31018125, Accuracy: 87.50%\n",
      "Iteration:    43, Loss: 0.30284951, Accuracy: 87.50%\n",
      "Iteration:    44, Loss: 0.29565287, Accuracy: 86.60%\n",
      "Iteration:    45, Loss: 0.28949489, Accuracy: 88.00%\n",
      "Iteration:    46, Loss: 0.28338501, Accuracy: 87.10%\n",
      "Iteration:    47, Loss: 0.27902678, Accuracy: 87.10%\n",
      "Iteration:    48, Loss: 0.26673673, Accuracy: 88.00%\n",
      "Iteration:    49, Loss: 0.26099102, Accuracy: 88.70%\n",
      "Iteration:    50, Loss: 0.25438949, Accuracy: 88.50%\n",
      "Iteration:    51, Loss: 0.25393749, Accuracy: 88.20%\n",
      "Iteration:    52, Loss: 0.24398597, Accuracy: 88.80%\n",
      "Iteration:    53, Loss: 0.23717264, Accuracy: 88.70%\n",
      "Iteration:    54, Loss: 0.23432696, Accuracy: 87.90%\n",
      "Iteration:    55, Loss: 0.22842338, Accuracy: 88.90%\n",
      "Iteration:    56, Loss: 0.22084949, Accuracy: 88.50%\n",
      "Iteration:    57, Loss: 0.22557106, Accuracy: 88.70%\n",
      "Iteration:    58, Loss: 0.22094533, Accuracy: 88.90%\n",
      "Iteration:    59, Loss: 0.22633381, Accuracy: 88.60%\n",
      "Iteration:    60, Loss: 0.22775557, Accuracy: 88.30%\n",
      "Iteration:    61, Loss: 0.22638620, Accuracy: 87.80%\n",
      "Iteration:    62, Loss: 0.21900987, Accuracy: 87.70%\n",
      "Iteration:    63, Loss: 0.20799100, Accuracy: 88.40%\n",
      "Iteration:    64, Loss: 0.20616318, Accuracy: 89.10%\n",
      "Iteration:    65, Loss: 0.19905943, Accuracy: 89.10%\n",
      "Iteration:    66, Loss: 0.19677125, Accuracy: 88.70%\n",
      "Iteration:    67, Loss: 0.19832533, Accuracy: 88.40%\n",
      "Iteration:    68, Loss: 0.19434831, Accuracy: 88.60%\n",
      "Iteration:    69, Loss: 0.19190083, Accuracy: 89.30%\n",
      "Iteration:    70, Loss: 0.19046190, Accuracy: 88.60%\n",
      "Iteration:    71, Loss: 0.19151914, Accuracy: 89.00%\n",
      "Iteration:    72, Loss: 0.18665957, Accuracy: 89.50%\n",
      "Iteration:    73, Loss: 0.18903285, Accuracy: 89.10%\n",
      "Iteration:    74, Loss: 0.19432010, Accuracy: 89.40%\n",
      "Iteration:    75, Loss: 0.19638307, Accuracy: 88.30%\n",
      "Iteration:    76, Loss: 0.19281267, Accuracy: 88.60%\n",
      "Iteration:    77, Loss: 0.19997227, Accuracy: 89.30%\n",
      "Iteration:    78, Loss: 0.18500987, Accuracy: 88.20%\n",
      "Iteration:    79, Loss: 0.19134015, Accuracy: 88.70%\n",
      "Iteration:    80, Loss: 0.18691982, Accuracy: 88.40%\n",
      "Iteration:    81, Loss: 0.18301686, Accuracy: 88.50%\n",
      "Iteration:    82, Loss: 0.17753902, Accuracy: 88.70%\n",
      "Iteration:    83, Loss: 0.17678933, Accuracy: 88.40%\n",
      "Iteration:    84, Loss: 0.17419487, Accuracy: 89.10%\n",
      "Iteration:    85, Loss: 0.16736253, Accuracy: 89.30%\n",
      "Iteration:    86, Loss: 0.16812248, Accuracy: 89.90%\n",
      "Iteration:    87, Loss: 0.16214352, Accuracy: 89.60%\n",
      "Iteration:    88, Loss: 0.15750055, Accuracy: 89.60%\n",
      "Iteration:    89, Loss: 0.15285146, Accuracy: 89.60%\n",
      "Iteration:    90, Loss: 0.14725410, Accuracy: 89.50%\n",
      "Iteration:    91, Loss: 0.14994503, Accuracy: 89.50%\n",
      "Iteration:    92, Loss: 0.14652838, Accuracy: 90.30%\n",
      "Iteration:    93, Loss: 0.14287084, Accuracy: 89.90%\n",
      "Iteration:    94, Loss: 0.14372734, Accuracy: 90.10%\n",
      "Iteration:    95, Loss: 0.14661198, Accuracy: 89.70%\n",
      "Iteration:    96, Loss: 0.14450318, Accuracy: 90.10%\n",
      "Iteration:    97, Loss: 0.13578919, Accuracy: 90.30%\n",
      "Iteration:    98, Loss: 0.13387632, Accuracy: 90.10%\n",
      "Iteration:    99, Loss: 0.13211089, Accuracy: 89.60%\n",
      "Iteration:   100, Loss: 0.13227424, Accuracy: 90.50%\n",
      "Iteration:   101, Loss: 0.12985877, Accuracy: 90.00%\n",
      "Iteration:   102, Loss: 0.13058462, Accuracy: 90.20%\n",
      "Iteration:   103, Loss: 0.13268807, Accuracy: 89.40%\n",
      "Iteration:   104, Loss: 0.12627249, Accuracy: 89.60%\n",
      "Iteration:   105, Loss: 0.12971594, Accuracy: 89.90%\n",
      "Iteration:   106, Loss: 0.12825432, Accuracy: 89.60%\n",
      "Iteration:   107, Loss: 0.12438592, Accuracy: 90.00%\n",
      "Iteration:   108, Loss: 0.12204190, Accuracy: 90.00%\n",
      "Iteration:   109, Loss: 0.12269711, Accuracy: 90.60%\n",
      "Iteration:   110, Loss: 0.11864822, Accuracy: 90.70%\n",
      "Iteration:   111, Loss: 0.11759343, Accuracy: 89.80%\n",
      "Iteration:   112, Loss: 0.11952082, Accuracy: 89.90%\n",
      "Iteration:   113, Loss: 0.11914713, Accuracy: 90.80%\n",
      "Iteration:   114, Loss: 0.11479665, Accuracy: 90.10%\n",
      "Iteration:   115, Loss: 0.11473234, Accuracy: 89.70%\n",
      "Iteration:   116, Loss: 0.11437674, Accuracy: 89.80%\n",
      "Iteration:   117, Loss: 0.11347453, Accuracy: 90.50%\n",
      "Iteration:   118, Loss: 0.10691131, Accuracy: 90.40%\n",
      "Iteration:   119, Loss: 0.10793402, Accuracy: 90.50%\n",
      "Iteration:   120, Loss: 0.10727222, Accuracy: 90.20%\n",
      "Iteration:   121, Loss: 0.11019366, Accuracy: 90.40%\n",
      "Iteration:   122, Loss: 0.11118102, Accuracy: 90.20%\n",
      "Iteration:   123, Loss: 0.10783619, Accuracy: 90.00%\n",
      "Iteration:   124, Loss: 0.10878197, Accuracy: 90.20%\n",
      "Iteration:   125, Loss: 0.10947441, Accuracy: 89.90%\n",
      "Iteration:   126, Loss: 0.10423538, Accuracy: 90.20%\n",
      "Iteration:   127, Loss: 0.10126177, Accuracy: 90.60%\n",
      "Iteration:   128, Loss: 0.10062811, Accuracy: 90.60%\n",
      "Iteration:   129, Loss: 0.10242171, Accuracy: 90.50%\n",
      "Iteration:   130, Loss: 0.10417251, Accuracy: 90.80%\n",
      "Iteration:   131, Loss: 0.10076042, Accuracy: 90.50%\n",
      "Iteration:   132, Loss: 0.09600063, Accuracy: 90.60%\n",
      "Iteration:   133, Loss: 0.09585889, Accuracy: 90.50%\n",
      "Iteration:   134, Loss: 0.09256004, Accuracy: 90.70%\n",
      "Iteration:   135, Loss: 0.09017138, Accuracy: 90.60%\n",
      "Iteration:   136, Loss: 0.08995003, Accuracy: 90.60%\n",
      "Iteration:   137, Loss: 0.09310486, Accuracy: 90.50%\n",
      "Iteration:   138, Loss: 0.08852083, Accuracy: 90.70%\n",
      "Iteration:   139, Loss: 0.08726647, Accuracy: 91.10%\n",
      "Iteration:   140, Loss: 0.08634132, Accuracy: 90.90%\n",
      "Iteration:   141, Loss: 0.08764095, Accuracy: 89.90%\n",
      "Iteration:   142, Loss: 0.09161605, Accuracy: 91.00%\n",
      "Iteration:   143, Loss: 0.09154918, Accuracy: 90.20%\n",
      "Iteration:   144, Loss: 0.08804816, Accuracy: 90.60%\n",
      "Iteration:   145, Loss: 0.08430827, Accuracy: 90.70%\n",
      "Iteration:   146, Loss: 0.08399969, Accuracy: 91.00%\n",
      "Iteration:   147, Loss: 0.08685983, Accuracy: 91.10%\n",
      "Iteration:   148, Loss: 0.08447340, Accuracy: 91.10%\n",
      "Iteration:   149, Loss: 0.08150281, Accuracy: 90.70%\n",
      "Iteration:   150, Loss: 0.08034869, Accuracy: 91.20%\n",
      "Iteration:   151, Loss: 0.08243642, Accuracy: 90.40%\n",
      "Iteration:   152, Loss: 0.08277133, Accuracy: 90.30%\n",
      "Iteration:   153, Loss: 0.08124593, Accuracy: 90.90%\n",
      "Iteration:   154, Loss: 0.08084916, Accuracy: 90.50%\n",
      "Iteration:   155, Loss: 0.08067698, Accuracy: 90.60%\n",
      "Iteration:   156, Loss: 0.08056705, Accuracy: 90.10%\n",
      "Iteration:   157, Loss: 0.08048181, Accuracy: 90.60%\n",
      "Iteration:   158, Loss: 0.07884176, Accuracy: 90.70%\n",
      "Iteration:   159, Loss: 0.07642654, Accuracy: 90.70%\n",
      "Iteration:   160, Loss: 0.07743012, Accuracy: 90.40%\n",
      "Iteration:   161, Loss: 0.07731189, Accuracy: 90.90%\n",
      "Iteration:   162, Loss: 0.07777732, Accuracy: 91.00%\n",
      "Iteration:   163, Loss: 0.07523523, Accuracy: 90.80%\n",
      "Iteration:   164, Loss: 0.07601638, Accuracy: 90.60%\n",
      "Iteration:   165, Loss: 0.07544973, Accuracy: 91.00%\n",
      "Iteration:   166, Loss: 0.07536389, Accuracy: 90.30%\n",
      "Iteration:   167, Loss: 0.07494012, Accuracy: 90.40%\n",
      "Iteration:   168, Loss: 0.07236473, Accuracy: 90.70%\n",
      "Iteration:   169, Loss: 0.07268546, Accuracy: 90.90%\n",
      "Iteration:   170, Loss: 0.07307109, Accuracy: 90.50%\n",
      "Iteration:   171, Loss: 0.07102795, Accuracy: 90.70%\n",
      "Iteration:   172, Loss: 0.06983225, Accuracy: 90.20%\n",
      "Iteration:   173, Loss: 0.06863654, Accuracy: 90.90%\n",
      "Iteration:   174, Loss: 0.07069447, Accuracy: 90.70%\n",
      "Iteration:   175, Loss: 0.06925744, Accuracy: 91.00%\n",
      "Iteration:   176, Loss: 0.06912609, Accuracy: 90.40%\n",
      "Iteration:   177, Loss: 0.06883146, Accuracy: 91.00%\n",
      "Iteration:   178, Loss: 0.06864536, Accuracy: 91.20%\n",
      "Iteration:   179, Loss: 0.06556731, Accuracy: 90.90%\n",
      "Iteration:   180, Loss: 0.06445854, Accuracy: 90.70%\n",
      "Iteration:   181, Loss: 0.06596267, Accuracy: 90.40%\n",
      "Iteration:   182, Loss: 0.06581438, Accuracy: 90.60%\n",
      "Iteration:   183, Loss: 0.06326968, Accuracy: 91.20%\n",
      "Iteration:   184, Loss: 0.06309853, Accuracy: 90.60%\n",
      "Iteration:   185, Loss: 0.06251626, Accuracy: 91.20%\n",
      "Iteration:   186, Loss: 0.06224941, Accuracy: 91.10%\n",
      "Iteration:   187, Loss: 0.06256181, Accuracy: 91.10%\n",
      "Iteration:   188, Loss: 0.06270384, Accuracy: 90.90%\n",
      "Iteration:   189, Loss: 0.06292777, Accuracy: 90.90%\n",
      "Iteration:   190, Loss: 0.06560907, Accuracy: 90.70%\n",
      "Iteration:   191, Loss: 0.06323027, Accuracy: 91.00%\n",
      "Iteration:   192, Loss: 0.06318542, Accuracy: 91.00%\n",
      "Iteration:   193, Loss: 0.06363688, Accuracy: 90.80%\n",
      "Iteration:   194, Loss: 0.06390301, Accuracy: 90.90%\n",
      "Iteration:   195, Loss: 0.06117397, Accuracy: 90.40%\n",
      "Iteration:   196, Loss: 0.06182745, Accuracy: 90.70%\n",
      "Iteration:   197, Loss: 0.06099565, Accuracy: 90.70%\n",
      "Iteration:   198, Loss: 0.06058241, Accuracy: 90.70%\n",
      "Iteration:   199, Loss: 0.06080925, Accuracy: 90.60%\n",
      "Iteration:   200, Loss: 0.05884227, Accuracy: 91.10%\n",
      "Iteration:   201, Loss: 0.05863767, Accuracy: 91.30%\n",
      "Iteration:   202, Loss: 0.05766303, Accuracy: 91.00%\n",
      "Iteration:   203, Loss: 0.05837642, Accuracy: 91.00%\n",
      "Iteration:   204, Loss: 0.05860958, Accuracy: 91.10%\n",
      "Iteration:   205, Loss: 0.06096945, Accuracy: 91.30%\n",
      "Iteration:   206, Loss: 0.06105485, Accuracy: 90.90%\n",
      "Iteration:   207, Loss: 0.05883258, Accuracy: 90.60%\n",
      "Iteration:   208, Loss: 0.05890696, Accuracy: 90.80%\n",
      "Iteration:   209, Loss: 0.05846391, Accuracy: 90.40%\n",
      "Iteration:   210, Loss: 0.05823171, Accuracy: 91.20%\n",
      "Iteration:   211, Loss: 0.05694035, Accuracy: 90.90%\n",
      "Iteration:   212, Loss: 0.05729579, Accuracy: 90.70%\n",
      "Iteration:   213, Loss: 0.05808456, Accuracy: 90.60%\n",
      "Iteration:   214, Loss: 0.05657837, Accuracy: 90.60%\n",
      "Iteration:   215, Loss: 0.05714182, Accuracy: 90.70%\n",
      "Iteration:   216, Loss: 0.05715764, Accuracy: 91.10%\n",
      "Iteration:   217, Loss: 0.05647720, Accuracy: 91.10%\n",
      "Iteration:   218, Loss: 0.05488002, Accuracy: 91.20%\n",
      "Iteration:   219, Loss: 0.05517891, Accuracy: 91.30%\n",
      "Iteration:   220, Loss: 0.05495574, Accuracy: 91.30%\n",
      "Iteration:   221, Loss: 0.05464681, Accuracy: 91.30%\n",
      "Iteration:   222, Loss: 0.05482126, Accuracy: 91.00%\n",
      "Iteration:   223, Loss: 0.05474211, Accuracy: 91.40%\n",
      "Iteration:   224, Loss: 0.05486375, Accuracy: 91.20%\n",
      "Iteration:   225, Loss: 0.05394338, Accuracy: 91.10%\n",
      "Iteration:   226, Loss: 0.05402853, Accuracy: 91.50%\n",
      "Iteration:   227, Loss: 0.05301144, Accuracy: 91.00%\n",
      "Iteration:   228, Loss: 0.05217506, Accuracy: 91.00%\n",
      "Iteration:   229, Loss: 0.05248716, Accuracy: 90.80%\n",
      "Iteration:   230, Loss: 0.05482952, Accuracy: 91.10%\n",
      "Iteration:   231, Loss: 0.05553456, Accuracy: 91.10%\n",
      "Iteration:   232, Loss: 0.05401123, Accuracy: 91.00%\n",
      "Iteration:   233, Loss: 0.05627239, Accuracy: 90.90%\n",
      "Iteration:   234, Loss: 0.05497002, Accuracy: 90.70%\n",
      "Iteration:   235, Loss: 0.05442936, Accuracy: 90.60%\n",
      "Iteration:   236, Loss: 0.05343163, Accuracy: 90.90%\n",
      "Iteration:   237, Loss: 0.05244634, Accuracy: 91.00%\n",
      "Iteration:   238, Loss: 0.05164012, Accuracy: 91.10%\n",
      "Iteration:   239, Loss: 0.05140097, Accuracy: 90.70%\n",
      "Iteration:   240, Loss: 0.05160740, Accuracy: 91.00%\n",
      "Iteration:   241, Loss: 0.05128041, Accuracy: 90.80%\n",
      "Iteration:   242, Loss: 0.05146883, Accuracy: 90.70%\n",
      "Iteration:   243, Loss: 0.05102548, Accuracy: 90.70%\n",
      "Iteration:   244, Loss: 0.05050939, Accuracy: 91.10%\n",
      "Iteration:   245, Loss: 0.04970964, Accuracy: 90.90%\n",
      "Iteration:   246, Loss: 0.04980900, Accuracy: 90.90%\n",
      "Iteration:   247, Loss: 0.05026930, Accuracy: 90.90%\n",
      "Iteration:   248, Loss: 0.04984235, Accuracy: 90.50%\n",
      "Iteration:   249, Loss: 0.04944118, Accuracy: 91.20%\n",
      "Iteration:   250, Loss: 0.04903000, Accuracy: 90.90%\n",
      "Iteration:   251, Loss: 0.04924648, Accuracy: 90.90%\n",
      "Iteration:   252, Loss: 0.04955781, Accuracy: 91.00%\n",
      "Iteration:   253, Loss: 0.04956269, Accuracy: 91.00%\n",
      "Iteration:   254, Loss: 0.04986060, Accuracy: 90.90%\n",
      "Iteration:   255, Loss: 0.04897991, Accuracy: 91.10%\n",
      "Iteration:   256, Loss: 0.04909185, Accuracy: 91.10%\n",
      "Iteration:   257, Loss: 0.04834270, Accuracy: 91.20%\n",
      "Iteration:   258, Loss: 0.04804462, Accuracy: 91.10%\n",
      "Iteration:   259, Loss: 0.04818871, Accuracy: 91.30%\n",
      "Iteration:   260, Loss: 0.04817484, Accuracy: 90.80%\n",
      "Iteration:   261, Loss: 0.04924636, Accuracy: 90.80%\n",
      "Iteration:   262, Loss: 0.04833194, Accuracy: 90.80%\n",
      "Iteration:   263, Loss: 0.04828136, Accuracy: 90.50%\n",
      "Iteration:   264, Loss: 0.04714138, Accuracy: 90.90%\n",
      "Iteration:   265, Loss: 0.04754748, Accuracy: 90.90%\n",
      "Iteration:   266, Loss: 0.04711334, Accuracy: 91.00%\n",
      "Iteration:   267, Loss: 0.04737609, Accuracy: 91.10%\n",
      "Iteration:   268, Loss: 0.04846395, Accuracy: 91.20%\n",
      "Iteration:   269, Loss: 0.04759547, Accuracy: 91.10%\n",
      "Iteration:   270, Loss: 0.04724480, Accuracy: 91.20%\n",
      "Iteration:   271, Loss: 0.04635668, Accuracy: 90.80%\n",
      "Iteration:   272, Loss: 0.04616868, Accuracy: 91.10%\n",
      "Iteration:   273, Loss: 0.04615939, Accuracy: 91.00%\n",
      "Iteration:   274, Loss: 0.04592512, Accuracy: 90.70%\n",
      "Iteration:   275, Loss: 0.04572451, Accuracy: 91.10%\n",
      "Iteration:   276, Loss: 0.04560381, Accuracy: 91.20%\n",
      "Iteration:   277, Loss: 0.04562486, Accuracy: 91.00%\n",
      "Iteration:   278, Loss: 0.04544858, Accuracy: 91.10%\n",
      "Iteration:   279, Loss: 0.04562996, Accuracy: 91.10%\n",
      "Iteration:   280, Loss: 0.04574144, Accuracy: 91.20%\n",
      "Iteration:   281, Loss: 0.04508912, Accuracy: 91.10%\n",
      "Iteration:   282, Loss: 0.04490677, Accuracy: 91.10%\n",
      "Iteration:   283, Loss: 0.04485902, Accuracy: 91.10%\n",
      "Iteration:   284, Loss: 0.04474223, Accuracy: 91.10%\n",
      "Iteration:   285, Loss: 0.04471464, Accuracy: 91.00%\n",
      "Iteration:   286, Loss: 0.04467775, Accuracy: 91.00%\n",
      "Iteration:   287, Loss: 0.04463234, Accuracy: 91.20%\n",
      "Iteration:   288, Loss: 0.04449246, Accuracy: 91.10%\n",
      "Iteration:   289, Loss: 0.04439865, Accuracy: 91.10%\n",
      "Iteration:   290, Loss: 0.04436180, Accuracy: 91.10%\n",
      "Iteration:   291, Loss: 0.04439067, Accuracy: 91.00%\n",
      "Iteration:   292, Loss: 0.04413940, Accuracy: 91.00%\n",
      "Iteration:   293, Loss: 0.04420277, Accuracy: 91.30%\n",
      "Iteration:   294, Loss: 0.04390705, Accuracy: 90.80%\n",
      "Iteration:   295, Loss: 0.04378686, Accuracy: 91.20%\n",
      "Iteration:   296, Loss: 0.04395691, Accuracy: 90.90%\n",
      "Iteration:   297, Loss: 0.04390284, Accuracy: 91.10%\n",
      "Iteration:   298, Loss: 0.04404920, Accuracy: 90.90%\n",
      "Iteration:   299, Loss: 0.04349920, Accuracy: 90.90%\n",
      "Iteration:   300, Loss: 0.04356653, Accuracy: 90.90%\n",
      "Iteration:   301, Loss: 0.04337318, Accuracy: 90.70%\n",
      "Iteration:   302, Loss: 0.04374150, Accuracy: 91.00%\n",
      "Iteration:   303, Loss: 0.04335891, Accuracy: 90.80%\n",
      "Iteration:   304, Loss: 0.04307591, Accuracy: 90.80%\n",
      "Iteration:   305, Loss: 0.04296850, Accuracy: 90.80%\n",
      "Iteration:   306, Loss: 0.04288099, Accuracy: 90.90%\n",
      "Iteration:   307, Loss: 0.04288084, Accuracy: 90.80%\n",
      "Iteration:   308, Loss: 0.04387270, Accuracy: 90.90%\n",
      "Iteration:   309, Loss: 0.04318125, Accuracy: 90.80%\n",
      "Iteration:   310, Loss: 0.04258277, Accuracy: 90.70%\n",
      "Iteration:   311, Loss: 0.04241747, Accuracy: 90.80%\n",
      "Iteration:   312, Loss: 0.04227056, Accuracy: 90.70%\n",
      "Iteration:   313, Loss: 0.04221296, Accuracy: 90.60%\n",
      "Iteration:   314, Loss: 0.04253344, Accuracy: 91.00%\n",
      "Iteration:   315, Loss: 0.04247280, Accuracy: 90.80%\n",
      "Iteration:   316, Loss: 0.04213735, Accuracy: 90.90%\n",
      "Iteration:   317, Loss: 0.04194272, Accuracy: 90.60%\n",
      "Iteration:   318, Loss: 0.04185156, Accuracy: 90.90%\n",
      "Iteration:   319, Loss: 0.04173666, Accuracy: 90.60%\n",
      "Iteration:   320, Loss: 0.04174683, Accuracy: 90.80%\n",
      "Iteration:   321, Loss: 0.04212019, Accuracy: 90.50%\n",
      "Iteration:   322, Loss: 0.04158269, Accuracy: 90.90%\n",
      "Iteration:   323, Loss: 0.04143864, Accuracy: 90.80%\n",
      "Iteration:   324, Loss: 0.04138663, Accuracy: 90.70%\n",
      "Iteration:   325, Loss: 0.04131442, Accuracy: 90.70%\n",
      "Iteration:   326, Loss: 0.04130846, Accuracy: 90.70%\n",
      "Iteration:   327, Loss: 0.04155053, Accuracy: 90.60%\n",
      "Iteration:   328, Loss: 0.04111918, Accuracy: 90.80%\n",
      "Iteration:   329, Loss: 0.04098812, Accuracy: 90.70%\n",
      "Iteration:   330, Loss: 0.04091943, Accuracy: 90.70%\n",
      "Iteration:   331, Loss: 0.04086092, Accuracy: 90.60%\n",
      "Iteration:   332, Loss: 0.04082344, Accuracy: 90.70%\n",
      "Iteration:   333, Loss: 0.04079100, Accuracy: 90.60%\n",
      "Iteration:   334, Loss: 0.04067126, Accuracy: 90.70%\n",
      "Iteration:   335, Loss: 0.04058536, Accuracy: 90.70%\n",
      "Iteration:   336, Loss: 0.04049599, Accuracy: 90.70%\n",
      "Iteration:   337, Loss: 0.04041459, Accuracy: 90.70%\n",
      "Iteration:   338, Loss: 0.04039313, Accuracy: 90.80%\n",
      "Iteration:   339, Loss: 0.04053708, Accuracy: 90.80%\n",
      "Iteration:   340, Loss: 0.04048669, Accuracy: 90.80%\n",
      "Iteration:   341, Loss: 0.04029847, Accuracy: 90.90%\n",
      "Iteration:   342, Loss: 0.04035619, Accuracy: 90.80%\n",
      "Iteration:   343, Loss: 0.04032930, Accuracy: 90.80%\n",
      "Iteration:   344, Loss: 0.04001428, Accuracy: 90.80%\n",
      "Iteration:   345, Loss: 0.03991504, Accuracy: 90.80%\n",
      "Iteration:   346, Loss: 0.03983932, Accuracy: 90.80%\n",
      "Iteration:   347, Loss: 0.03980933, Accuracy: 90.70%\n",
      "Iteration:   348, Loss: 0.03969069, Accuracy: 90.80%\n",
      "Iteration:   349, Loss: 0.03960374, Accuracy: 90.70%\n",
      "Iteration:   350, Loss: 0.03953198, Accuracy: 90.80%\n",
      "Iteration:   351, Loss: 0.03956976, Accuracy: 90.70%\n",
      "Iteration:   352, Loss: 0.03944805, Accuracy: 90.80%\n",
      "Iteration:   353, Loss: 0.03937358, Accuracy: 90.70%\n",
      "Iteration:   354, Loss: 0.03928551, Accuracy: 90.80%\n",
      "Iteration:   355, Loss: 0.03923726, Accuracy: 90.80%\n",
      "Iteration:   356, Loss: 0.03918460, Accuracy: 90.80%\n",
      "Iteration:   357, Loss: 0.03917327, Accuracy: 90.90%\n",
      "Iteration:   358, Loss: 0.03918923, Accuracy: 90.80%\n",
      "Iteration:   359, Loss: 0.03905044, Accuracy: 90.70%\n",
      "Iteration:   360, Loss: 0.03901535, Accuracy: 90.90%\n",
      "Iteration:   361, Loss: 0.03938162, Accuracy: 90.60%\n",
      "Iteration:   362, Loss: 0.03924572, Accuracy: 90.60%\n",
      "Iteration:   363, Loss: 0.03920828, Accuracy: 90.60%\n",
      "Iteration:   364, Loss: 0.03950355, Accuracy: 90.80%\n",
      "Iteration:   365, Loss: 0.03897717, Accuracy: 90.80%\n",
      "Iteration:   366, Loss: 0.03992436, Accuracy: 90.90%\n",
      "Iteration:   367, Loss: 0.03917702, Accuracy: 90.90%\n",
      "Iteration:   368, Loss: 0.03894915, Accuracy: 90.90%\n",
      "Iteration:   369, Loss: 0.04000222, Accuracy: 91.00%\n",
      "Iteration:   370, Loss: 0.03948474, Accuracy: 90.90%\n",
      "Iteration:   371, Loss: 0.03898931, Accuracy: 91.10%\n",
      "Iteration:   372, Loss: 0.03881475, Accuracy: 90.70%\n",
      "Iteration:   373, Loss: 0.03875680, Accuracy: 91.00%\n",
      "Iteration:   374, Loss: 0.03855992, Accuracy: 90.90%\n",
      "Iteration:   375, Loss: 0.03829895, Accuracy: 91.00%\n",
      "Iteration:   376, Loss: 0.03805692, Accuracy: 90.90%\n",
      "Iteration:   377, Loss: 0.03808180, Accuracy: 91.00%\n",
      "Iteration:   378, Loss: 0.03802388, Accuracy: 91.10%\n",
      "Iteration:   379, Loss: 0.03790250, Accuracy: 90.90%\n",
      "Iteration:   380, Loss: 0.03778418, Accuracy: 91.00%\n",
      "Iteration:   381, Loss: 0.03768716, Accuracy: 90.90%\n",
      "Iteration:   382, Loss: 0.03761983, Accuracy: 90.90%\n",
      "Iteration:   383, Loss: 0.03757317, Accuracy: 90.90%\n",
      "Iteration:   384, Loss: 0.03757338, Accuracy: 91.00%\n",
      "Iteration:   385, Loss: 0.03745439, Accuracy: 90.90%\n",
      "Iteration:   386, Loss: 0.03736963, Accuracy: 90.90%\n",
      "Iteration:   387, Loss: 0.03731297, Accuracy: 90.90%\n",
      "Iteration:   388, Loss: 0.03724556, Accuracy: 90.90%\n",
      "Iteration:   389, Loss: 0.03719503, Accuracy: 90.70%\n",
      "Iteration:   390, Loss: 0.03724635, Accuracy: 90.90%\n",
      "Iteration:   391, Loss: 0.03757547, Accuracy: 90.60%\n",
      "Iteration:   392, Loss: 0.03740066, Accuracy: 90.50%\n",
      "Iteration:   393, Loss: 0.03737355, Accuracy: 91.00%\n",
      "Iteration:   394, Loss: 0.03799798, Accuracy: 90.50%\n",
      "Iteration:   395, Loss: 0.03784465, Accuracy: 90.70%\n",
      "Iteration:   396, Loss: 0.03712916, Accuracy: 90.70%\n",
      "Iteration:   397, Loss: 0.03701594, Accuracy: 90.70%\n",
      "Iteration:   398, Loss: 0.03708503, Accuracy: 90.80%\n",
      "Iteration:   399, Loss: 0.03679926, Accuracy: 90.90%\n",
      "Iteration:   400, Loss: 0.03677219, Accuracy: 91.00%\n",
      "Iteration:   401, Loss: 0.03667527, Accuracy: 90.90%\n",
      "Iteration:   402, Loss: 0.03686859, Accuracy: 90.60%\n",
      "Iteration:   403, Loss: 0.03669845, Accuracy: 90.50%\n",
      "Iteration:   404, Loss: 0.03656482, Accuracy: 90.90%\n",
      "Iteration:   405, Loss: 0.03657657, Accuracy: 90.70%\n",
      "Iteration:   406, Loss: 0.03631241, Accuracy: 90.80%\n",
      "Iteration:   407, Loss: 0.03626475, Accuracy: 90.90%\n",
      "Iteration:   408, Loss: 0.03618661, Accuracy: 90.80%\n",
      "Iteration:   409, Loss: 0.03613401, Accuracy: 90.80%\n",
      "Iteration:   410, Loss: 0.03610728, Accuracy: 90.70%\n",
      "Iteration:   411, Loss: 0.03604058, Accuracy: 90.90%\n",
      "Iteration:   412, Loss: 0.03598767, Accuracy: 90.70%\n",
      "Iteration:   413, Loss: 0.03592637, Accuracy: 90.70%\n",
      "Iteration:   414, Loss: 0.03587132, Accuracy: 90.70%\n",
      "Iteration:   415, Loss: 0.03581880, Accuracy: 90.70%\n",
      "Iteration:   416, Loss: 0.03576491, Accuracy: 90.70%\n",
      "Iteration:   417, Loss: 0.03572123, Accuracy: 90.70%\n",
      "Iteration:   418, Loss: 0.03567827, Accuracy: 90.70%\n",
      "Iteration:   419, Loss: 0.03571038, Accuracy: 90.90%\n",
      "Iteration:   420, Loss: 0.03562380, Accuracy: 90.60%\n",
      "Iteration:   421, Loss: 0.03560293, Accuracy: 90.90%\n",
      "Iteration:   422, Loss: 0.03548683, Accuracy: 90.70%\n",
      "Iteration:   423, Loss: 0.03542892, Accuracy: 90.70%\n",
      "Iteration:   424, Loss: 0.03539228, Accuracy: 90.70%\n",
      "Iteration:   425, Loss: 0.03537140, Accuracy: 90.70%\n",
      "Iteration:   426, Loss: 0.03529614, Accuracy: 90.70%\n",
      "Iteration:   427, Loss: 0.03534403, Accuracy: 90.70%\n",
      "Iteration:   428, Loss: 0.03527038, Accuracy: 90.80%\n",
      "Iteration:   429, Loss: 0.03515572, Accuracy: 90.70%\n",
      "Iteration:   430, Loss: 0.03511838, Accuracy: 90.80%\n",
      "Iteration:   431, Loss: 0.03504535, Accuracy: 90.70%\n",
      "Iteration:   432, Loss: 0.03498692, Accuracy: 90.70%\n",
      "Iteration:   433, Loss: 0.03493926, Accuracy: 90.70%\n",
      "Iteration:   434, Loss: 0.03487833, Accuracy: 90.70%\n",
      "Iteration:   435, Loss: 0.03482982, Accuracy: 90.70%\n",
      "Iteration:   436, Loss: 0.03478287, Accuracy: 90.70%\n",
      "Iteration:   437, Loss: 0.03473452, Accuracy: 90.70%\n",
      "Iteration:   438, Loss: 0.03468780, Accuracy: 90.70%\n",
      "Iteration:   439, Loss: 0.03464101, Accuracy: 90.70%\n",
      "Iteration:   440, Loss: 0.03459405, Accuracy: 90.70%\n",
      "Iteration:   441, Loss: 0.03454644, Accuracy: 90.70%\n",
      "Iteration:   442, Loss: 0.03450128, Accuracy: 90.70%\n",
      "Iteration:   443, Loss: 0.03445514, Accuracy: 90.70%\n",
      "Iteration:   444, Loss: 0.03441011, Accuracy: 90.70%\n",
      "Iteration:   445, Loss: 0.03436511, Accuracy: 90.70%\n",
      "Iteration:   446, Loss: 0.03432224, Accuracy: 90.60%\n",
      "Iteration:   447, Loss: 0.03427554, Accuracy: 90.60%\n",
      "Iteration:   448, Loss: 0.03423616, Accuracy: 90.60%\n",
      "Iteration:   449, Loss: 0.03418437, Accuracy: 90.60%\n",
      "Iteration:   450, Loss: 0.03413809, Accuracy: 90.60%\n",
      "Iteration:   451, Loss: 0.03409338, Accuracy: 90.60%\n",
      "Iteration:   452, Loss: 0.03405667, Accuracy: 90.60%\n",
      "Iteration:   453, Loss: 0.03402989, Accuracy: 90.60%\n",
      "Iteration:   454, Loss: 0.03419570, Accuracy: 90.80%\n",
      "Iteration:   455, Loss: 0.03469863, Accuracy: 90.60%\n",
      "Iteration:   456, Loss: 0.03393857, Accuracy: 90.70%\n",
      "Iteration:   457, Loss: 0.03387897, Accuracy: 90.60%\n",
      "Iteration:   458, Loss: 0.03382593, Accuracy: 90.70%\n",
      "Iteration:   459, Loss: 0.03377719, Accuracy: 90.70%\n",
      "Iteration:   460, Loss: 0.03371205, Accuracy: 90.70%\n",
      "Iteration:   461, Loss: 0.03369710, Accuracy: 90.70%\n",
      "Iteration:   462, Loss: 0.03376309, Accuracy: 90.70%\n",
      "Iteration:   463, Loss: 0.03380234, Accuracy: 90.60%\n",
      "Iteration:   464, Loss: 0.03370271, Accuracy: 90.70%\n",
      "Iteration:   465, Loss: 0.03360310, Accuracy: 90.70%\n",
      "Iteration:   466, Loss: 0.03345244, Accuracy: 90.70%\n",
      "Iteration:   467, Loss: 0.03339805, Accuracy: 90.60%\n",
      "Iteration:   468, Loss: 0.03334205, Accuracy: 90.60%\n",
      "Iteration:   469, Loss: 0.03328644, Accuracy: 90.50%\n",
      "Iteration:   470, Loss: 0.03324649, Accuracy: 90.60%\n",
      "Iteration:   471, Loss: 0.03319691, Accuracy: 90.50%\n",
      "Iteration:   472, Loss: 0.03315077, Accuracy: 90.50%\n",
      "Iteration:   473, Loss: 0.03310726, Accuracy: 90.40%\n",
      "Iteration:   474, Loss: 0.03306280, Accuracy: 90.50%\n",
      "Iteration:   475, Loss: 0.03299544, Accuracy: 90.50%\n",
      "Iteration:   476, Loss: 0.03295789, Accuracy: 90.50%\n",
      "Iteration:   477, Loss: 0.03307999, Accuracy: 90.40%\n",
      "Iteration:   478, Loss: 0.03301581, Accuracy: 90.40%\n",
      "Iteration:   479, Loss: 0.03282655, Accuracy: 90.40%\n",
      "Iteration:   480, Loss: 0.03277954, Accuracy: 90.50%\n",
      "Iteration:   481, Loss: 0.03273892, Accuracy: 90.40%\n",
      "Iteration:   482, Loss: 0.03269856, Accuracy: 90.50%\n",
      "Iteration:   483, Loss: 0.03265021, Accuracy: 90.40%\n",
      "Iteration:   484, Loss: 0.03259065, Accuracy: 90.40%\n",
      "Iteration:   485, Loss: 0.03263918, Accuracy: 90.40%\n",
      "Iteration:   486, Loss: 0.03295114, Accuracy: 90.40%\n",
      "Iteration:   487, Loss: 0.03290950, Accuracy: 90.40%\n",
      "Iteration:   488, Loss: 0.03278951, Accuracy: 90.50%\n",
      "Iteration:   489, Loss: 0.03271607, Accuracy: 90.40%\n",
      "Iteration:   490, Loss: 0.03265320, Accuracy: 90.40%\n",
      "Iteration:   491, Loss: 0.03254581, Accuracy: 90.40%\n",
      "Iteration:   492, Loss: 0.03251309, Accuracy: 90.40%\n",
      "Iteration:   493, Loss: 0.03252613, Accuracy: 90.50%\n",
      "Iteration:   494, Loss: 0.03239942, Accuracy: 90.60%\n",
      "Iteration:   495, Loss: 0.03228887, Accuracy: 90.50%\n",
      "Iteration:   496, Loss: 0.03250915, Accuracy: 90.60%\n",
      "Iteration:   497, Loss: 0.03255753, Accuracy: 90.70%\n",
      "Iteration:   498, Loss: 0.03244937, Accuracy: 90.70%\n",
      "Iteration:   499, Loss: 0.03236455, Accuracy: 90.50%\n",
      "Iteration:   500, Loss: 0.03277470, Accuracy: 90.90%\n",
      "Iteration:   501, Loss: 0.03252919, Accuracy: 90.50%\n",
      "Iteration:   502, Loss: 0.03234683, Accuracy: 90.40%\n",
      "Iteration:   503, Loss: 0.03208208, Accuracy: 90.50%\n",
      "Iteration:   504, Loss: 0.03188948, Accuracy: 90.70%\n",
      "Iteration:   505, Loss: 0.03184076, Accuracy: 90.50%\n",
      "Iteration:   506, Loss: 0.03195869, Accuracy: 90.70%\n",
      "Iteration:   507, Loss: 0.03181143, Accuracy: 90.50%\n",
      "Iteration:   508, Loss: 0.03184297, Accuracy: 90.60%\n",
      "Iteration:   509, Loss: 0.03172202, Accuracy: 90.40%\n",
      "Iteration:   510, Loss: 0.03171685, Accuracy: 90.50%\n",
      "Iteration:   511, Loss: 0.03153646, Accuracy: 90.50%\n",
      "Iteration:   512, Loss: 0.03147715, Accuracy: 90.50%\n",
      "Iteration:   513, Loss: 0.03143607, Accuracy: 90.50%\n",
      "Iteration:   514, Loss: 0.03129932, Accuracy: 90.50%\n",
      "Iteration:   515, Loss: 0.03125205, Accuracy: 90.50%\n",
      "Iteration:   516, Loss: 0.03121129, Accuracy: 90.50%\n",
      "Iteration:   517, Loss: 0.03117283, Accuracy: 90.50%\n",
      "Iteration:   518, Loss: 0.03113407, Accuracy: 90.50%\n",
      "Iteration:   519, Loss: 0.03109558, Accuracy: 90.50%\n",
      "Iteration:   520, Loss: 0.03105692, Accuracy: 90.50%\n",
      "Iteration:   521, Loss: 0.03101743, Accuracy: 90.50%\n",
      "Iteration:   522, Loss: 0.03097876, Accuracy: 90.50%\n",
      "Iteration:   523, Loss: 0.03094106, Accuracy: 90.50%\n",
      "Iteration:   524, Loss: 0.03090364, Accuracy: 90.50%\n",
      "Iteration:   525, Loss: 0.03086657, Accuracy: 90.50%\n",
      "Iteration:   526, Loss: 0.03082894, Accuracy: 90.50%\n",
      "Iteration:   527, Loss: 0.03079075, Accuracy: 90.50%\n",
      "Iteration:   528, Loss: 0.03074934, Accuracy: 90.50%\n",
      "Iteration:   529, Loss: 0.03062889, Accuracy: 90.50%\n",
      "Iteration:   530, Loss: 0.03068140, Accuracy: 90.50%\n",
      "Iteration:   531, Loss: 0.03054120, Accuracy: 90.50%\n",
      "Iteration:   532, Loss: 0.03050523, Accuracy: 90.50%\n",
      "Iteration:   533, Loss: 0.03046918, Accuracy: 90.50%\n",
      "Iteration:   534, Loss: 0.03043020, Accuracy: 90.50%\n",
      "Iteration:   535, Loss: 0.03039346, Accuracy: 90.50%\n",
      "Iteration:   536, Loss: 0.03035657, Accuracy: 90.50%\n",
      "Iteration:   537, Loss: 0.03031882, Accuracy: 90.60%\n",
      "Iteration:   538, Loss: 0.03028201, Accuracy: 90.60%\n",
      "Iteration:   539, Loss: 0.03024477, Accuracy: 90.50%\n",
      "Iteration:   540, Loss: 0.03020942, Accuracy: 90.60%\n",
      "Iteration:   541, Loss: 0.03017402, Accuracy: 90.60%\n",
      "Iteration:   542, Loss: 0.03013758, Accuracy: 90.60%\n",
      "Iteration:   543, Loss: 0.03009680, Accuracy: 90.60%\n",
      "Iteration:   544, Loss: 0.03005835, Accuracy: 90.60%\n",
      "Iteration:   545, Loss: 0.03002336, Accuracy: 90.60%\n",
      "Iteration:   546, Loss: 0.02998818, Accuracy: 90.60%\n",
      "Iteration:   547, Loss: 0.02995232, Accuracy: 90.60%\n",
      "Iteration:   548, Loss: 0.02991604, Accuracy: 90.60%\n",
      "Iteration:   549, Loss: 0.02988065, Accuracy: 90.60%\n",
      "Iteration:   550, Loss: 0.02984633, Accuracy: 90.60%\n",
      "Iteration:   551, Loss: 0.02981232, Accuracy: 90.60%\n",
      "Iteration:   552, Loss: 0.02977808, Accuracy: 90.60%\n",
      "Iteration:   553, Loss: 0.02974407, Accuracy: 90.60%\n",
      "Iteration:   554, Loss: 0.02970915, Accuracy: 90.60%\n",
      "Iteration:   555, Loss: 0.02967506, Accuracy: 90.60%\n",
      "Iteration:   556, Loss: 0.02964118, Accuracy: 90.60%\n",
      "Iteration:   557, Loss: 0.02960750, Accuracy: 90.60%\n",
      "Iteration:   558, Loss: 0.02957454, Accuracy: 90.60%\n",
      "Iteration:   559, Loss: 0.02954113, Accuracy: 90.60%\n",
      "Iteration:   560, Loss: 0.02950790, Accuracy: 90.60%\n",
      "Iteration:   561, Loss: 0.02947485, Accuracy: 90.60%\n",
      "Iteration:   562, Loss: 0.02944177, Accuracy: 90.60%\n",
      "Iteration:   563, Loss: 0.02940724, Accuracy: 90.60%\n",
      "Iteration:   564, Loss: 0.02937327, Accuracy: 90.60%\n",
      "Iteration:   565, Loss: 0.02934051, Accuracy: 90.60%\n",
      "Iteration:   566, Loss: 0.02930668, Accuracy: 90.60%\n",
      "Iteration:   567, Loss: 0.02927249, Accuracy: 90.60%\n",
      "Iteration:   568, Loss: 0.02923896, Accuracy: 90.50%\n",
      "Iteration:   569, Loss: 0.02920651, Accuracy: 90.50%\n",
      "Iteration:   570, Loss: 0.02917283, Accuracy: 90.50%\n",
      "Iteration:   571, Loss: 0.02914617, Accuracy: 90.50%\n",
      "Iteration:   572, Loss: 0.02910995, Accuracy: 90.50%\n",
      "Iteration:   573, Loss: 0.02909199, Accuracy: 90.60%\n",
      "Iteration:   574, Loss: 0.02910841, Accuracy: 90.40%\n",
      "Iteration:   575, Loss: 0.02909933, Accuracy: 90.70%\n",
      "Iteration:   576, Loss: 0.02900617, Accuracy: 90.40%\n",
      "Iteration:   577, Loss: 0.02895579, Accuracy: 90.50%\n",
      "Iteration:   578, Loss: 0.02891919, Accuracy: 90.50%\n",
      "Iteration:   579, Loss: 0.02888643, Accuracy: 90.50%\n",
      "Iteration:   580, Loss: 0.02884688, Accuracy: 90.50%\n",
      "Iteration:   581, Loss: 0.02897533, Accuracy: 90.50%\n",
      "Iteration:   582, Loss: 0.02963068, Accuracy: 90.60%\n",
      "Iteration:   583, Loss: 0.02944976, Accuracy: 90.60%\n",
      "Iteration:   584, Loss: 0.02902913, Accuracy: 90.70%\n",
      "Iteration:   585, Loss: 0.02980587, Accuracy: 90.50%\n",
      "Iteration:   586, Loss: 0.02885120, Accuracy: 90.70%\n",
      "Iteration:   587, Loss: 0.02874597, Accuracy: 90.60%\n",
      "Iteration:   588, Loss: 0.02872483, Accuracy: 90.60%\n",
      "Iteration:   589, Loss: 0.02898708, Accuracy: 90.50%\n",
      "Iteration:   590, Loss: 0.02879112, Accuracy: 90.50%\n",
      "Iteration:   591, Loss: 0.02854704, Accuracy: 90.50%\n",
      "Iteration:   592, Loss: 0.02851300, Accuracy: 90.50%\n",
      "Iteration:   593, Loss: 0.02848411, Accuracy: 90.60%\n",
      "Iteration:   594, Loss: 0.02845211, Accuracy: 90.40%\n",
      "Iteration:   595, Loss: 0.02850298, Accuracy: 90.60%\n",
      "Iteration:   596, Loss: 0.02838203, Accuracy: 90.70%\n",
      "Iteration:   597, Loss: 0.02834203, Accuracy: 90.40%\n",
      "Iteration:   598, Loss: 0.02831899, Accuracy: 90.70%\n",
      "Iteration:   599, Loss: 0.02830615, Accuracy: 90.50%\n",
      "Iteration:   600, Loss: 0.02865609, Accuracy: 90.80%\n",
      "Iteration:   601, Loss: 0.02920654, Accuracy: 90.50%\n",
      "Iteration:   602, Loss: 0.02905567, Accuracy: 90.50%\n",
      "Iteration:   603, Loss: 0.02878543, Accuracy: 90.60%\n",
      "Iteration:   604, Loss: 0.02818819, Accuracy: 90.80%\n",
      "Iteration:   605, Loss: 0.02814118, Accuracy: 90.80%\n",
      "Iteration:   606, Loss: 0.02811223, Accuracy: 90.80%\n",
      "Iteration:   607, Loss: 0.02806074, Accuracy: 90.80%\n",
      "Iteration:   608, Loss: 0.02802787, Accuracy: 90.80%\n",
      "Iteration:   609, Loss: 0.02799815, Accuracy: 90.80%\n",
      "Iteration:   610, Loss: 0.02797456, Accuracy: 90.80%\n",
      "Iteration:   611, Loss: 0.02793058, Accuracy: 90.80%\n",
      "Iteration:   612, Loss: 0.02789663, Accuracy: 90.80%\n",
      "Iteration:   613, Loss: 0.02786748, Accuracy: 90.80%\n",
      "Iteration:   614, Loss: 0.02783783, Accuracy: 90.80%\n",
      "Iteration:   615, Loss: 0.02784736, Accuracy: 90.80%\n",
      "Iteration:   616, Loss: 0.02776575, Accuracy: 90.80%\n",
      "Iteration:   617, Loss: 0.02772284, Accuracy: 90.80%\n",
      "Iteration:   618, Loss: 0.02768921, Accuracy: 90.80%\n",
      "Iteration:   619, Loss: 0.02765651, Accuracy: 90.80%\n",
      "Iteration:   620, Loss: 0.02761878, Accuracy: 90.80%\n",
      "Iteration:   621, Loss: 0.02758698, Accuracy: 90.80%\n",
      "Iteration:   622, Loss: 0.02756308, Accuracy: 90.80%\n",
      "Iteration:   623, Loss: 0.02752466, Accuracy: 90.80%\n",
      "Iteration:   624, Loss: 0.02749459, Accuracy: 90.80%\n",
      "Iteration:   625, Loss: 0.02746506, Accuracy: 90.80%\n",
      "Iteration:   626, Loss: 0.02743583, Accuracy: 90.80%\n",
      "Iteration:   627, Loss: 0.02740684, Accuracy: 90.80%\n",
      "Iteration:   628, Loss: 0.02737808, Accuracy: 90.80%\n",
      "Iteration:   629, Loss: 0.02734879, Accuracy: 90.90%\n",
      "Iteration:   630, Loss: 0.02731815, Accuracy: 90.90%\n",
      "Iteration:   631, Loss: 0.02729307, Accuracy: 90.90%\n",
      "Iteration:   632, Loss: 0.02726601, Accuracy: 90.90%\n",
      "Iteration:   633, Loss: 0.02722588, Accuracy: 91.00%\n",
      "Iteration:   634, Loss: 0.02719038, Accuracy: 91.00%\n",
      "Iteration:   635, Loss: 0.02716095, Accuracy: 91.00%\n",
      "Iteration:   636, Loss: 0.02713164, Accuracy: 91.00%\n",
      "Iteration:   637, Loss: 0.02710288, Accuracy: 91.00%\n",
      "Iteration:   638, Loss: 0.02707289, Accuracy: 91.00%\n",
      "Iteration:   639, Loss: 0.02704704, Accuracy: 91.00%\n",
      "Iteration:   640, Loss: 0.02701983, Accuracy: 91.00%\n",
      "Iteration:   641, Loss: 0.02698788, Accuracy: 91.00%\n",
      "Iteration:   642, Loss: 0.02695681, Accuracy: 91.00%\n",
      "Iteration:   643, Loss: 0.02692830, Accuracy: 91.00%\n",
      "Iteration:   644, Loss: 0.02689960, Accuracy: 91.00%\n",
      "Iteration:   645, Loss: 0.02686910, Accuracy: 91.00%\n",
      "Iteration:   646, Loss: 0.02686105, Accuracy: 91.00%\n",
      "Iteration:   647, Loss: 0.02730518, Accuracy: 90.80%\n",
      "Iteration:   648, Loss: 0.02713170, Accuracy: 90.90%\n",
      "Iteration:   649, Loss: 0.02703186, Accuracy: 90.90%\n",
      "Iteration:   650, Loss: 0.02688724, Accuracy: 90.90%\n",
      "Iteration:   651, Loss: 0.02678384, Accuracy: 90.90%\n",
      "Iteration:   652, Loss: 0.02673715, Accuracy: 90.90%\n",
      "Iteration:   653, Loss: 0.02668678, Accuracy: 90.90%\n",
      "Iteration:   654, Loss: 0.02666753, Accuracy: 90.90%\n",
      "Iteration:   655, Loss: 0.02662422, Accuracy: 90.90%\n",
      "Iteration:   656, Loss: 0.02660574, Accuracy: 90.90%\n",
      "Iteration:   657, Loss: 0.02655258, Accuracy: 90.90%\n",
      "Iteration:   658, Loss: 0.02652050, Accuracy: 90.90%\n",
      "Iteration:   659, Loss: 0.02650773, Accuracy: 90.90%\n",
      "Iteration:   660, Loss: 0.02648549, Accuracy: 90.90%\n",
      "Iteration:   661, Loss: 0.02644450, Accuracy: 90.90%\n",
      "Iteration:   662, Loss: 0.02640572, Accuracy: 90.90%\n",
      "Iteration:   663, Loss: 0.02637774, Accuracy: 90.90%\n",
      "Iteration:   664, Loss: 0.02634887, Accuracy: 90.90%\n",
      "Iteration:   665, Loss: 0.02632172, Accuracy: 90.90%\n",
      "Iteration:   666, Loss: 0.02629430, Accuracy: 90.90%\n",
      "Iteration:   667, Loss: 0.02626586, Accuracy: 90.90%\n",
      "Iteration:   668, Loss: 0.02623580, Accuracy: 90.90%\n",
      "Iteration:   669, Loss: 0.02620919, Accuracy: 90.90%\n",
      "Iteration:   670, Loss: 0.02618347, Accuracy: 90.90%\n",
      "Iteration:   671, Loss: 0.02615576, Accuracy: 90.90%\n",
      "Iteration:   672, Loss: 0.02612864, Accuracy: 90.90%\n",
      "Iteration:   673, Loss: 0.02610143, Accuracy: 90.90%\n",
      "Iteration:   674, Loss: 0.02607440, Accuracy: 90.90%\n",
      "Iteration:   675, Loss: 0.02604887, Accuracy: 90.90%\n",
      "Iteration:   676, Loss: 0.02602031, Accuracy: 90.90%\n",
      "Iteration:   677, Loss: 0.02599213, Accuracy: 90.90%\n",
      "Iteration:   678, Loss: 0.02596521, Accuracy: 90.90%\n",
      "Iteration:   679, Loss: 0.02593831, Accuracy: 90.90%\n",
      "Iteration:   680, Loss: 0.02591108, Accuracy: 90.90%\n",
      "Iteration:   681, Loss: 0.02588250, Accuracy: 90.90%\n",
      "Iteration:   682, Loss: 0.02584940, Accuracy: 90.80%\n",
      "Iteration:   683, Loss: 0.02582935, Accuracy: 90.80%\n",
      "Iteration:   684, Loss: 0.02580334, Accuracy: 90.80%\n",
      "Iteration:   685, Loss: 0.02596637, Accuracy: 90.90%\n",
      "Iteration:   686, Loss: 0.02589025, Accuracy: 90.80%\n",
      "Iteration:   687, Loss: 0.02574395, Accuracy: 91.00%\n",
      "Iteration:   688, Loss: 0.02573160, Accuracy: 90.90%\n",
      "Iteration:   689, Loss: 0.02564656, Accuracy: 90.90%\n",
      "Iteration:   690, Loss: 0.02561917, Accuracy: 91.00%\n",
      "Iteration:   691, Loss: 0.02559305, Accuracy: 90.90%\n",
      "Iteration:   692, Loss: 0.02556657, Accuracy: 90.90%\n",
      "Iteration:   693, Loss: 0.02554049, Accuracy: 90.90%\n",
      "Iteration:   694, Loss: 0.02551415, Accuracy: 90.80%\n",
      "Iteration:   695, Loss: 0.02548779, Accuracy: 90.80%\n",
      "Iteration:   696, Loss: 0.02545948, Accuracy: 90.80%\n",
      "Iteration:   697, Loss: 0.02543314, Accuracy: 90.80%\n",
      "Iteration:   698, Loss: 0.02540788, Accuracy: 90.80%\n",
      "Iteration:   699, Loss: 0.02538285, Accuracy: 90.80%\n",
      "Iteration:   700, Loss: 0.02535779, Accuracy: 90.80%\n",
      "Iteration:   701, Loss: 0.02533309, Accuracy: 90.80%\n",
      "Iteration:   702, Loss: 0.02530819, Accuracy: 90.80%\n",
      "Iteration:   703, Loss: 0.02528332, Accuracy: 90.80%\n",
      "Iteration:   704, Loss: 0.02525795, Accuracy: 90.80%\n",
      "Iteration:   705, Loss: 0.02523094, Accuracy: 90.80%\n",
      "Iteration:   706, Loss: 0.02520581, Accuracy: 90.80%\n",
      "Iteration:   707, Loss: 0.02518113, Accuracy: 90.80%\n",
      "Iteration:   708, Loss: 0.02515655, Accuracy: 90.80%\n",
      "Iteration:   709, Loss: 0.02513221, Accuracy: 90.80%\n",
      "Iteration:   710, Loss: 0.02510802, Accuracy: 90.80%\n",
      "Iteration:   711, Loss: 0.02508392, Accuracy: 90.80%\n",
      "Iteration:   712, Loss: 0.02505970, Accuracy: 90.80%\n",
      "Iteration:   713, Loss: 0.02503551, Accuracy: 90.80%\n",
      "Iteration:   714, Loss: 0.02501095, Accuracy: 90.80%\n",
      "Iteration:   715, Loss: 0.02498594, Accuracy: 90.80%\n",
      "Iteration:   716, Loss: 0.02495947, Accuracy: 90.90%\n",
      "Iteration:   717, Loss: 0.02493500, Accuracy: 90.80%\n",
      "Iteration:   718, Loss: 0.02491100, Accuracy: 90.80%\n",
      "Iteration:   719, Loss: 0.02488713, Accuracy: 90.80%\n",
      "Iteration:   720, Loss: 0.02486312, Accuracy: 90.80%\n",
      "Iteration:   721, Loss: 0.02483937, Accuracy: 90.80%\n",
      "Iteration:   722, Loss: 0.02481583, Accuracy: 90.80%\n",
      "Iteration:   723, Loss: 0.02479234, Accuracy: 90.80%\n",
      "Iteration:   724, Loss: 0.02476891, Accuracy: 90.80%\n",
      "Iteration:   725, Loss: 0.02474552, Accuracy: 90.80%\n",
      "Iteration:   726, Loss: 0.02472212, Accuracy: 90.80%\n",
      "Iteration:   727, Loss: 0.02469871, Accuracy: 90.80%\n",
      "Iteration:   728, Loss: 0.02467430, Accuracy: 90.80%\n",
      "Iteration:   729, Loss: 0.02464143, Accuracy: 90.80%\n",
      "Iteration:   730, Loss: 0.02471339, Accuracy: 90.70%\n",
      "Iteration:   731, Loss: 0.02460495, Accuracy: 90.80%\n",
      "Iteration:   732, Loss: 0.02456398, Accuracy: 90.80%\n",
      "Iteration:   733, Loss: 0.02454059, Accuracy: 90.80%\n",
      "Iteration:   734, Loss: 0.02451750, Accuracy: 90.80%\n",
      "Iteration:   735, Loss: 0.02449429, Accuracy: 90.80%\n",
      "Iteration:   736, Loss: 0.02447100, Accuracy: 90.80%\n",
      "Iteration:   737, Loss: 0.02444629, Accuracy: 90.80%\n",
      "Iteration:   738, Loss: 0.02442221, Accuracy: 90.80%\n",
      "Iteration:   739, Loss: 0.02439889, Accuracy: 90.80%\n",
      "Iteration:   740, Loss: 0.02437594, Accuracy: 90.80%\n",
      "Iteration:   741, Loss: 0.02435269, Accuracy: 90.80%\n",
      "Iteration:   742, Loss: 0.02432962, Accuracy: 90.90%\n",
      "Iteration:   743, Loss: 0.02430677, Accuracy: 90.90%\n",
      "Iteration:   744, Loss: 0.02428416, Accuracy: 90.90%\n",
      "Iteration:   745, Loss: 0.02426157, Accuracy: 90.90%\n",
      "Iteration:   746, Loss: 0.02423921, Accuracy: 90.90%\n",
      "Iteration:   747, Loss: 0.02421661, Accuracy: 90.90%\n",
      "Iteration:   748, Loss: 0.02419427, Accuracy: 90.90%\n",
      "Iteration:   749, Loss: 0.02417203, Accuracy: 90.90%\n",
      "Iteration:   750, Loss: 0.02414990, Accuracy: 90.90%\n",
      "Iteration:   751, Loss: 0.02412785, Accuracy: 90.90%\n",
      "Iteration:   752, Loss: 0.02410596, Accuracy: 90.90%\n",
      "Iteration:   753, Loss: 0.02408384, Accuracy: 90.90%\n",
      "Iteration:   754, Loss: 0.02406194, Accuracy: 91.00%\n",
      "Iteration:   755, Loss: 0.02404009, Accuracy: 91.00%\n",
      "Iteration:   756, Loss: 0.02401838, Accuracy: 91.00%\n",
      "Iteration:   757, Loss: 0.02399637, Accuracy: 91.00%\n",
      "Iteration:   758, Loss: 0.02397455, Accuracy: 91.00%\n",
      "Iteration:   759, Loss: 0.02395269, Accuracy: 91.00%\n",
      "Iteration:   760, Loss: 0.02393077, Accuracy: 91.00%\n",
      "Iteration:   761, Loss: 0.02390889, Accuracy: 91.00%\n",
      "Iteration:   762, Loss: 0.02388723, Accuracy: 91.00%\n",
      "Iteration:   763, Loss: 0.02386551, Accuracy: 91.00%\n",
      "Iteration:   764, Loss: 0.02384405, Accuracy: 91.00%\n",
      "Iteration:   765, Loss: 0.02382245, Accuracy: 91.00%\n",
      "Iteration:   766, Loss: 0.02380100, Accuracy: 91.00%\n",
      "Iteration:   767, Loss: 0.02377944, Accuracy: 91.00%\n",
      "Iteration:   768, Loss: 0.02375752, Accuracy: 91.00%\n",
      "Iteration:   769, Loss: 0.02373022, Accuracy: 91.00%\n",
      "Iteration:   770, Loss: 0.02371218, Accuracy: 91.00%\n",
      "Iteration:   771, Loss: 0.02368378, Accuracy: 91.00%\n",
      "Iteration:   772, Loss: 0.02366042, Accuracy: 91.00%\n",
      "Iteration:   773, Loss: 0.02363601, Accuracy: 91.00%\n",
      "Iteration:   774, Loss: 0.02361629, Accuracy: 91.00%\n",
      "Iteration:   775, Loss: 0.02359244, Accuracy: 91.00%\n",
      "Iteration:   776, Loss: 0.02357120, Accuracy: 91.00%\n",
      "Iteration:   777, Loss: 0.02355011, Accuracy: 91.00%\n",
      "Iteration:   778, Loss: 0.02352903, Accuracy: 91.00%\n",
      "Iteration:   779, Loss: 0.02350786, Accuracy: 91.00%\n",
      "Iteration:   780, Loss: 0.02348620, Accuracy: 91.00%\n",
      "Iteration:   781, Loss: 0.02346422, Accuracy: 91.00%\n",
      "Iteration:   782, Loss: 0.02344307, Accuracy: 91.00%\n",
      "Iteration:   783, Loss: 0.02342207, Accuracy: 91.00%\n",
      "Iteration:   784, Loss: 0.02340118, Accuracy: 91.10%\n",
      "Iteration:   785, Loss: 0.02338034, Accuracy: 91.10%\n",
      "Iteration:   786, Loss: 0.02335941, Accuracy: 91.10%\n",
      "Iteration:   787, Loss: 0.02333827, Accuracy: 91.10%\n",
      "Iteration:   788, Loss: 0.02331660, Accuracy: 91.10%\n",
      "Iteration:   789, Loss: 0.02329544, Accuracy: 91.10%\n",
      "Iteration:   790, Loss: 0.02327431, Accuracy: 91.10%\n",
      "Iteration:   791, Loss: 0.02325374, Accuracy: 91.10%\n",
      "Iteration:   792, Loss: 0.02323311, Accuracy: 91.10%\n",
      "Iteration:   793, Loss: 0.02321278, Accuracy: 91.10%\n",
      "Iteration:   794, Loss: 0.02319195, Accuracy: 91.10%\n",
      "Iteration:   795, Loss: 0.02317127, Accuracy: 91.10%\n",
      "Iteration:   796, Loss: 0.02315036, Accuracy: 91.10%\n",
      "Iteration:   797, Loss: 0.02313004, Accuracy: 91.10%\n",
      "Iteration:   798, Loss: 0.02310933, Accuracy: 91.10%\n",
      "Iteration:   799, Loss: 0.02308899, Accuracy: 91.10%\n",
      "Iteration:   800, Loss: 0.02306847, Accuracy: 91.10%\n",
      "Iteration:   801, Loss: 0.02304818, Accuracy: 91.10%\n",
      "Iteration:   802, Loss: 0.02302765, Accuracy: 91.10%\n",
      "Iteration:   803, Loss: 0.02300757, Accuracy: 91.10%\n",
      "Iteration:   804, Loss: 0.02298698, Accuracy: 91.10%\n",
      "Iteration:   805, Loss: 0.02296687, Accuracy: 91.10%\n",
      "Iteration:   806, Loss: 0.02294667, Accuracy: 91.10%\n",
      "Iteration:   807, Loss: 0.02292657, Accuracy: 91.10%\n",
      "Iteration:   808, Loss: 0.02290675, Accuracy: 91.00%\n",
      "Iteration:   809, Loss: 0.02288682, Accuracy: 91.10%\n",
      "Iteration:   810, Loss: 0.02286609, Accuracy: 91.10%\n",
      "Iteration:   811, Loss: 0.02284630, Accuracy: 91.10%\n",
      "Iteration:   812, Loss: 0.02282559, Accuracy: 91.10%\n",
      "Iteration:   813, Loss: 0.02280511, Accuracy: 91.10%\n",
      "Iteration:   814, Loss: 0.02278382, Accuracy: 91.10%\n",
      "Iteration:   815, Loss: 0.02276291, Accuracy: 91.10%\n",
      "Iteration:   816, Loss: 0.02274221, Accuracy: 91.10%\n",
      "Iteration:   817, Loss: 0.02272310, Accuracy: 91.10%\n",
      "Iteration:   818, Loss: 0.02270087, Accuracy: 91.10%\n",
      "Iteration:   819, Loss: 0.02268059, Accuracy: 91.10%\n",
      "Iteration:   820, Loss: 0.02266040, Accuracy: 91.10%\n",
      "Iteration:   821, Loss: 0.02264078, Accuracy: 91.10%\n",
      "Iteration:   822, Loss: 0.02262098, Accuracy: 91.10%\n",
      "Iteration:   823, Loss: 0.02260169, Accuracy: 91.10%\n",
      "Iteration:   824, Loss: 0.02258269, Accuracy: 91.10%\n",
      "Iteration:   825, Loss: 0.02256276, Accuracy: 91.10%\n",
      "Iteration:   826, Loss: 0.02254313, Accuracy: 91.10%\n",
      "Iteration:   827, Loss: 0.02252406, Accuracy: 91.10%\n",
      "Iteration:   828, Loss: 0.02250464, Accuracy: 91.10%\n",
      "Iteration:   829, Loss: 0.02248569, Accuracy: 91.10%\n",
      "Iteration:   830, Loss: 0.02246682, Accuracy: 91.10%\n",
      "Iteration:   831, Loss: 0.02244611, Accuracy: 91.10%\n",
      "Iteration:   832, Loss: 0.02242475, Accuracy: 91.10%\n",
      "Iteration:   833, Loss: 0.02240496, Accuracy: 91.10%\n",
      "Iteration:   834, Loss: 0.02238533, Accuracy: 91.10%\n",
      "Iteration:   835, Loss: 0.02236625, Accuracy: 91.10%\n",
      "Iteration:   836, Loss: 0.02234728, Accuracy: 91.10%\n",
      "Iteration:   837, Loss: 0.02232740, Accuracy: 91.10%\n",
      "Iteration:   838, Loss: 0.02230799, Accuracy: 91.10%\n",
      "Iteration:   839, Loss: 0.02228911, Accuracy: 91.10%\n",
      "Iteration:   840, Loss: 0.02227018, Accuracy: 91.10%\n",
      "Iteration:   841, Loss: 0.02225181, Accuracy: 91.10%\n",
      "Iteration:   842, Loss: 0.02223508, Accuracy: 91.10%\n",
      "Iteration:   843, Loss: 0.02222091, Accuracy: 91.00%\n",
      "Iteration:   844, Loss: 0.02225215, Accuracy: 91.10%\n",
      "Iteration:   845, Loss: 0.02256431, Accuracy: 90.90%\n",
      "Iteration:   846, Loss: 0.02216124, Accuracy: 91.00%\n",
      "Iteration:   847, Loss: 0.02214444, Accuracy: 91.10%\n",
      "Iteration:   848, Loss: 0.02214896, Accuracy: 91.00%\n",
      "Iteration:   849, Loss: 0.02213345, Accuracy: 91.10%\n",
      "Iteration:   850, Loss: 0.02212059, Accuracy: 90.90%\n",
      "Iteration:   851, Loss: 0.02206879, Accuracy: 91.00%\n",
      "Iteration:   852, Loss: 0.02204786, Accuracy: 91.00%\n",
      "Iteration:   853, Loss: 0.02202787, Accuracy: 91.00%\n",
      "Iteration:   854, Loss: 0.02200824, Accuracy: 91.00%\n",
      "Iteration:   855, Loss: 0.02198890, Accuracy: 91.00%\n",
      "Iteration:   856, Loss: 0.02196996, Accuracy: 91.00%\n",
      "Iteration:   857, Loss: 0.02195065, Accuracy: 91.00%\n",
      "Iteration:   858, Loss: 0.02193239, Accuracy: 91.00%\n",
      "Iteration:   859, Loss: 0.02191503, Accuracy: 91.10%\n",
      "Iteration:   860, Loss: 0.02189604, Accuracy: 91.00%\n",
      "Iteration:   861, Loss: 0.02187555, Accuracy: 91.10%\n",
      "Iteration:   862, Loss: 0.02185194, Accuracy: 91.10%\n",
      "Iteration:   863, Loss: 0.02183416, Accuracy: 91.10%\n",
      "Iteration:   864, Loss: 0.02182239, Accuracy: 91.00%\n",
      "Iteration:   865, Loss: 0.02180649, Accuracy: 91.10%\n",
      "Iteration:   866, Loss: 0.02180137, Accuracy: 91.00%\n",
      "Iteration:   867, Loss: 0.02181622, Accuracy: 91.10%\n",
      "Iteration:   868, Loss: 0.02209287, Accuracy: 90.90%\n",
      "Iteration:   869, Loss: 0.02175268, Accuracy: 91.10%\n",
      "Iteration:   870, Loss: 0.02171203, Accuracy: 91.10%\n",
      "Iteration:   871, Loss: 0.02169391, Accuracy: 91.10%\n",
      "Iteration:   872, Loss: 0.02168718, Accuracy: 91.10%\n",
      "Iteration:   873, Loss: 0.02165870, Accuracy: 91.10%\n",
      "Iteration:   874, Loss: 0.02164976, Accuracy: 91.10%\n",
      "Iteration:   875, Loss: 0.02161704, Accuracy: 91.10%\n",
      "Iteration:   876, Loss: 0.02159899, Accuracy: 91.10%\n",
      "Iteration:   877, Loss: 0.02158056, Accuracy: 91.10%\n",
      "Iteration:   878, Loss: 0.02156302, Accuracy: 91.10%\n",
      "Iteration:   879, Loss: 0.02154496, Accuracy: 91.10%\n",
      "Iteration:   880, Loss: 0.02152832, Accuracy: 91.10%\n",
      "Iteration:   881, Loss: 0.02150932, Accuracy: 91.10%\n",
      "Iteration:   882, Loss: 0.02149178, Accuracy: 91.10%\n",
      "Iteration:   883, Loss: 0.02147353, Accuracy: 91.10%\n",
      "Iteration:   884, Loss: 0.02145581, Accuracy: 91.10%\n",
      "Iteration:   885, Loss: 0.02143781, Accuracy: 91.10%\n",
      "Iteration:   886, Loss: 0.02142063, Accuracy: 91.10%\n",
      "Iteration:   887, Loss: 0.02140322, Accuracy: 91.10%\n",
      "Iteration:   888, Loss: 0.02138701, Accuracy: 91.10%\n",
      "Iteration:   889, Loss: 0.02136888, Accuracy: 91.10%\n",
      "Iteration:   890, Loss: 0.02135199, Accuracy: 91.20%\n",
      "Iteration:   891, Loss: 0.02133475, Accuracy: 91.20%\n",
      "Iteration:   892, Loss: 0.02131792, Accuracy: 91.20%\n",
      "Iteration:   893, Loss: 0.02130080, Accuracy: 91.10%\n",
      "Iteration:   894, Loss: 0.02128362, Accuracy: 91.20%\n",
      "Iteration:   895, Loss: 0.02126451, Accuracy: 91.10%\n",
      "Iteration:   896, Loss: 0.02123727, Accuracy: 91.20%\n",
      "Iteration:   897, Loss: 0.02121944, Accuracy: 91.10%\n",
      "Iteration:   898, Loss: 0.02119805, Accuracy: 91.20%\n",
      "Iteration:   899, Loss: 0.02117934, Accuracy: 91.20%\n",
      "Iteration:   900, Loss: 0.02116137, Accuracy: 91.20%\n",
      "Iteration:   901, Loss: 0.02114320, Accuracy: 91.20%\n",
      "Iteration:   902, Loss: 0.02112612, Accuracy: 91.20%\n",
      "Iteration:   903, Loss: 0.02110958, Accuracy: 91.20%\n",
      "Iteration:   904, Loss: 0.02109142, Accuracy: 91.20%\n",
      "Iteration:   905, Loss: 0.02107399, Accuracy: 91.20%\n",
      "Iteration:   906, Loss: 0.02105708, Accuracy: 91.20%\n",
      "Iteration:   907, Loss: 0.02104050, Accuracy: 91.20%\n",
      "Iteration:   908, Loss: 0.02102268, Accuracy: 91.10%\n",
      "Iteration:   909, Loss: 0.02100493, Accuracy: 91.10%\n",
      "Iteration:   910, Loss: 0.02098693, Accuracy: 91.10%\n",
      "Iteration:   911, Loss: 0.02096885, Accuracy: 91.10%\n",
      "Iteration:   912, Loss: 0.02095004, Accuracy: 91.10%\n",
      "Iteration:   913, Loss: 0.02093247, Accuracy: 91.10%\n",
      "Iteration:   914, Loss: 0.02090342, Accuracy: 91.10%\n",
      "Iteration:   915, Loss: 0.02088367, Accuracy: 91.10%\n",
      "Iteration:   916, Loss: 0.02086872, Accuracy: 91.10%\n",
      "Iteration:   917, Loss: 0.02091113, Accuracy: 91.10%\n",
      "Iteration:   918, Loss: 0.02112094, Accuracy: 91.10%\n",
      "Iteration:   919, Loss: 0.02091271, Accuracy: 91.00%\n",
      "Iteration:   920, Loss: 0.02089150, Accuracy: 91.00%\n",
      "Iteration:   921, Loss: 0.02085599, Accuracy: 91.00%\n",
      "Iteration:   922, Loss: 0.02084154, Accuracy: 91.00%\n",
      "Iteration:   923, Loss: 0.02078667, Accuracy: 91.00%\n",
      "Iteration:   924, Loss: 0.02086971, Accuracy: 91.00%\n",
      "Iteration:   925, Loss: 0.02085503, Accuracy: 91.00%\n",
      "Iteration:   926, Loss: 0.02106052, Accuracy: 90.90%\n",
      "Iteration:   927, Loss: 0.02107447, Accuracy: 91.00%\n",
      "Iteration:   928, Loss: 0.02097915, Accuracy: 91.00%\n",
      "Iteration:   929, Loss: 0.02091725, Accuracy: 91.00%\n",
      "Iteration:   930, Loss: 0.02081557, Accuracy: 91.00%\n",
      "Iteration:   931, Loss: 0.02075620, Accuracy: 91.00%\n",
      "Iteration:   932, Loss: 0.02063707, Accuracy: 91.00%\n",
      "Iteration:   933, Loss: 0.02062876, Accuracy: 91.00%\n",
      "Iteration:   934, Loss: 0.02060996, Accuracy: 91.00%\n",
      "Iteration:   935, Loss: 0.02059197, Accuracy: 91.00%\n",
      "Iteration:   936, Loss: 0.02057202, Accuracy: 91.00%\n",
      "Iteration:   937, Loss: 0.02055317, Accuracy: 91.00%\n",
      "Iteration:   938, Loss: 0.02053583, Accuracy: 91.00%\n",
      "Iteration:   939, Loss: 0.02051866, Accuracy: 91.00%\n",
      "Iteration:   940, Loss: 0.02049829, Accuracy: 91.00%\n",
      "Iteration:   941, Loss: 0.02048153, Accuracy: 91.00%\n",
      "Iteration:   942, Loss: 0.02046527, Accuracy: 91.00%\n",
      "Iteration:   943, Loss: 0.02044891, Accuracy: 91.00%\n",
      "Iteration:   944, Loss: 0.02043281, Accuracy: 91.00%\n",
      "Iteration:   945, Loss: 0.02041647, Accuracy: 91.00%\n",
      "Iteration:   946, Loss: 0.02040013, Accuracy: 91.00%\n",
      "Iteration:   947, Loss: 0.02038319, Accuracy: 91.00%\n",
      "Iteration:   948, Loss: 0.02036563, Accuracy: 91.00%\n",
      "Iteration:   949, Loss: 0.02034495, Accuracy: 91.00%\n",
      "Iteration:   950, Loss: 0.02032857, Accuracy: 91.00%\n",
      "Iteration:   951, Loss: 0.02031227, Accuracy: 91.00%\n",
      "Iteration:   952, Loss: 0.02029626, Accuracy: 91.00%\n",
      "Iteration:   953, Loss: 0.02028006, Accuracy: 91.00%\n",
      "Iteration:   954, Loss: 0.02026395, Accuracy: 91.00%\n",
      "Iteration:   955, Loss: 0.02024745, Accuracy: 91.00%\n",
      "Iteration:   956, Loss: 0.02023119, Accuracy: 91.00%\n",
      "Iteration:   957, Loss: 0.02021494, Accuracy: 91.00%\n",
      "Iteration:   958, Loss: 0.02019899, Accuracy: 91.00%\n",
      "Iteration:   959, Loss: 0.02018280, Accuracy: 91.00%\n",
      "Iteration:   960, Loss: 0.02016605, Accuracy: 91.00%\n",
      "Iteration:   961, Loss: 0.02014019, Accuracy: 91.00%\n",
      "Iteration:   962, Loss: 0.02012120, Accuracy: 91.00%\n",
      "Iteration:   963, Loss: 0.02010473, Accuracy: 91.00%\n",
      "Iteration:   964, Loss: 0.02008842, Accuracy: 91.00%\n",
      "Iteration:   965, Loss: 0.02007207, Accuracy: 91.00%\n",
      "Iteration:   966, Loss: 0.02005580, Accuracy: 91.00%\n",
      "Iteration:   967, Loss: 0.02003956, Accuracy: 91.00%\n",
      "Iteration:   968, Loss: 0.02002380, Accuracy: 91.00%\n",
      "Iteration:   969, Loss: 0.02000811, Accuracy: 91.00%\n",
      "Iteration:   970, Loss: 0.01999270, Accuracy: 91.00%\n",
      "Iteration:   971, Loss: 0.01997702, Accuracy: 91.00%\n",
      "Iteration:   972, Loss: 0.01996163, Accuracy: 91.00%\n",
      "Iteration:   973, Loss: 0.01994632, Accuracy: 91.00%\n",
      "Iteration:   974, Loss: 0.01993114, Accuracy: 91.00%\n",
      "Iteration:   975, Loss: 0.01991570, Accuracy: 91.00%\n",
      "Iteration:   976, Loss: 0.01990053, Accuracy: 91.00%\n",
      "Iteration:   977, Loss: 0.01988523, Accuracy: 91.00%\n",
      "Iteration:   978, Loss: 0.01987013, Accuracy: 91.00%\n",
      "Iteration:   979, Loss: 0.01985457, Accuracy: 91.00%\n",
      "Iteration:   980, Loss: 0.01983923, Accuracy: 91.00%\n",
      "Iteration:   981, Loss: 0.01982392, Accuracy: 91.00%\n",
      "Iteration:   982, Loss: 0.01980878, Accuracy: 91.00%\n",
      "Iteration:   983, Loss: 0.01979349, Accuracy: 91.00%\n",
      "Iteration:   984, Loss: 0.01977850, Accuracy: 91.00%\n",
      "Iteration:   985, Loss: 0.01976314, Accuracy: 91.00%\n",
      "Iteration:   986, Loss: 0.01974792, Accuracy: 91.00%\n",
      "Iteration:   987, Loss: 0.01973244, Accuracy: 91.00%\n",
      "Iteration:   988, Loss: 0.01971742, Accuracy: 91.00%\n",
      "Iteration:   989, Loss: 0.01970205, Accuracy: 91.00%\n",
      "Iteration:   990, Loss: 0.01968707, Accuracy: 91.00%\n",
      "Iteration:   991, Loss: 0.01967211, Accuracy: 91.00%\n",
      "Iteration:   992, Loss: 0.01965717, Accuracy: 91.00%\n",
      "Iteration:   993, Loss: 0.01964228, Accuracy: 91.00%\n",
      "Iteration:   994, Loss: 0.01962743, Accuracy: 91.00%\n",
      "Iteration:   995, Loss: 0.01961263, Accuracy: 91.00%\n",
      "Iteration:   996, Loss: 0.01959660, Accuracy: 91.00%\n",
      "Iteration:   997, Loss: 0.01958051, Accuracy: 91.00%\n",
      "Iteration:   998, Loss: 0.01956512, Accuracy: 91.00%\n",
      "Iteration:   999, Loss: 0.01955042, Accuracy: 91.00%\n",
      "Iteration:  1000, Loss: 0.01953826, Accuracy: 91.00%\n",
      "Iteration:  1001, Loss: 0.01953110, Accuracy: 91.00%\n",
      "Iteration:  1002, Loss: 0.01964225, Accuracy: 90.90%\n",
      "Iteration:  1003, Loss: 0.02011833, Accuracy: 91.00%\n",
      "Iteration:  1004, Loss: 0.02000509, Accuracy: 90.60%\n",
      "Iteration:  1005, Loss: 0.01977521, Accuracy: 90.70%\n",
      "Iteration:  1006, Loss: 0.01946159, Accuracy: 90.90%\n",
      "Iteration:  1007, Loss: 0.01956077, Accuracy: 90.90%\n",
      "Iteration:  1008, Loss: 0.01983900, Accuracy: 90.60%\n",
      "Iteration:  1009, Loss: 0.01979467, Accuracy: 90.60%\n",
      "Iteration:  1010, Loss: 0.01980926, Accuracy: 90.70%\n",
      "Iteration:  1011, Loss: 0.01976127, Accuracy: 90.70%\n",
      "Iteration:  1012, Loss: 0.01963452, Accuracy: 90.70%\n",
      "Iteration:  1013, Loss: 0.01983850, Accuracy: 91.00%\n",
      "Iteration:  1014, Loss: 0.01951979, Accuracy: 90.90%\n",
      "Iteration:  1015, Loss: 0.01945471, Accuracy: 90.90%\n",
      "Iteration:  1016, Loss: 0.01936268, Accuracy: 90.80%\n",
      "Iteration:  1017, Loss: 0.01929390, Accuracy: 90.90%\n",
      "Iteration:  1018, Loss: 0.01927776, Accuracy: 90.90%\n",
      "Iteration:  1019, Loss: 0.01924375, Accuracy: 90.90%\n",
      "Iteration:  1020, Loss: 0.01923321, Accuracy: 90.80%\n",
      "Iteration:  1021, Loss: 0.01920327, Accuracy: 90.80%\n",
      "Iteration:  1022, Loss: 0.01918572, Accuracy: 90.80%\n",
      "Iteration:  1023, Loss: 0.01916997, Accuracy: 90.80%\n",
      "Iteration:  1024, Loss: 0.01915499, Accuracy: 90.80%\n",
      "Iteration:  1025, Loss: 0.01913993, Accuracy: 90.80%\n",
      "Iteration:  1026, Loss: 0.01912495, Accuracy: 90.80%\n",
      "Iteration:  1027, Loss: 0.01910957, Accuracy: 90.80%\n",
      "Iteration:  1028, Loss: 0.01909521, Accuracy: 90.80%\n",
      "Iteration:  1029, Loss: 0.01908122, Accuracy: 90.80%\n",
      "Iteration:  1030, Loss: 0.01906682, Accuracy: 90.80%\n",
      "Iteration:  1031, Loss: 0.01905275, Accuracy: 90.80%\n",
      "Iteration:  1032, Loss: 0.01903843, Accuracy: 90.80%\n",
      "Iteration:  1033, Loss: 0.01902444, Accuracy: 90.80%\n",
      "Iteration:  1034, Loss: 0.01900981, Accuracy: 90.80%\n",
      "Iteration:  1035, Loss: 0.01899582, Accuracy: 90.80%\n",
      "Iteration:  1036, Loss: 0.01898206, Accuracy: 90.80%\n",
      "Iteration:  1037, Loss: 0.01896784, Accuracy: 90.80%\n",
      "Iteration:  1038, Loss: 0.01895399, Accuracy: 90.80%\n",
      "Iteration:  1039, Loss: 0.01893967, Accuracy: 90.80%\n",
      "Iteration:  1040, Loss: 0.01892411, Accuracy: 90.80%\n",
      "Iteration:  1041, Loss: 0.01890784, Accuracy: 90.80%\n",
      "Iteration:  1042, Loss: 0.01889207, Accuracy: 90.80%\n",
      "Iteration:  1043, Loss: 0.01887724, Accuracy: 90.80%\n",
      "Iteration:  1044, Loss: 0.01886256, Accuracy: 90.80%\n",
      "Iteration:  1045, Loss: 0.01885039, Accuracy: 90.80%\n",
      "Iteration:  1046, Loss: 0.01883661, Accuracy: 90.80%\n",
      "Iteration:  1047, Loss: 0.01883058, Accuracy: 90.90%\n",
      "Iteration:  1048, Loss: 0.01881445, Accuracy: 90.90%\n",
      "Iteration:  1049, Loss: 0.01889059, Accuracy: 90.90%\n",
      "Iteration:  1050, Loss: 0.01877887, Accuracy: 90.90%\n",
      "Iteration:  1051, Loss: 0.01876153, Accuracy: 90.90%\n",
      "Iteration:  1052, Loss: 0.01874124, Accuracy: 90.90%\n",
      "Iteration:  1053, Loss: 0.01872370, Accuracy: 90.90%\n",
      "Iteration:  1054, Loss: 0.01870670, Accuracy: 90.90%\n",
      "Iteration:  1055, Loss: 0.01869052, Accuracy: 90.90%\n",
      "Iteration:  1056, Loss: 0.01867443, Accuracy: 90.90%\n",
      "Iteration:  1057, Loss: 0.01866108, Accuracy: 90.90%\n",
      "Iteration:  1058, Loss: 0.01864681, Accuracy: 90.80%\n",
      "Iteration:  1059, Loss: 0.01863329, Accuracy: 90.80%\n",
      "Iteration:  1060, Loss: 0.01862001, Accuracy: 90.80%\n",
      "Iteration:  1061, Loss: 0.01860611, Accuracy: 90.80%\n",
      "Iteration:  1062, Loss: 0.01859282, Accuracy: 90.80%\n",
      "Iteration:  1063, Loss: 0.01857911, Accuracy: 90.80%\n",
      "Iteration:  1064, Loss: 0.01856591, Accuracy: 90.80%\n",
      "Iteration:  1065, Loss: 0.01855175, Accuracy: 90.80%\n",
      "Iteration:  1066, Loss: 0.01853805, Accuracy: 90.80%\n",
      "Iteration:  1067, Loss: 0.01852471, Accuracy: 90.80%\n",
      "Iteration:  1068, Loss: 0.01851169, Accuracy: 90.80%\n",
      "Iteration:  1069, Loss: 0.01849781, Accuracy: 90.80%\n",
      "Iteration:  1070, Loss: 0.01848457, Accuracy: 90.80%\n",
      "Iteration:  1071, Loss: 0.01847146, Accuracy: 90.80%\n",
      "Iteration:  1072, Loss: 0.01845861, Accuracy: 90.90%\n",
      "Iteration:  1073, Loss: 0.01844503, Accuracy: 90.90%\n",
      "Iteration:  1074, Loss: 0.01843214, Accuracy: 90.90%\n",
      "Iteration:  1075, Loss: 0.01841901, Accuracy: 90.90%\n",
      "Iteration:  1076, Loss: 0.01840625, Accuracy: 90.90%\n",
      "Iteration:  1077, Loss: 0.01839258, Accuracy: 90.90%\n",
      "Iteration:  1078, Loss: 0.01837954, Accuracy: 90.90%\n",
      "Iteration:  1079, Loss: 0.01836652, Accuracy: 90.90%\n",
      "Iteration:  1080, Loss: 0.01835358, Accuracy: 90.90%\n",
      "Iteration:  1081, Loss: 0.01834081, Accuracy: 90.90%\n",
      "Iteration:  1082, Loss: 0.01832816, Accuracy: 90.90%\n",
      "Iteration:  1083, Loss: 0.01831461, Accuracy: 90.90%\n",
      "Iteration:  1084, Loss: 0.01830176, Accuracy: 90.90%\n",
      "Iteration:  1085, Loss: 0.01828907, Accuracy: 90.90%\n",
      "Iteration:  1086, Loss: 0.01827637, Accuracy: 90.90%\n",
      "Iteration:  1087, Loss: 0.01826288, Accuracy: 90.90%\n",
      "Iteration:  1088, Loss: 0.01825008, Accuracy: 90.90%\n",
      "Iteration:  1089, Loss: 0.01823738, Accuracy: 90.90%\n",
      "Iteration:  1090, Loss: 0.01822464, Accuracy: 90.90%\n",
      "Iteration:  1091, Loss: 0.01821125, Accuracy: 90.90%\n",
      "Iteration:  1092, Loss: 0.01819864, Accuracy: 90.90%\n",
      "Iteration:  1093, Loss: 0.01818594, Accuracy: 90.90%\n",
      "Iteration:  1094, Loss: 0.01817330, Accuracy: 90.90%\n",
      "Iteration:  1095, Loss: 0.01816002, Accuracy: 90.90%\n",
      "Iteration:  1096, Loss: 0.01814746, Accuracy: 90.90%\n",
      "Iteration:  1097, Loss: 0.01813452, Accuracy: 90.90%\n",
      "Iteration:  1098, Loss: 0.01812139, Accuracy: 90.90%\n",
      "Iteration:  1099, Loss: 0.01810652, Accuracy: 90.90%\n",
      "Iteration:  1100, Loss: 0.01809378, Accuracy: 90.90%\n",
      "Iteration:  1101, Loss: 0.01808083, Accuracy: 90.90%\n",
      "Iteration:  1102, Loss: 0.01806826, Accuracy: 90.90%\n",
      "Iteration:  1103, Loss: 0.01805513, Accuracy: 90.90%\n",
      "Iteration:  1104, Loss: 0.01804273, Accuracy: 90.90%\n",
      "Iteration:  1105, Loss: 0.01802985, Accuracy: 90.90%\n",
      "Iteration:  1106, Loss: 0.01801742, Accuracy: 90.90%\n",
      "Iteration:  1107, Loss: 0.01800440, Accuracy: 90.90%\n",
      "Iteration:  1108, Loss: 0.01799200, Accuracy: 90.90%\n",
      "Iteration:  1109, Loss: 0.01797904, Accuracy: 90.90%\n",
      "Iteration:  1110, Loss: 0.01796656, Accuracy: 90.90%\n",
      "Iteration:  1111, Loss: 0.01795362, Accuracy: 90.90%\n",
      "Iteration:  1112, Loss: 0.01794114, Accuracy: 90.90%\n",
      "Iteration:  1113, Loss: 0.01792828, Accuracy: 90.90%\n",
      "Iteration:  1114, Loss: 0.01791576, Accuracy: 90.90%\n",
      "Iteration:  1115, Loss: 0.01790304, Accuracy: 90.90%\n",
      "Iteration:  1116, Loss: 0.01789032, Accuracy: 90.90%\n",
      "Iteration:  1117, Loss: 0.01787754, Accuracy: 90.90%\n",
      "Iteration:  1118, Loss: 0.01786290, Accuracy: 90.90%\n",
      "Iteration:  1119, Loss: 0.01784844, Accuracy: 90.90%\n",
      "Iteration:  1120, Loss: 0.01783780, Accuracy: 90.90%\n",
      "Iteration:  1121, Loss: 0.01782625, Accuracy: 90.90%\n",
      "Iteration:  1122, Loss: 0.01780798, Accuracy: 90.90%\n",
      "Iteration:  1123, Loss: 0.01779401, Accuracy: 90.90%\n",
      "Iteration:  1124, Loss: 0.01778151, Accuracy: 90.90%\n",
      "Iteration:  1125, Loss: 0.01776939, Accuracy: 90.90%\n",
      "Iteration:  1126, Loss: 0.01775683, Accuracy: 90.90%\n",
      "Iteration:  1127, Loss: 0.01774458, Accuracy: 90.80%\n",
      "Iteration:  1128, Loss: 0.01773148, Accuracy: 90.80%\n",
      "Iteration:  1129, Loss: 0.01771920, Accuracy: 90.80%\n",
      "Iteration:  1130, Loss: 0.01770606, Accuracy: 90.80%\n",
      "Iteration:  1131, Loss: 0.01769383, Accuracy: 90.80%\n",
      "Iteration:  1132, Loss: 0.01768100, Accuracy: 90.80%\n",
      "Iteration:  1133, Loss: 0.01766910, Accuracy: 90.80%\n",
      "Iteration:  1134, Loss: 0.01765706, Accuracy: 90.80%\n",
      "Iteration:  1135, Loss: 0.01764522, Accuracy: 90.80%\n",
      "Iteration:  1136, Loss: 0.01763249, Accuracy: 90.80%\n",
      "Iteration:  1137, Loss: 0.01762053, Accuracy: 90.80%\n",
      "Iteration:  1138, Loss: 0.01760878, Accuracy: 90.80%\n",
      "Iteration:  1139, Loss: 0.01759710, Accuracy: 90.80%\n",
      "Iteration:  1140, Loss: 0.01758445, Accuracy: 90.80%\n",
      "Iteration:  1141, Loss: 0.01757251, Accuracy: 90.80%\n",
      "Iteration:  1142, Loss: 0.01756067, Accuracy: 90.80%\n",
      "Iteration:  1143, Loss: 0.01754904, Accuracy: 90.80%\n",
      "Iteration:  1144, Loss: 0.01753658, Accuracy: 90.80%\n",
      "Iteration:  1145, Loss: 0.01752486, Accuracy: 90.80%\n",
      "Iteration:  1146, Loss: 0.01751288, Accuracy: 90.80%\n",
      "Iteration:  1147, Loss: 0.01750128, Accuracy: 90.80%\n",
      "Iteration:  1148, Loss: 0.01748850, Accuracy: 90.80%\n",
      "Iteration:  1149, Loss: 0.01747286, Accuracy: 90.90%\n",
      "Iteration:  1150, Loss: 0.01748298, Accuracy: 90.90%\n",
      "Iteration:  1151, Loss: 0.01751334, Accuracy: 90.90%\n",
      "Iteration:  1152, Loss: 0.01745261, Accuracy: 90.80%\n",
      "Iteration:  1153, Loss: 0.01742322, Accuracy: 90.80%\n",
      "Iteration:  1154, Loss: 0.01740953, Accuracy: 90.80%\n",
      "Iteration:  1155, Loss: 0.01739793, Accuracy: 90.80%\n",
      "Iteration:  1156, Loss: 0.01738566, Accuracy: 90.80%\n",
      "Iteration:  1157, Loss: 0.01737399, Accuracy: 90.80%\n",
      "Iteration:  1158, Loss: 0.01736201, Accuracy: 90.80%\n",
      "Iteration:  1159, Loss: 0.01735057, Accuracy: 90.80%\n",
      "Iteration:  1160, Loss: 0.01733830, Accuracy: 90.80%\n",
      "Iteration:  1161, Loss: 0.01732656, Accuracy: 90.80%\n",
      "Iteration:  1162, Loss: 0.01731482, Accuracy: 90.80%\n",
      "Iteration:  1163, Loss: 0.01730305, Accuracy: 90.80%\n",
      "Iteration:  1164, Loss: 0.01729134, Accuracy: 90.80%\n",
      "Iteration:  1165, Loss: 0.01727985, Accuracy: 90.80%\n",
      "Iteration:  1166, Loss: 0.01726806, Accuracy: 90.80%\n",
      "Iteration:  1167, Loss: 0.01725674, Accuracy: 90.80%\n",
      "Iteration:  1168, Loss: 0.01724464, Accuracy: 90.80%\n",
      "Iteration:  1169, Loss: 0.01723293, Accuracy: 90.80%\n",
      "Iteration:  1170, Loss: 0.01722080, Accuracy: 90.80%\n",
      "Iteration:  1171, Loss: 0.01720946, Accuracy: 90.80%\n",
      "Iteration:  1172, Loss: 0.01719822, Accuracy: 90.80%\n",
      "Iteration:  1173, Loss: 0.01718658, Accuracy: 90.80%\n",
      "Iteration:  1174, Loss: 0.01717535, Accuracy: 90.80%\n",
      "Iteration:  1175, Loss: 0.01716418, Accuracy: 90.80%\n",
      "Iteration:  1176, Loss: 0.01715207, Accuracy: 90.80%\n",
      "Iteration:  1177, Loss: 0.01714096, Accuracy: 90.80%\n",
      "Iteration:  1178, Loss: 0.01712944, Accuracy: 90.80%\n",
      "Iteration:  1179, Loss: 0.01711783, Accuracy: 90.80%\n",
      "Iteration:  1180, Loss: 0.01710642, Accuracy: 90.80%\n",
      "Iteration:  1181, Loss: 0.01709528, Accuracy: 90.80%\n",
      "Iteration:  1182, Loss: 0.01708370, Accuracy: 90.80%\n",
      "Iteration:  1183, Loss: 0.01707211, Accuracy: 90.80%\n",
      "Iteration:  1184, Loss: 0.01706091, Accuracy: 90.80%\n",
      "Iteration:  1185, Loss: 0.01704955, Accuracy: 90.80%\n",
      "Iteration:  1186, Loss: 0.01703800, Accuracy: 90.80%\n",
      "Iteration:  1187, Loss: 0.01702628, Accuracy: 90.80%\n",
      "Iteration:  1188, Loss: 0.01701480, Accuracy: 90.80%\n",
      "Iteration:  1189, Loss: 0.01700274, Accuracy: 90.80%\n",
      "Iteration:  1190, Loss: 0.01697143, Accuracy: 90.80%\n",
      "Iteration:  1191, Loss: 0.01698822, Accuracy: 90.80%\n",
      "Iteration:  1192, Loss: 0.01694184, Accuracy: 90.80%\n",
      "Iteration:  1193, Loss: 0.01692943, Accuracy: 90.80%\n",
      "Iteration:  1194, Loss: 0.01691793, Accuracy: 90.80%\n",
      "Iteration:  1195, Loss: 0.01690665, Accuracy: 90.80%\n",
      "Iteration:  1196, Loss: 0.01689536, Accuracy: 90.80%\n",
      "Iteration:  1197, Loss: 0.01688430, Accuracy: 90.80%\n",
      "Iteration:  1198, Loss: 0.01687275, Accuracy: 90.80%\n",
      "Iteration:  1199, Loss: 0.01686157, Accuracy: 90.80%\n",
      "Iteration:  1200, Loss: 0.01685050, Accuracy: 90.80%\n",
      "Iteration:  1201, Loss: 0.01683933, Accuracy: 90.80%\n",
      "Iteration:  1202, Loss: 0.01682845, Accuracy: 90.80%\n",
      "Iteration:  1203, Loss: 0.01681685, Accuracy: 90.80%\n",
      "Iteration:  1204, Loss: 0.01680581, Accuracy: 90.80%\n",
      "Iteration:  1205, Loss: 0.01679450, Accuracy: 90.80%\n",
      "Iteration:  1206, Loss: 0.01678346, Accuracy: 90.80%\n",
      "Iteration:  1207, Loss: 0.01677224, Accuracy: 90.80%\n",
      "Iteration:  1208, Loss: 0.01676123, Accuracy: 90.80%\n",
      "Iteration:  1209, Loss: 0.01674916, Accuracy: 90.80%\n",
      "Iteration:  1210, Loss: 0.01673637, Accuracy: 90.80%\n",
      "Iteration:  1211, Loss: 0.01672206, Accuracy: 90.80%\n",
      "Iteration:  1212, Loss: 0.01670982, Accuracy: 90.80%\n",
      "Iteration:  1213, Loss: 0.01669731, Accuracy: 90.80%\n",
      "Iteration:  1214, Loss: 0.01668575, Accuracy: 90.80%\n",
      "Iteration:  1215, Loss: 0.01667456, Accuracy: 90.80%\n",
      "Iteration:  1216, Loss: 0.01666368, Accuracy: 90.80%\n",
      "Iteration:  1217, Loss: 0.01665243, Accuracy: 90.80%\n",
      "Iteration:  1218, Loss: 0.01664162, Accuracy: 90.80%\n",
      "Iteration:  1219, Loss: 0.01663043, Accuracy: 90.80%\n",
      "Iteration:  1220, Loss: 0.01661961, Accuracy: 90.80%\n",
      "Iteration:  1221, Loss: 0.01660857, Accuracy: 90.80%\n",
      "Iteration:  1222, Loss: 0.01659778, Accuracy: 90.80%\n",
      "Iteration:  1223, Loss: 0.01658652, Accuracy: 90.80%\n",
      "Iteration:  1224, Loss: 0.01657568, Accuracy: 90.80%\n",
      "Iteration:  1225, Loss: 0.01656472, Accuracy: 90.80%\n",
      "Iteration:  1226, Loss: 0.01655392, Accuracy: 90.80%\n",
      "Iteration:  1227, Loss: 0.01654307, Accuracy: 90.80%\n",
      "Iteration:  1228, Loss: 0.01653226, Accuracy: 90.80%\n",
      "Iteration:  1229, Loss: 0.01652094, Accuracy: 90.80%\n",
      "Iteration:  1230, Loss: 0.01651022, Accuracy: 90.80%\n",
      "Iteration:  1231, Loss: 0.01649948, Accuracy: 90.80%\n",
      "Iteration:  1232, Loss: 0.01648893, Accuracy: 90.80%\n",
      "Iteration:  1233, Loss: 0.01647784, Accuracy: 90.80%\n",
      "Iteration:  1234, Loss: 0.01646701, Accuracy: 90.80%\n",
      "Iteration:  1235, Loss: 0.01645606, Accuracy: 90.80%\n",
      "Iteration:  1236, Loss: 0.01644534, Accuracy: 90.80%\n",
      "Iteration:  1237, Loss: 0.01643467, Accuracy: 90.80%\n",
      "Iteration:  1238, Loss: 0.01642421, Accuracy: 90.80%\n",
      "Iteration:  1239, Loss: 0.01641336, Accuracy: 90.80%\n",
      "Iteration:  1240, Loss: 0.01640283, Accuracy: 90.80%\n",
      "Iteration:  1241, Loss: 0.01639229, Accuracy: 90.80%\n",
      "Iteration:  1242, Loss: 0.01638187, Accuracy: 90.80%\n",
      "Iteration:  1243, Loss: 0.01637129, Accuracy: 90.80%\n",
      "Iteration:  1244, Loss: 0.01636092, Accuracy: 90.80%\n",
      "Iteration:  1245, Loss: 0.01635019, Accuracy: 90.80%\n",
      "Iteration:  1246, Loss: 0.01633972, Accuracy: 90.80%\n",
      "Iteration:  1247, Loss: 0.01632922, Accuracy: 90.80%\n",
      "Iteration:  1248, Loss: 0.01631881, Accuracy: 90.80%\n",
      "Iteration:  1249, Loss: 0.01630800, Accuracy: 90.80%\n",
      "Iteration:  1250, Loss: 0.01629742, Accuracy: 90.80%\n",
      "Iteration:  1251, Loss: 0.01628681, Accuracy: 90.80%\n",
      "Iteration:  1252, Loss: 0.01627616, Accuracy: 90.80%\n",
      "Iteration:  1253, Loss: 0.01626424, Accuracy: 90.80%\n",
      "Iteration:  1254, Loss: 0.01624847, Accuracy: 90.80%\n",
      "Iteration:  1255, Loss: 0.01623781, Accuracy: 90.80%\n",
      "Iteration:  1256, Loss: 0.01622721, Accuracy: 90.80%\n",
      "Iteration:  1257, Loss: 0.01621612, Accuracy: 90.80%\n",
      "Iteration:  1258, Loss: 0.01620511, Accuracy: 90.80%\n",
      "Iteration:  1259, Loss: 0.01619400, Accuracy: 90.80%\n",
      "Iteration:  1260, Loss: 0.01617976, Accuracy: 90.80%\n",
      "Iteration:  1261, Loss: 0.01618637, Accuracy: 90.80%\n",
      "Iteration:  1262, Loss: 0.01623439, Accuracy: 90.80%\n",
      "Iteration:  1263, Loss: 0.01614229, Accuracy: 90.80%\n",
      "Iteration:  1264, Loss: 0.01612780, Accuracy: 90.80%\n",
      "Iteration:  1265, Loss: 0.01611649, Accuracy: 90.80%\n",
      "Iteration:  1266, Loss: 0.01610570, Accuracy: 90.80%\n",
      "Iteration:  1267, Loss: 0.01609519, Accuracy: 90.80%\n",
      "Iteration:  1268, Loss: 0.01608499, Accuracy: 90.80%\n",
      "Iteration:  1269, Loss: 0.01607462, Accuracy: 90.80%\n",
      "Iteration:  1270, Loss: 0.01606446, Accuracy: 90.80%\n",
      "Iteration:  1271, Loss: 0.01605418, Accuracy: 90.80%\n",
      "Iteration:  1272, Loss: 0.01604403, Accuracy: 90.80%\n",
      "Iteration:  1273, Loss: 0.01603360, Accuracy: 90.80%\n",
      "Iteration:  1274, Loss: 0.01602333, Accuracy: 90.80%\n",
      "Iteration:  1275, Loss: 0.01601294, Accuracy: 90.80%\n",
      "Iteration:  1276, Loss: 0.01600239, Accuracy: 90.80%\n",
      "Iteration:  1277, Loss: 0.01599048, Accuracy: 90.80%\n",
      "Iteration:  1278, Loss: 0.01597732, Accuracy: 90.80%\n",
      "Iteration:  1279, Loss: 0.01596667, Accuracy: 90.80%\n",
      "Iteration:  1280, Loss: 0.01595649, Accuracy: 90.80%\n",
      "Iteration:  1281, Loss: 0.01594591, Accuracy: 90.80%\n",
      "Iteration:  1282, Loss: 0.01593555, Accuracy: 90.80%\n",
      "Iteration:  1283, Loss: 0.01592508, Accuracy: 90.80%\n",
      "Iteration:  1284, Loss: 0.01591504, Accuracy: 90.80%\n",
      "Iteration:  1285, Loss: 0.01590485, Accuracy: 90.80%\n",
      "Iteration:  1286, Loss: 0.01589486, Accuracy: 90.80%\n",
      "Iteration:  1287, Loss: 0.01588487, Accuracy: 90.80%\n",
      "Iteration:  1288, Loss: 0.01587497, Accuracy: 90.80%\n",
      "Iteration:  1289, Loss: 0.01586485, Accuracy: 90.80%\n",
      "Iteration:  1290, Loss: 0.01585491, Accuracy: 90.80%\n",
      "Iteration:  1291, Loss: 0.01584494, Accuracy: 90.80%\n",
      "Iteration:  1292, Loss: 0.01583503, Accuracy: 90.80%\n",
      "Iteration:  1293, Loss: 0.01582484, Accuracy: 90.80%\n",
      "Iteration:  1294, Loss: 0.01581467, Accuracy: 90.80%\n",
      "Iteration:  1295, Loss: 0.01580403, Accuracy: 90.80%\n",
      "Iteration:  1296, Loss: 0.01579187, Accuracy: 90.80%\n",
      "Iteration:  1297, Loss: 0.01578144, Accuracy: 90.80%\n",
      "Iteration:  1298, Loss: 0.01577137, Accuracy: 90.80%\n",
      "Iteration:  1299, Loss: 0.01576136, Accuracy: 90.80%\n",
      "Iteration:  1300, Loss: 0.01575149, Accuracy: 90.80%\n",
      "Iteration:  1301, Loss: 0.01574145, Accuracy: 90.80%\n",
      "Iteration:  1302, Loss: 0.01573161, Accuracy: 90.80%\n",
      "Iteration:  1303, Loss: 0.01572174, Accuracy: 90.80%\n",
      "Iteration:  1304, Loss: 0.01571198, Accuracy: 90.80%\n",
      "Iteration:  1305, Loss: 0.01570205, Accuracy: 90.80%\n",
      "Iteration:  1306, Loss: 0.01569228, Accuracy: 90.80%\n",
      "Iteration:  1307, Loss: 0.01568247, Accuracy: 90.80%\n",
      "Iteration:  1308, Loss: 0.01567274, Accuracy: 90.80%\n",
      "Iteration:  1309, Loss: 0.01566283, Accuracy: 90.80%\n",
      "Iteration:  1310, Loss: 0.01565304, Accuracy: 90.80%\n",
      "Iteration:  1311, Loss: 0.01564321, Accuracy: 90.80%\n",
      "Iteration:  1312, Loss: 0.01563348, Accuracy: 90.80%\n",
      "Iteration:  1313, Loss: 0.01562362, Accuracy: 90.80%\n",
      "Iteration:  1314, Loss: 0.01561389, Accuracy: 90.80%\n",
      "Iteration:  1315, Loss: 0.01560409, Accuracy: 90.80%\n",
      "Iteration:  1316, Loss: 0.01559432, Accuracy: 90.80%\n",
      "Iteration:  1317, Loss: 0.01558438, Accuracy: 90.80%\n",
      "Iteration:  1318, Loss: 0.01557456, Accuracy: 90.80%\n",
      "Iteration:  1319, Loss: 0.01556460, Accuracy: 90.80%\n",
      "Iteration:  1320, Loss: 0.01555478, Accuracy: 90.80%\n",
      "Iteration:  1321, Loss: 0.01554497, Accuracy: 90.80%\n",
      "Iteration:  1322, Loss: 0.01553536, Accuracy: 90.80%\n",
      "Iteration:  1323, Loss: 0.01552574, Accuracy: 90.80%\n",
      "Iteration:  1324, Loss: 0.01551623, Accuracy: 90.80%\n",
      "Iteration:  1325, Loss: 0.01550661, Accuracy: 90.80%\n",
      "Iteration:  1326, Loss: 0.01549712, Accuracy: 90.80%\n",
      "Iteration:  1327, Loss: 0.01548758, Accuracy: 90.80%\n",
      "Iteration:  1328, Loss: 0.01547814, Accuracy: 90.80%\n",
      "Iteration:  1329, Loss: 0.01546860, Accuracy: 90.90%\n",
      "Iteration:  1330, Loss: 0.01545917, Accuracy: 90.90%\n",
      "Iteration:  1331, Loss: 0.01544969, Accuracy: 90.90%\n",
      "Iteration:  1332, Loss: 0.01544030, Accuracy: 90.90%\n",
      "Iteration:  1333, Loss: 0.01543082, Accuracy: 90.90%\n",
      "Iteration:  1334, Loss: 0.01542145, Accuracy: 90.90%\n",
      "Iteration:  1335, Loss: 0.01541202, Accuracy: 90.90%\n",
      "Iteration:  1336, Loss: 0.01540267, Accuracy: 90.90%\n",
      "Iteration:  1337, Loss: 0.01539323, Accuracy: 90.90%\n",
      "Iteration:  1338, Loss: 0.01538389, Accuracy: 90.90%\n",
      "Iteration:  1339, Loss: 0.01537448, Accuracy: 90.90%\n",
      "Iteration:  1340, Loss: 0.01536514, Accuracy: 90.90%\n",
      "Iteration:  1341, Loss: 0.01535570, Accuracy: 90.90%\n",
      "Iteration:  1342, Loss: 0.01534632, Accuracy: 90.90%\n",
      "Iteration:  1343, Loss: 0.01533685, Accuracy: 90.90%\n",
      "Iteration:  1344, Loss: 0.01532745, Accuracy: 90.90%\n",
      "Iteration:  1345, Loss: 0.01531798, Accuracy: 90.90%\n",
      "Iteration:  1346, Loss: 0.01530857, Accuracy: 90.90%\n",
      "Iteration:  1347, Loss: 0.01529901, Accuracy: 90.90%\n",
      "Iteration:  1348, Loss: 0.01528935, Accuracy: 90.90%\n",
      "Iteration:  1349, Loss: 0.01527940, Accuracy: 90.90%\n",
      "Iteration:  1350, Loss: 0.01526920, Accuracy: 90.90%\n",
      "Iteration:  1351, Loss: 0.01525741, Accuracy: 90.90%\n",
      "Iteration:  1352, Loss: 0.01524535, Accuracy: 90.90%\n",
      "Iteration:  1353, Loss: 0.01523497, Accuracy: 90.90%\n",
      "Iteration:  1354, Loss: 0.01522435, Accuracy: 90.90%\n",
      "Iteration:  1355, Loss: 0.01521472, Accuracy: 90.90%\n",
      "Iteration:  1356, Loss: 0.01520519, Accuracy: 90.90%\n",
      "Iteration:  1357, Loss: 0.01519568, Accuracy: 90.90%\n",
      "Iteration:  1358, Loss: 0.01518630, Accuracy: 90.90%\n",
      "Iteration:  1359, Loss: 0.01517689, Accuracy: 90.90%\n",
      "Iteration:  1360, Loss: 0.01516754, Accuracy: 90.90%\n",
      "Iteration:  1361, Loss: 0.01515818, Accuracy: 90.90%\n",
      "Iteration:  1362, Loss: 0.01514893, Accuracy: 90.90%\n",
      "Iteration:  1363, Loss: 0.01513963, Accuracy: 90.90%\n",
      "Iteration:  1364, Loss: 0.01513038, Accuracy: 90.90%\n",
      "Iteration:  1365, Loss: 0.01512094, Accuracy: 90.90%\n",
      "Iteration:  1366, Loss: 0.01511098, Accuracy: 90.90%\n",
      "Iteration:  1367, Loss: 0.01509977, Accuracy: 90.90%\n",
      "Iteration:  1368, Loss: 0.01508983, Accuracy: 90.90%\n",
      "Iteration:  1369, Loss: 0.01507854, Accuracy: 90.90%\n",
      "Iteration:  1370, Loss: 0.01507314, Accuracy: 90.90%\n",
      "Iteration:  1371, Loss: 0.01508565, Accuracy: 90.90%\n",
      "Iteration:  1372, Loss: 0.01505494, Accuracy: 90.90%\n",
      "Iteration:  1373, Loss: 0.01504529, Accuracy: 90.90%\n",
      "Iteration:  1374, Loss: 0.01503410, Accuracy: 90.90%\n",
      "Iteration:  1375, Loss: 0.01502407, Accuracy: 90.90%\n",
      "Iteration:  1376, Loss: 0.01501312, Accuracy: 90.90%\n",
      "Iteration:  1377, Loss: 0.01500365, Accuracy: 90.90%\n",
      "Iteration:  1378, Loss: 0.01499441, Accuracy: 90.90%\n",
      "Iteration:  1379, Loss: 0.01498523, Accuracy: 90.90%\n",
      "Iteration:  1380, Loss: 0.01497611, Accuracy: 90.90%\n",
      "Iteration:  1381, Loss: 0.01496678, Accuracy: 90.90%\n",
      "Iteration:  1382, Loss: 0.01495539, Accuracy: 90.90%\n",
      "Iteration:  1383, Loss: 0.01494618, Accuracy: 90.90%\n",
      "Iteration:  1384, Loss: 0.01493726, Accuracy: 90.90%\n",
      "Iteration:  1385, Loss: 0.01492921, Accuracy: 90.80%\n",
      "Iteration:  1386, Loss: 0.01491897, Accuracy: 90.90%\n",
      "Iteration:  1387, Loss: 0.01490919, Accuracy: 90.80%\n",
      "Iteration:  1388, Loss: 0.01489983, Accuracy: 90.90%\n",
      "Iteration:  1389, Loss: 0.01489054, Accuracy: 90.90%\n",
      "Iteration:  1390, Loss: 0.01488140, Accuracy: 90.80%\n",
      "Iteration:  1391, Loss: 0.01487228, Accuracy: 90.80%\n",
      "Iteration:  1392, Loss: 0.01486319, Accuracy: 90.80%\n",
      "Iteration:  1393, Loss: 0.01485409, Accuracy: 90.80%\n",
      "Iteration:  1394, Loss: 0.01484510, Accuracy: 90.80%\n",
      "Iteration:  1395, Loss: 0.01483617, Accuracy: 90.80%\n",
      "Iteration:  1396, Loss: 0.01482731, Accuracy: 90.80%\n",
      "Iteration:  1397, Loss: 0.01481846, Accuracy: 90.80%\n",
      "Iteration:  1398, Loss: 0.01480966, Accuracy: 90.80%\n",
      "Iteration:  1399, Loss: 0.01480085, Accuracy: 90.80%\n",
      "Iteration:  1400, Loss: 0.01479206, Accuracy: 90.80%\n",
      "Iteration:  1401, Loss: 0.01478326, Accuracy: 90.80%\n",
      "Iteration:  1402, Loss: 0.01477448, Accuracy: 90.80%\n",
      "Iteration:  1403, Loss: 0.01476566, Accuracy: 90.80%\n",
      "Iteration:  1404, Loss: 0.01475687, Accuracy: 90.80%\n",
      "Iteration:  1405, Loss: 0.01474803, Accuracy: 90.80%\n",
      "Iteration:  1406, Loss: 0.01473918, Accuracy: 90.80%\n",
      "Iteration:  1407, Loss: 0.01473029, Accuracy: 90.80%\n",
      "Iteration:  1408, Loss: 0.01472143, Accuracy: 90.80%\n",
      "Iteration:  1409, Loss: 0.01471257, Accuracy: 90.80%\n",
      "Iteration:  1410, Loss: 0.01470370, Accuracy: 90.80%\n",
      "Iteration:  1411, Loss: 0.01469467, Accuracy: 90.80%\n",
      "Iteration:  1412, Loss: 0.01468509, Accuracy: 90.80%\n",
      "Iteration:  1413, Loss: 0.01467038, Accuracy: 90.80%\n",
      "Iteration:  1414, Loss: 0.01465943, Accuracy: 90.80%\n",
      "Iteration:  1415, Loss: 0.01464862, Accuracy: 90.80%\n",
      "Iteration:  1416, Loss: 0.01463995, Accuracy: 90.80%\n",
      "Iteration:  1417, Loss: 0.01463130, Accuracy: 90.80%\n",
      "Iteration:  1418, Loss: 0.01462266, Accuracy: 90.80%\n",
      "Iteration:  1419, Loss: 0.01461399, Accuracy: 90.80%\n",
      "Iteration:  1420, Loss: 0.01460528, Accuracy: 90.80%\n",
      "Iteration:  1421, Loss: 0.01459655, Accuracy: 90.80%\n",
      "Iteration:  1422, Loss: 0.01458794, Accuracy: 90.80%\n",
      "Iteration:  1423, Loss: 0.01457937, Accuracy: 90.80%\n",
      "Iteration:  1424, Loss: 0.01457084, Accuracy: 90.80%\n",
      "Iteration:  1425, Loss: 0.01456231, Accuracy: 90.80%\n",
      "Iteration:  1426, Loss: 0.01455381, Accuracy: 90.80%\n",
      "Iteration:  1427, Loss: 0.01454530, Accuracy: 90.80%\n",
      "Iteration:  1428, Loss: 0.01453679, Accuracy: 90.80%\n",
      "Iteration:  1429, Loss: 0.01452822, Accuracy: 90.80%\n",
      "Iteration:  1430, Loss: 0.01451955, Accuracy: 90.80%\n",
      "Iteration:  1431, Loss: 0.01451073, Accuracy: 90.80%\n",
      "Iteration:  1432, Loss: 0.01450198, Accuracy: 90.80%\n",
      "Iteration:  1433, Loss: 0.01449331, Accuracy: 90.80%\n",
      "Iteration:  1434, Loss: 0.01448452, Accuracy: 90.80%\n",
      "Iteration:  1435, Loss: 0.01447542, Accuracy: 90.80%\n",
      "Iteration:  1436, Loss: 0.01446677, Accuracy: 90.80%\n",
      "Iteration:  1437, Loss: 0.01445826, Accuracy: 90.80%\n",
      "Iteration:  1438, Loss: 0.01444978, Accuracy: 90.80%\n",
      "Iteration:  1439, Loss: 0.01444125, Accuracy: 90.80%\n",
      "Iteration:  1440, Loss: 0.01443240, Accuracy: 90.80%\n",
      "Iteration:  1441, Loss: 0.01442125, Accuracy: 90.80%\n",
      "Iteration:  1442, Loss: 0.01440978, Accuracy: 90.80%\n",
      "Iteration:  1443, Loss: 0.01440086, Accuracy: 90.80%\n",
      "Iteration:  1444, Loss: 0.01439250, Accuracy: 90.80%\n",
      "Iteration:  1445, Loss: 0.01438411, Accuracy: 90.80%\n",
      "Iteration:  1446, Loss: 0.01437568, Accuracy: 90.80%\n",
      "Iteration:  1447, Loss: 0.01436719, Accuracy: 90.80%\n",
      "Iteration:  1448, Loss: 0.01435873, Accuracy: 90.80%\n",
      "Iteration:  1449, Loss: 0.01435035, Accuracy: 90.80%\n",
      "Iteration:  1450, Loss: 0.01434202, Accuracy: 90.80%\n",
      "Iteration:  1451, Loss: 0.01433370, Accuracy: 90.80%\n",
      "Iteration:  1452, Loss: 0.01432539, Accuracy: 90.80%\n",
      "Iteration:  1453, Loss: 0.01431707, Accuracy: 90.80%\n",
      "Iteration:  1454, Loss: 0.01430871, Accuracy: 90.80%\n",
      "Iteration:  1455, Loss: 0.01430023, Accuracy: 90.80%\n",
      "Iteration:  1456, Loss: 0.01429135, Accuracy: 90.80%\n",
      "Iteration:  1457, Loss: 0.01428082, Accuracy: 90.80%\n",
      "Iteration:  1458, Loss: 0.01427039, Accuracy: 90.90%\n",
      "Iteration:  1459, Loss: 0.01426109, Accuracy: 90.90%\n",
      "Iteration:  1460, Loss: 0.01425275, Accuracy: 90.90%\n",
      "Iteration:  1461, Loss: 0.01424447, Accuracy: 90.90%\n",
      "Iteration:  1462, Loss: 0.01423621, Accuracy: 90.90%\n",
      "Iteration:  1463, Loss: 0.01422797, Accuracy: 90.90%\n",
      "Iteration:  1464, Loss: 0.01421972, Accuracy: 90.90%\n",
      "Iteration:  1465, Loss: 0.01421143, Accuracy: 90.90%\n",
      "Iteration:  1466, Loss: 0.01420297, Accuracy: 90.90%\n",
      "Iteration:  1467, Loss: 0.01419413, Accuracy: 90.90%\n",
      "Iteration:  1468, Loss: 0.01418537, Accuracy: 90.90%\n",
      "Iteration:  1469, Loss: 0.01417699, Accuracy: 90.90%\n",
      "Iteration:  1470, Loss: 0.01416872, Accuracy: 90.90%\n",
      "Iteration:  1471, Loss: 0.01416049, Accuracy: 90.90%\n",
      "Iteration:  1472, Loss: 0.01415219, Accuracy: 91.00%\n",
      "Iteration:  1473, Loss: 0.01414315, Accuracy: 91.00%\n",
      "Iteration:  1474, Loss: 0.01412068, Accuracy: 91.00%\n",
      "Iteration:  1475, Loss: 0.01411217, Accuracy: 91.00%\n",
      "Iteration:  1476, Loss: 0.01410377, Accuracy: 91.00%\n",
      "Iteration:  1477, Loss: 0.01409554, Accuracy: 91.00%\n",
      "Iteration:  1478, Loss: 0.01408730, Accuracy: 91.00%\n",
      "Iteration:  1479, Loss: 0.01407900, Accuracy: 91.00%\n",
      "Iteration:  1480, Loss: 0.01407058, Accuracy: 91.00%\n",
      "Iteration:  1481, Loss: 0.01406198, Accuracy: 91.00%\n",
      "Iteration:  1482, Loss: 0.01405337, Accuracy: 91.00%\n",
      "Iteration:  1483, Loss: 0.01404496, Accuracy: 91.00%\n",
      "Iteration:  1484, Loss: 0.01403673, Accuracy: 91.00%\n",
      "Iteration:  1485, Loss: 0.01402856, Accuracy: 91.00%\n",
      "Iteration:  1486, Loss: 0.01402043, Accuracy: 91.00%\n",
      "Iteration:  1487, Loss: 0.01401231, Accuracy: 91.00%\n",
      "Iteration:  1488, Loss: 0.01400422, Accuracy: 91.00%\n",
      "Iteration:  1489, Loss: 0.01399616, Accuracy: 91.00%\n",
      "Iteration:  1490, Loss: 0.01398811, Accuracy: 91.00%\n",
      "Iteration:  1491, Loss: 0.01398007, Accuracy: 91.00%\n",
      "Iteration:  1492, Loss: 0.01397196, Accuracy: 91.00%\n",
      "Iteration:  1493, Loss: 0.01396356, Accuracy: 91.00%\n",
      "Iteration:  1494, Loss: 0.01395363, Accuracy: 91.00%\n",
      "Iteration:  1495, Loss: 0.01394457, Accuracy: 91.00%\n",
      "Iteration:  1496, Loss: 0.01393626, Accuracy: 91.00%\n",
      "Iteration:  1497, Loss: 0.01392823, Accuracy: 91.00%\n",
      "Iteration:  1498, Loss: 0.01392024, Accuracy: 91.00%\n",
      "Iteration:  1499, Loss: 0.01391229, Accuracy: 91.00%\n",
      "Iteration:  1500, Loss: 0.01390436, Accuracy: 91.00%\n",
      "Iteration:  1501, Loss: 0.01389644, Accuracy: 91.00%\n",
      "Iteration:  1502, Loss: 0.01388852, Accuracy: 91.00%\n",
      "Iteration:  1503, Loss: 0.01388059, Accuracy: 91.00%\n",
      "Iteration:  1504, Loss: 0.01387268, Accuracy: 91.00%\n",
      "Iteration:  1505, Loss: 0.01386478, Accuracy: 91.00%\n",
      "Iteration:  1506, Loss: 0.01385690, Accuracy: 91.00%\n",
      "Iteration:  1507, Loss: 0.01384904, Accuracy: 91.00%\n",
      "Iteration:  1508, Loss: 0.01384119, Accuracy: 91.00%\n",
      "Iteration:  1509, Loss: 0.01383333, Accuracy: 91.00%\n",
      "Iteration:  1510, Loss: 0.01382546, Accuracy: 91.00%\n",
      "Iteration:  1511, Loss: 0.01381754, Accuracy: 91.00%\n",
      "Iteration:  1512, Loss: 0.01380942, Accuracy: 91.00%\n",
      "Iteration:  1513, Loss: 0.01379993, Accuracy: 91.00%\n",
      "Iteration:  1514, Loss: 0.01379073, Accuracy: 91.00%\n",
      "Iteration:  1515, Loss: 0.01378386, Accuracy: 91.00%\n",
      "Iteration:  1516, Loss: 0.01377310, Accuracy: 91.00%\n",
      "Iteration:  1517, Loss: 0.01376134, Accuracy: 91.00%\n",
      "Iteration:  1518, Loss: 0.01374950, Accuracy: 91.00%\n",
      "Iteration:  1519, Loss: 0.01373803, Accuracy: 91.00%\n",
      "Iteration:  1520, Loss: 0.01372990, Accuracy: 91.00%\n",
      "Iteration:  1521, Loss: 0.01372185, Accuracy: 91.00%\n",
      "Iteration:  1522, Loss: 0.01371343, Accuracy: 90.90%\n",
      "Iteration:  1523, Loss: 0.01370127, Accuracy: 90.90%\n",
      "Iteration:  1524, Loss: 0.01369247, Accuracy: 90.90%\n",
      "Iteration:  1525, Loss: 0.01368619, Accuracy: 90.90%\n",
      "Iteration:  1526, Loss: 0.01367814, Accuracy: 90.90%\n",
      "Iteration:  1527, Loss: 0.01366960, Accuracy: 90.90%\n",
      "Iteration:  1528, Loss: 0.01366020, Accuracy: 90.90%\n",
      "Iteration:  1529, Loss: 0.01365236, Accuracy: 90.90%\n",
      "Iteration:  1530, Loss: 0.01364460, Accuracy: 90.90%\n",
      "Iteration:  1531, Loss: 0.01363686, Accuracy: 90.90%\n",
      "Iteration:  1532, Loss: 0.01362910, Accuracy: 91.00%\n",
      "Iteration:  1533, Loss: 0.01362123, Accuracy: 91.00%\n",
      "Iteration:  1534, Loss: 0.01361242, Accuracy: 91.00%\n",
      "Iteration:  1535, Loss: 0.01360393, Accuracy: 91.00%\n",
      "Iteration:  1536, Loss: 0.01360428, Accuracy: 91.00%\n",
      "Iteration:  1537, Loss: 0.01358900, Accuracy: 91.00%\n",
      "Iteration:  1538, Loss: 0.01357271, Accuracy: 91.00%\n",
      "Iteration:  1539, Loss: 0.01356335, Accuracy: 91.00%\n",
      "Iteration:  1540, Loss: 0.01355524, Accuracy: 91.00%\n",
      "Iteration:  1541, Loss: 0.01354724, Accuracy: 91.00%\n",
      "Iteration:  1542, Loss: 0.01353940, Accuracy: 91.00%\n",
      "Iteration:  1543, Loss: 0.01353182, Accuracy: 91.00%\n",
      "Iteration:  1544, Loss: 0.01352398, Accuracy: 91.00%\n",
      "Iteration:  1545, Loss: 0.01351702, Accuracy: 91.00%\n",
      "Iteration:  1546, Loss: 0.01350905, Accuracy: 91.00%\n",
      "Iteration:  1547, Loss: 0.01350405, Accuracy: 91.00%\n",
      "Iteration:  1548, Loss: 0.01349957, Accuracy: 91.00%\n",
      "Iteration:  1549, Loss: 0.01350368, Accuracy: 91.00%\n",
      "Iteration:  1550, Loss: 0.01349322, Accuracy: 91.00%\n",
      "Iteration:  1551, Loss: 0.01347401, Accuracy: 91.00%\n",
      "Iteration:  1552, Loss: 0.01346218, Accuracy: 91.00%\n",
      "Iteration:  1553, Loss: 0.01345415, Accuracy: 91.00%\n",
      "Iteration:  1554, Loss: 0.01344594, Accuracy: 91.00%\n",
      "Iteration:  1555, Loss: 0.01343703, Accuracy: 91.00%\n",
      "Iteration:  1556, Loss: 0.01342804, Accuracy: 91.00%\n",
      "Iteration:  1557, Loss: 0.01341913, Accuracy: 91.00%\n",
      "Iteration:  1558, Loss: 0.01341013, Accuracy: 91.00%\n",
      "Iteration:  1559, Loss: 0.01340228, Accuracy: 91.00%\n",
      "Iteration:  1560, Loss: 0.01339462, Accuracy: 91.00%\n",
      "Iteration:  1561, Loss: 0.01338704, Accuracy: 91.00%\n",
      "Iteration:  1562, Loss: 0.01337951, Accuracy: 91.00%\n",
      "Iteration:  1563, Loss: 0.01337202, Accuracy: 91.00%\n",
      "Iteration:  1564, Loss: 0.01336456, Accuracy: 91.00%\n",
      "Iteration:  1565, Loss: 0.01335711, Accuracy: 91.00%\n",
      "Iteration:  1566, Loss: 0.01334964, Accuracy: 91.00%\n",
      "Iteration:  1567, Loss: 0.01334213, Accuracy: 91.00%\n",
      "Iteration:  1568, Loss: 0.01333453, Accuracy: 91.00%\n",
      "Iteration:  1569, Loss: 0.01332681, Accuracy: 91.00%\n",
      "Iteration:  1570, Loss: 0.01331849, Accuracy: 91.00%\n",
      "Iteration:  1571, Loss: 0.01331038, Accuracy: 91.00%\n",
      "Iteration:  1572, Loss: 0.01330258, Accuracy: 91.00%\n",
      "Iteration:  1573, Loss: 0.01329501, Accuracy: 91.00%\n",
      "Iteration:  1574, Loss: 0.01328756, Accuracy: 91.00%\n",
      "Iteration:  1575, Loss: 0.01328017, Accuracy: 91.00%\n",
      "Iteration:  1576, Loss: 0.01327282, Accuracy: 91.00%\n",
      "Iteration:  1577, Loss: 0.01326549, Accuracy: 91.00%\n",
      "Iteration:  1578, Loss: 0.01325816, Accuracy: 91.00%\n",
      "Iteration:  1579, Loss: 0.01325071, Accuracy: 91.00%\n",
      "Iteration:  1580, Loss: 0.01324211, Accuracy: 91.00%\n",
      "Iteration:  1581, Loss: 0.01323583, Accuracy: 91.00%\n",
      "Iteration:  1582, Loss: 0.01322847, Accuracy: 91.00%\n",
      "Iteration:  1583, Loss: 0.01321852, Accuracy: 91.00%\n",
      "Iteration:  1584, Loss: 0.01321121, Accuracy: 91.00%\n",
      "Iteration:  1585, Loss: 0.01320386, Accuracy: 91.00%\n",
      "Iteration:  1586, Loss: 0.01319633, Accuracy: 91.00%\n",
      "Iteration:  1587, Loss: 0.01318860, Accuracy: 91.00%\n",
      "Iteration:  1588, Loss: 0.01318116, Accuracy: 91.00%\n",
      "Iteration:  1589, Loss: 0.01317385, Accuracy: 91.00%\n",
      "Iteration:  1590, Loss: 0.01316659, Accuracy: 91.00%\n",
      "Iteration:  1591, Loss: 0.01315935, Accuracy: 91.00%\n",
      "Iteration:  1592, Loss: 0.01315209, Accuracy: 91.00%\n",
      "Iteration:  1593, Loss: 0.01314477, Accuracy: 91.00%\n",
      "Iteration:  1594, Loss: 0.01313726, Accuracy: 91.00%\n",
      "Iteration:  1595, Loss: 0.01312951, Accuracy: 91.00%\n",
      "Iteration:  1596, Loss: 0.01312196, Accuracy: 91.00%\n",
      "Iteration:  1597, Loss: 0.01311456, Accuracy: 91.00%\n",
      "Iteration:  1598, Loss: 0.01310723, Accuracy: 91.00%\n",
      "Iteration:  1599, Loss: 0.01309994, Accuracy: 91.00%\n",
      "Iteration:  1600, Loss: 0.01309272, Accuracy: 91.00%\n",
      "Iteration:  1601, Loss: 0.01308552, Accuracy: 91.00%\n",
      "Iteration:  1602, Loss: 0.01307832, Accuracy: 91.00%\n",
      "Iteration:  1603, Loss: 0.01307103, Accuracy: 91.00%\n",
      "Iteration:  1604, Loss: 0.01306361, Accuracy: 91.00%\n",
      "Iteration:  1605, Loss: 0.01305631, Accuracy: 91.00%\n",
      "Iteration:  1606, Loss: 0.01304913, Accuracy: 91.00%\n",
      "Iteration:  1607, Loss: 0.01304202, Accuracy: 91.00%\n",
      "Iteration:  1608, Loss: 0.01303495, Accuracy: 91.00%\n",
      "Iteration:  1609, Loss: 0.01302791, Accuracy: 91.00%\n",
      "Iteration:  1610, Loss: 0.01302089, Accuracy: 91.00%\n",
      "Iteration:  1611, Loss: 0.01301388, Accuracy: 91.00%\n",
      "Iteration:  1612, Loss: 0.01300689, Accuracy: 91.00%\n",
      "Iteration:  1613, Loss: 0.01299991, Accuracy: 91.00%\n",
      "Iteration:  1614, Loss: 0.01299295, Accuracy: 91.00%\n",
      "Iteration:  1615, Loss: 0.01298600, Accuracy: 91.00%\n",
      "Iteration:  1616, Loss: 0.01297905, Accuracy: 91.00%\n",
      "Iteration:  1617, Loss: 0.01297212, Accuracy: 91.00%\n",
      "Iteration:  1618, Loss: 0.01296520, Accuracy: 91.00%\n",
      "Iteration:  1619, Loss: 0.01295828, Accuracy: 91.00%\n",
      "Iteration:  1620, Loss: 0.01295137, Accuracy: 91.00%\n",
      "Iteration:  1621, Loss: 0.01294446, Accuracy: 91.00%\n",
      "Iteration:  1622, Loss: 0.01293756, Accuracy: 91.00%\n",
      "Iteration:  1623, Loss: 0.01293066, Accuracy: 91.00%\n",
      "Iteration:  1624, Loss: 0.01292375, Accuracy: 91.00%\n",
      "Iteration:  1625, Loss: 0.01291685, Accuracy: 91.00%\n",
      "Iteration:  1626, Loss: 0.01290993, Accuracy: 91.00%\n",
      "Iteration:  1627, Loss: 0.01290301, Accuracy: 91.00%\n",
      "Iteration:  1628, Loss: 0.01289607, Accuracy: 91.00%\n",
      "Iteration:  1629, Loss: 0.01288913, Accuracy: 91.00%\n",
      "Iteration:  1630, Loss: 0.01288219, Accuracy: 91.00%\n",
      "Iteration:  1631, Loss: 0.01287525, Accuracy: 91.00%\n",
      "Iteration:  1632, Loss: 0.01286832, Accuracy: 91.00%\n",
      "Iteration:  1633, Loss: 0.01286142, Accuracy: 91.00%\n",
      "Iteration:  1634, Loss: 0.01285453, Accuracy: 91.00%\n",
      "Iteration:  1635, Loss: 0.01284765, Accuracy: 91.00%\n",
      "Iteration:  1636, Loss: 0.01284076, Accuracy: 91.00%\n",
      "Iteration:  1637, Loss: 0.01283386, Accuracy: 91.00%\n",
      "Iteration:  1638, Loss: 0.01282694, Accuracy: 91.00%\n",
      "Iteration:  1639, Loss: 0.01282003, Accuracy: 91.00%\n",
      "Iteration:  1640, Loss: 0.01281315, Accuracy: 91.00%\n",
      "Iteration:  1641, Loss: 0.01280631, Accuracy: 91.00%\n",
      "Iteration:  1642, Loss: 0.01279949, Accuracy: 91.00%\n",
      "Iteration:  1643, Loss: 0.01279269, Accuracy: 91.00%\n",
      "Iteration:  1644, Loss: 0.01278592, Accuracy: 91.00%\n",
      "Iteration:  1645, Loss: 0.01277915, Accuracy: 91.00%\n",
      "Iteration:  1646, Loss: 0.01277239, Accuracy: 91.00%\n",
      "Iteration:  1647, Loss: 0.01276564, Accuracy: 91.00%\n",
      "Iteration:  1648, Loss: 0.01275890, Accuracy: 91.00%\n",
      "Iteration:  1649, Loss: 0.01275216, Accuracy: 91.00%\n",
      "Iteration:  1650, Loss: 0.01274542, Accuracy: 91.00%\n",
      "Iteration:  1651, Loss: 0.01273869, Accuracy: 91.00%\n",
      "Iteration:  1652, Loss: 0.01273194, Accuracy: 91.00%\n",
      "Iteration:  1653, Loss: 0.01272516, Accuracy: 91.00%\n",
      "Iteration:  1654, Loss: 0.01271831, Accuracy: 91.00%\n",
      "Iteration:  1655, Loss: 0.01271137, Accuracy: 91.00%\n",
      "Iteration:  1656, Loss: 0.01270448, Accuracy: 91.00%\n",
      "Iteration:  1657, Loss: 0.01269767, Accuracy: 91.00%\n",
      "Iteration:  1658, Loss: 0.01269088, Accuracy: 91.00%\n",
      "Iteration:  1659, Loss: 0.01268413, Accuracy: 91.00%\n",
      "Iteration:  1660, Loss: 0.01267743, Accuracy: 91.00%\n",
      "Iteration:  1661, Loss: 0.01267074, Accuracy: 91.00%\n",
      "Iteration:  1662, Loss: 0.01266408, Accuracy: 91.00%\n",
      "Iteration:  1663, Loss: 0.01265743, Accuracy: 91.00%\n",
      "Iteration:  1664, Loss: 0.01265079, Accuracy: 91.00%\n",
      "Iteration:  1665, Loss: 0.01264416, Accuracy: 91.00%\n",
      "Iteration:  1666, Loss: 0.01263753, Accuracy: 91.00%\n",
      "Iteration:  1667, Loss: 0.01263091, Accuracy: 91.00%\n",
      "Iteration:  1668, Loss: 0.01262427, Accuracy: 91.00%\n",
      "Iteration:  1669, Loss: 0.01261763, Accuracy: 91.00%\n",
      "Iteration:  1670, Loss: 0.01261093, Accuracy: 91.00%\n",
      "Iteration:  1671, Loss: 0.01260402, Accuracy: 91.00%\n",
      "Iteration:  1672, Loss: 0.01259647, Accuracy: 91.10%\n",
      "Iteration:  1673, Loss: 0.01258954, Accuracy: 91.10%\n",
      "Iteration:  1674, Loss: 0.01258282, Accuracy: 91.10%\n",
      "Iteration:  1675, Loss: 0.01257615, Accuracy: 91.10%\n",
      "Iteration:  1676, Loss: 0.01256948, Accuracy: 91.10%\n",
      "Iteration:  1677, Loss: 0.01256273, Accuracy: 91.10%\n",
      "Iteration:  1678, Loss: 0.01255579, Accuracy: 91.10%\n",
      "Iteration:  1679, Loss: 0.01254856, Accuracy: 91.10%\n",
      "Iteration:  1680, Loss: 0.01254046, Accuracy: 91.10%\n",
      "Iteration:  1681, Loss: 0.01253187, Accuracy: 91.10%\n",
      "Iteration:  1682, Loss: 0.01252502, Accuracy: 91.10%\n",
      "Iteration:  1683, Loss: 0.01251831, Accuracy: 91.10%\n",
      "Iteration:  1684, Loss: 0.01251165, Accuracy: 91.10%\n",
      "Iteration:  1685, Loss: 0.01250504, Accuracy: 91.10%\n",
      "Iteration:  1686, Loss: 0.01249845, Accuracy: 91.10%\n",
      "Iteration:  1687, Loss: 0.01249188, Accuracy: 91.10%\n",
      "Iteration:  1688, Loss: 0.01248533, Accuracy: 91.10%\n",
      "Iteration:  1689, Loss: 0.01247879, Accuracy: 91.10%\n",
      "Iteration:  1690, Loss: 0.01247226, Accuracy: 91.10%\n",
      "Iteration:  1691, Loss: 0.01246574, Accuracy: 91.10%\n",
      "Iteration:  1692, Loss: 0.01245923, Accuracy: 91.10%\n",
      "Iteration:  1693, Loss: 0.01245272, Accuracy: 91.10%\n",
      "Iteration:  1694, Loss: 0.01244622, Accuracy: 91.10%\n",
      "Iteration:  1695, Loss: 0.01243972, Accuracy: 91.10%\n",
      "Iteration:  1696, Loss: 0.01243320, Accuracy: 91.10%\n",
      "Iteration:  1697, Loss: 0.01242661, Accuracy: 91.10%\n",
      "Iteration:  1698, Loss: 0.01241953, Accuracy: 91.10%\n",
      "Iteration:  1699, Loss: 0.01240867, Accuracy: 91.10%\n",
      "Iteration:  1700, Loss: 0.01240109, Accuracy: 91.10%\n",
      "Iteration:  1701, Loss: 0.01239001, Accuracy: 91.10%\n",
      "Iteration:  1702, Loss: 0.01238137, Accuracy: 91.10%\n",
      "Iteration:  1703, Loss: 0.01237498, Accuracy: 91.10%\n",
      "Iteration:  1704, Loss: 0.01237450, Accuracy: 91.10%\n",
      "Iteration:  1705, Loss: 0.01236073, Accuracy: 91.10%\n",
      "Iteration:  1706, Loss: 0.01235472, Accuracy: 91.10%\n",
      "Iteration:  1707, Loss: 0.01237054, Accuracy: 91.10%\n",
      "Iteration:  1708, Loss: 0.01265182, Accuracy: 91.00%\n",
      "Iteration:  1709, Loss: 0.01320296, Accuracy: 91.30%\n",
      "Iteration:  1710, Loss: 0.01314819, Accuracy: 91.30%\n",
      "Iteration:  1711, Loss: 0.01288003, Accuracy: 91.10%\n",
      "Iteration:  1712, Loss: 0.01283454, Accuracy: 91.10%\n",
      "Iteration:  1713, Loss: 0.01281446, Accuracy: 91.10%\n",
      "Iteration:  1714, Loss: 0.01277849, Accuracy: 91.10%\n",
      "Iteration:  1715, Loss: 0.01276704, Accuracy: 91.10%\n",
      "Iteration:  1716, Loss: 0.01257047, Accuracy: 91.10%\n",
      "Iteration:  1717, Loss: 0.01233970, Accuracy: 91.00%\n",
      "Iteration:  1718, Loss: 0.01231468, Accuracy: 91.00%\n",
      "Iteration:  1719, Loss: 0.01230389, Accuracy: 91.00%\n",
      "Iteration:  1720, Loss: 0.01229365, Accuracy: 91.00%\n",
      "Iteration:  1721, Loss: 0.01228489, Accuracy: 91.00%\n",
      "Iteration:  1722, Loss: 0.01227786, Accuracy: 91.00%\n",
      "Iteration:  1723, Loss: 0.01227048, Accuracy: 91.00%\n",
      "Iteration:  1724, Loss: 0.01226349, Accuracy: 91.00%\n",
      "Iteration:  1725, Loss: 0.01225653, Accuracy: 91.00%\n",
      "Iteration:  1726, Loss: 0.01224963, Accuracy: 91.00%\n",
      "Iteration:  1727, Loss: 0.01224268, Accuracy: 91.00%\n",
      "Iteration:  1728, Loss: 0.01223589, Accuracy: 91.00%\n",
      "Iteration:  1729, Loss: 0.01222907, Accuracy: 91.00%\n",
      "Iteration:  1730, Loss: 0.01222243, Accuracy: 91.00%\n",
      "Iteration:  1731, Loss: 0.01221579, Accuracy: 91.00%\n",
      "Iteration:  1732, Loss: 0.01220922, Accuracy: 91.00%\n",
      "Iteration:  1733, Loss: 0.01220252, Accuracy: 91.00%\n",
      "Iteration:  1734, Loss: 0.01219561, Accuracy: 91.00%\n",
      "Iteration:  1735, Loss: 0.01218803, Accuracy: 91.00%\n",
      "Iteration:  1736, Loss: 0.01218058, Accuracy: 91.00%\n",
      "Iteration:  1737, Loss: 0.01217121, Accuracy: 91.00%\n",
      "Iteration:  1738, Loss: 0.01216217, Accuracy: 91.00%\n",
      "Iteration:  1739, Loss: 0.01215510, Accuracy: 91.00%\n",
      "Iteration:  1740, Loss: 0.01214864, Accuracy: 91.00%\n",
      "Iteration:  1741, Loss: 0.01214220, Accuracy: 91.00%\n",
      "Iteration:  1742, Loss: 0.01213585, Accuracy: 91.00%\n",
      "Iteration:  1743, Loss: 0.01212948, Accuracy: 91.00%\n",
      "Iteration:  1744, Loss: 0.01212317, Accuracy: 91.00%\n",
      "Iteration:  1745, Loss: 0.01211683, Accuracy: 91.00%\n",
      "Iteration:  1746, Loss: 0.01211053, Accuracy: 91.00%\n",
      "Iteration:  1747, Loss: 0.01210418, Accuracy: 91.00%\n",
      "Iteration:  1748, Loss: 0.01209783, Accuracy: 91.00%\n",
      "Iteration:  1749, Loss: 0.01209129, Accuracy: 91.00%\n",
      "Iteration:  1750, Loss: 0.01208402, Accuracy: 91.00%\n",
      "Iteration:  1751, Loss: 0.01207654, Accuracy: 91.00%\n",
      "Iteration:  1752, Loss: 0.01206967, Accuracy: 91.00%\n",
      "Iteration:  1753, Loss: 0.01206305, Accuracy: 91.00%\n",
      "Iteration:  1754, Loss: 0.01205666, Accuracy: 91.00%\n",
      "Iteration:  1755, Loss: 0.01205029, Accuracy: 91.00%\n",
      "Iteration:  1756, Loss: 0.01204391, Accuracy: 91.00%\n",
      "Iteration:  1757, Loss: 0.01203732, Accuracy: 91.00%\n",
      "Iteration:  1758, Loss: 0.01203076, Accuracy: 91.00%\n",
      "Iteration:  1759, Loss: 0.01202441, Accuracy: 91.00%\n",
      "Iteration:  1760, Loss: 0.01201809, Accuracy: 91.00%\n",
      "Iteration:  1761, Loss: 0.01201172, Accuracy: 91.00%\n",
      "Iteration:  1762, Loss: 0.01200543, Accuracy: 91.00%\n",
      "Iteration:  1763, Loss: 0.01199914, Accuracy: 91.00%\n",
      "Iteration:  1764, Loss: 0.01199285, Accuracy: 91.00%\n",
      "Iteration:  1765, Loss: 0.01198644, Accuracy: 91.00%\n",
      "Iteration:  1766, Loss: 0.01197989, Accuracy: 91.00%\n",
      "Iteration:  1767, Loss: 0.01197328, Accuracy: 91.00%\n",
      "Iteration:  1768, Loss: 0.01196691, Accuracy: 91.00%\n",
      "Iteration:  1769, Loss: 0.01196061, Accuracy: 91.00%\n",
      "Iteration:  1770, Loss: 0.01195435, Accuracy: 91.00%\n",
      "Iteration:  1771, Loss: 0.01194803, Accuracy: 91.00%\n",
      "Iteration:  1772, Loss: 0.01194163, Accuracy: 91.00%\n",
      "Iteration:  1773, Loss: 0.01193518, Accuracy: 91.00%\n",
      "Iteration:  1774, Loss: 0.01192891, Accuracy: 91.00%\n",
      "Iteration:  1775, Loss: 0.01192271, Accuracy: 91.00%\n",
      "Iteration:  1776, Loss: 0.01191661, Accuracy: 91.00%\n",
      "Iteration:  1777, Loss: 0.01191051, Accuracy: 91.00%\n",
      "Iteration:  1778, Loss: 0.01190447, Accuracy: 91.00%\n",
      "Iteration:  1779, Loss: 0.01189842, Accuracy: 91.00%\n",
      "Iteration:  1780, Loss: 0.01189242, Accuracy: 91.00%\n",
      "Iteration:  1781, Loss: 0.01188640, Accuracy: 91.00%\n",
      "Iteration:  1782, Loss: 0.01188042, Accuracy: 91.00%\n",
      "Iteration:  1783, Loss: 0.01187441, Accuracy: 91.00%\n",
      "Iteration:  1784, Loss: 0.01186845, Accuracy: 91.00%\n",
      "Iteration:  1785, Loss: 0.01186246, Accuracy: 91.00%\n",
      "Iteration:  1786, Loss: 0.01185651, Accuracy: 91.00%\n",
      "Iteration:  1787, Loss: 0.01185053, Accuracy: 91.00%\n",
      "Iteration:  1788, Loss: 0.01184458, Accuracy: 91.00%\n",
      "Iteration:  1789, Loss: 0.01183860, Accuracy: 91.00%\n",
      "Iteration:  1790, Loss: 0.01183265, Accuracy: 91.00%\n",
      "Iteration:  1791, Loss: 0.01182663, Accuracy: 91.00%\n",
      "Iteration:  1792, Loss: 0.01182057, Accuracy: 91.00%\n",
      "Iteration:  1793, Loss: 0.01181416, Accuracy: 91.00%\n",
      "Iteration:  1794, Loss: 0.01180625, Accuracy: 91.00%\n",
      "Iteration:  1795, Loss: 0.01179928, Accuracy: 91.00%\n",
      "Iteration:  1796, Loss: 0.01179309, Accuracy: 91.00%\n",
      "Iteration:  1797, Loss: 0.01178681, Accuracy: 91.00%\n",
      "Iteration:  1798, Loss: 0.01178066, Accuracy: 91.00%\n",
      "Iteration:  1799, Loss: 0.01177458, Accuracy: 91.00%\n",
      "Iteration:  1800, Loss: 0.01176851, Accuracy: 91.00%\n",
      "Iteration:  1801, Loss: 0.01176229, Accuracy: 91.00%\n",
      "Iteration:  1802, Loss: 0.01175601, Accuracy: 91.00%\n",
      "Iteration:  1803, Loss: 0.01174985, Accuracy: 91.00%\n",
      "Iteration:  1804, Loss: 0.01174382, Accuracy: 91.00%\n",
      "Iteration:  1805, Loss: 0.01173779, Accuracy: 91.00%\n",
      "Iteration:  1806, Loss: 0.01173180, Accuracy: 91.00%\n",
      "Iteration:  1807, Loss: 0.01172576, Accuracy: 91.00%\n",
      "Iteration:  1808, Loss: 0.01171972, Accuracy: 91.00%\n",
      "Iteration:  1809, Loss: 0.01171362, Accuracy: 91.00%\n",
      "Iteration:  1810, Loss: 0.01170755, Accuracy: 91.00%\n",
      "Iteration:  1811, Loss: 0.01170150, Accuracy: 91.00%\n",
      "Iteration:  1812, Loss: 0.01169555, Accuracy: 91.00%\n",
      "Iteration:  1813, Loss: 0.01168959, Accuracy: 91.00%\n",
      "Iteration:  1814, Loss: 0.01168370, Accuracy: 91.00%\n",
      "Iteration:  1815, Loss: 0.01167778, Accuracy: 91.00%\n",
      "Iteration:  1816, Loss: 0.01167191, Accuracy: 91.00%\n",
      "Iteration:  1817, Loss: 0.01166599, Accuracy: 91.00%\n",
      "Iteration:  1818, Loss: 0.01166010, Accuracy: 91.00%\n",
      "Iteration:  1819, Loss: 0.01165413, Accuracy: 91.00%\n",
      "Iteration:  1820, Loss: 0.01164809, Accuracy: 91.00%\n",
      "Iteration:  1821, Loss: 0.01164129, Accuracy: 91.00%\n",
      "Iteration:  1822, Loss: 0.01163271, Accuracy: 91.00%\n",
      "Iteration:  1823, Loss: 0.01162666, Accuracy: 91.00%\n",
      "Iteration:  1824, Loss: 0.01162067, Accuracy: 91.00%\n",
      "Iteration:  1825, Loss: 0.01161460, Accuracy: 91.00%\n",
      "Iteration:  1826, Loss: 0.01160847, Accuracy: 91.00%\n",
      "Iteration:  1827, Loss: 0.01160190, Accuracy: 91.00%\n",
      "Iteration:  1828, Loss: 0.01159376, Accuracy: 91.00%\n",
      "Iteration:  1829, Loss: 0.01158712, Accuracy: 91.00%\n",
      "Iteration:  1830, Loss: 0.01158120, Accuracy: 91.00%\n",
      "Iteration:  1831, Loss: 0.01157536, Accuracy: 91.00%\n",
      "Iteration:  1832, Loss: 0.01156994, Accuracy: 91.10%\n",
      "Iteration:  1833, Loss: 0.01156421, Accuracy: 91.10%\n",
      "Iteration:  1834, Loss: 0.01156007, Accuracy: 91.10%\n",
      "Iteration:  1835, Loss: 0.01155355, Accuracy: 91.10%\n",
      "Iteration:  1836, Loss: 0.01155185, Accuracy: 91.10%\n",
      "Iteration:  1837, Loss: 0.01154678, Accuracy: 91.20%\n",
      "Iteration:  1838, Loss: 0.01165296, Accuracy: 91.10%\n",
      "Iteration:  1839, Loss: 0.01213006, Accuracy: 91.10%\n",
      "Iteration:  1840, Loss: 0.01154603, Accuracy: 91.00%\n",
      "Iteration:  1841, Loss: 0.01153304, Accuracy: 91.00%\n",
      "Iteration:  1842, Loss: 0.01151523, Accuracy: 91.10%\n",
      "Iteration:  1843, Loss: 0.01151120, Accuracy: 91.10%\n",
      "Iteration:  1844, Loss: 0.01150292, Accuracy: 91.20%\n",
      "Iteration:  1845, Loss: 0.01149386, Accuracy: 91.20%\n",
      "Iteration:  1846, Loss: 0.01148658, Accuracy: 91.20%\n",
      "Iteration:  1847, Loss: 0.01147988, Accuracy: 91.20%\n",
      "Iteration:  1848, Loss: 0.01147332, Accuracy: 91.20%\n",
      "Iteration:  1849, Loss: 0.01146658, Accuracy: 91.20%\n",
      "Iteration:  1850, Loss: 0.01145981, Accuracy: 91.20%\n",
      "Iteration:  1851, Loss: 0.01145309, Accuracy: 91.20%\n",
      "Iteration:  1852, Loss: 0.01144658, Accuracy: 91.20%\n",
      "Iteration:  1853, Loss: 0.01144046, Accuracy: 91.20%\n",
      "Iteration:  1854, Loss: 0.01143460, Accuracy: 91.20%\n",
      "Iteration:  1855, Loss: 0.01142876, Accuracy: 91.20%\n",
      "Iteration:  1856, Loss: 0.01142299, Accuracy: 91.20%\n",
      "Iteration:  1857, Loss: 0.01141709, Accuracy: 91.20%\n",
      "Iteration:  1858, Loss: 0.01141073, Accuracy: 91.20%\n",
      "Iteration:  1859, Loss: 0.01140341, Accuracy: 91.20%\n",
      "Iteration:  1860, Loss: 0.01139728, Accuracy: 91.20%\n",
      "Iteration:  1861, Loss: 0.01139131, Accuracy: 91.20%\n",
      "Iteration:  1862, Loss: 0.01138551, Accuracy: 91.20%\n",
      "Iteration:  1863, Loss: 0.01137970, Accuracy: 91.20%\n",
      "Iteration:  1864, Loss: 0.01137400, Accuracy: 91.20%\n",
      "Iteration:  1865, Loss: 0.01136829, Accuracy: 91.20%\n",
      "Iteration:  1866, Loss: 0.01136267, Accuracy: 91.20%\n",
      "Iteration:  1867, Loss: 0.01135702, Accuracy: 91.20%\n",
      "Iteration:  1868, Loss: 0.01135145, Accuracy: 91.20%\n",
      "Iteration:  1869, Loss: 0.01134582, Accuracy: 91.20%\n",
      "Iteration:  1870, Loss: 0.01134027, Accuracy: 91.20%\n",
      "Iteration:  1871, Loss: 0.01133466, Accuracy: 91.20%\n",
      "Iteration:  1872, Loss: 0.01132911, Accuracy: 91.20%\n",
      "Iteration:  1873, Loss: 0.01132351, Accuracy: 91.30%\n",
      "Iteration:  1874, Loss: 0.01131798, Accuracy: 91.30%\n",
      "Iteration:  1875, Loss: 0.01131238, Accuracy: 91.30%\n",
      "Iteration:  1876, Loss: 0.01130683, Accuracy: 91.30%\n",
      "Iteration:  1877, Loss: 0.01130111, Accuracy: 91.30%\n",
      "Iteration:  1878, Loss: 0.01129492, Accuracy: 91.30%\n",
      "Iteration:  1879, Loss: 0.01128715, Accuracy: 91.30%\n",
      "Iteration:  1880, Loss: 0.01128113, Accuracy: 91.30%\n",
      "Iteration:  1881, Loss: 0.01127534, Accuracy: 91.30%\n",
      "Iteration:  1882, Loss: 0.01126972, Accuracy: 91.30%\n",
      "Iteration:  1883, Loss: 0.01126414, Accuracy: 91.30%\n",
      "Iteration:  1884, Loss: 0.01125864, Accuracy: 91.30%\n",
      "Iteration:  1885, Loss: 0.01125310, Accuracy: 91.30%\n",
      "Iteration:  1886, Loss: 0.01124760, Accuracy: 91.30%\n",
      "Iteration:  1887, Loss: 0.01124198, Accuracy: 91.30%\n",
      "Iteration:  1888, Loss: 0.01123622, Accuracy: 91.30%\n",
      "Iteration:  1889, Loss: 0.01122990, Accuracy: 91.30%\n",
      "Iteration:  1890, Loss: 0.01122412, Accuracy: 91.30%\n",
      "Iteration:  1891, Loss: 0.01121846, Accuracy: 91.30%\n",
      "Iteration:  1892, Loss: 0.01121294, Accuracy: 91.30%\n",
      "Iteration:  1893, Loss: 0.01120737, Accuracy: 91.30%\n",
      "Iteration:  1894, Loss: 0.01120187, Accuracy: 91.30%\n",
      "Iteration:  1895, Loss: 0.01119626, Accuracy: 91.30%\n",
      "Iteration:  1896, Loss: 0.01119061, Accuracy: 91.30%\n",
      "Iteration:  1897, Loss: 0.01118446, Accuracy: 91.30%\n",
      "Iteration:  1898, Loss: 0.01117780, Accuracy: 91.30%\n",
      "Iteration:  1899, Loss: 0.01117204, Accuracy: 91.30%\n",
      "Iteration:  1900, Loss: 0.01116644, Accuracy: 91.30%\n",
      "Iteration:  1901, Loss: 0.01116081, Accuracy: 91.30%\n",
      "Iteration:  1902, Loss: 0.01115530, Accuracy: 91.30%\n",
      "Iteration:  1903, Loss: 0.01114975, Accuracy: 91.30%\n",
      "Iteration:  1904, Loss: 0.01114431, Accuracy: 91.30%\n",
      "Iteration:  1905, Loss: 0.01113882, Accuracy: 91.30%\n",
      "Iteration:  1906, Loss: 0.01113341, Accuracy: 91.30%\n",
      "Iteration:  1907, Loss: 0.01112790, Accuracy: 91.30%\n",
      "Iteration:  1908, Loss: 0.01112235, Accuracy: 91.30%\n",
      "Iteration:  1909, Loss: 0.01111616, Accuracy: 91.30%\n",
      "Iteration:  1910, Loss: 0.01110543, Accuracy: 91.30%\n",
      "Iteration:  1911, Loss: 0.01109952, Accuracy: 91.30%\n",
      "Iteration:  1912, Loss: 0.01109357, Accuracy: 91.30%\n",
      "Iteration:  1913, Loss: 0.01108778, Accuracy: 91.30%\n",
      "Iteration:  1914, Loss: 0.01108218, Accuracy: 91.30%\n",
      "Iteration:  1915, Loss: 0.01107657, Accuracy: 91.30%\n",
      "Iteration:  1916, Loss: 0.01107045, Accuracy: 91.30%\n",
      "Iteration:  1917, Loss: 0.01106378, Accuracy: 91.30%\n",
      "Iteration:  1918, Loss: 0.01105821, Accuracy: 91.30%\n",
      "Iteration:  1919, Loss: 0.01105277, Accuracy: 91.30%\n",
      "Iteration:  1920, Loss: 0.01104747, Accuracy: 91.30%\n",
      "Iteration:  1921, Loss: 0.01104213, Accuracy: 91.30%\n",
      "Iteration:  1922, Loss: 0.01103687, Accuracy: 91.30%\n",
      "Iteration:  1923, Loss: 0.01103157, Accuracy: 91.30%\n",
      "Iteration:  1924, Loss: 0.01102634, Accuracy: 91.30%\n",
      "Iteration:  1925, Loss: 0.01102105, Accuracy: 91.30%\n",
      "Iteration:  1926, Loss: 0.01101583, Accuracy: 91.30%\n",
      "Iteration:  1927, Loss: 0.01101055, Accuracy: 91.30%\n",
      "Iteration:  1928, Loss: 0.01100535, Accuracy: 91.30%\n",
      "Iteration:  1929, Loss: 0.01100008, Accuracy: 91.30%\n",
      "Iteration:  1930, Loss: 0.01099488, Accuracy: 91.30%\n",
      "Iteration:  1931, Loss: 0.01098962, Accuracy: 91.30%\n",
      "Iteration:  1932, Loss: 0.01098443, Accuracy: 91.30%\n",
      "Iteration:  1933, Loss: 0.01097918, Accuracy: 91.30%\n",
      "Iteration:  1934, Loss: 0.01097399, Accuracy: 91.30%\n",
      "Iteration:  1935, Loss: 0.01096874, Accuracy: 91.30%\n",
      "Iteration:  1936, Loss: 0.01096356, Accuracy: 91.30%\n",
      "Iteration:  1937, Loss: 0.01095831, Accuracy: 91.30%\n",
      "Iteration:  1938, Loss: 0.01095314, Accuracy: 91.30%\n",
      "Iteration:  1939, Loss: 0.01094792, Accuracy: 91.30%\n",
      "Iteration:  1940, Loss: 0.01094278, Accuracy: 91.30%\n",
      "Iteration:  1941, Loss: 0.01093758, Accuracy: 91.30%\n",
      "Iteration:  1942, Loss: 0.01093246, Accuracy: 91.30%\n",
      "Iteration:  1943, Loss: 0.01092727, Accuracy: 91.30%\n",
      "Iteration:  1944, Loss: 0.01092216, Accuracy: 91.30%\n",
      "Iteration:  1945, Loss: 0.01091699, Accuracy: 91.30%\n",
      "Iteration:  1946, Loss: 0.01091188, Accuracy: 91.30%\n",
      "Iteration:  1947, Loss: 0.01090672, Accuracy: 91.30%\n",
      "Iteration:  1948, Loss: 0.01090162, Accuracy: 91.30%\n",
      "Iteration:  1949, Loss: 0.01089644, Accuracy: 91.30%\n",
      "Iteration:  1950, Loss: 0.01089133, Accuracy: 91.30%\n",
      "Iteration:  1951, Loss: 0.01088610, Accuracy: 91.30%\n",
      "Iteration:  1952, Loss: 0.01088079, Accuracy: 91.30%\n",
      "Iteration:  1953, Loss: 0.01087444, Accuracy: 91.30%\n",
      "Iteration:  1954, Loss: 0.01086106, Accuracy: 91.30%\n",
      "Iteration:  1955, Loss: 0.01086354, Accuracy: 91.30%\n",
      "Iteration:  1956, Loss: 0.01088344, Accuracy: 91.20%\n",
      "Iteration:  1957, Loss: 0.01106388, Accuracy: 91.30%\n",
      "Iteration:  1958, Loss: 0.01085848, Accuracy: 91.40%\n",
      "Iteration:  1959, Loss: 0.01083304, Accuracy: 91.30%\n",
      "Iteration:  1960, Loss: 0.01082654, Accuracy: 91.30%\n",
      "Iteration:  1961, Loss: 0.01082098, Accuracy: 91.30%\n",
      "Iteration:  1962, Loss: 0.01081506, Accuracy: 91.30%\n",
      "Iteration:  1963, Loss: 0.01080951, Accuracy: 91.30%\n",
      "Iteration:  1964, Loss: 0.01080360, Accuracy: 91.30%\n",
      "Iteration:  1965, Loss: 0.01079789, Accuracy: 91.30%\n",
      "Iteration:  1966, Loss: 0.01079211, Accuracy: 91.30%\n",
      "Iteration:  1967, Loss: 0.01078667, Accuracy: 91.30%\n",
      "Iteration:  1968, Loss: 0.01078140, Accuracy: 91.30%\n",
      "Iteration:  1969, Loss: 0.01077613, Accuracy: 91.30%\n",
      "Iteration:  1970, Loss: 0.01077095, Accuracy: 91.30%\n",
      "Iteration:  1971, Loss: 0.01076572, Accuracy: 91.30%\n",
      "Iteration:  1972, Loss: 0.01076058, Accuracy: 91.30%\n",
      "Iteration:  1973, Loss: 0.01075537, Accuracy: 91.30%\n",
      "Iteration:  1974, Loss: 0.01075026, Accuracy: 91.30%\n",
      "Iteration:  1975, Loss: 0.01074508, Accuracy: 91.30%\n",
      "Iteration:  1976, Loss: 0.01074000, Accuracy: 91.30%\n",
      "Iteration:  1977, Loss: 0.01073485, Accuracy: 91.30%\n",
      "Iteration:  1978, Loss: 0.01072979, Accuracy: 91.30%\n",
      "Iteration:  1979, Loss: 0.01072467, Accuracy: 91.30%\n",
      "Iteration:  1980, Loss: 0.01071964, Accuracy: 91.30%\n",
      "Iteration:  1981, Loss: 0.01071455, Accuracy: 91.30%\n",
      "Iteration:  1982, Loss: 0.01070954, Accuracy: 91.30%\n",
      "Iteration:  1983, Loss: 0.01070447, Accuracy: 91.30%\n",
      "Iteration:  1984, Loss: 0.01069948, Accuracy: 91.30%\n",
      "Iteration:  1985, Loss: 0.01069443, Accuracy: 91.30%\n",
      "Iteration:  1986, Loss: 0.01068947, Accuracy: 91.30%\n",
      "Iteration:  1987, Loss: 0.01068444, Accuracy: 91.30%\n",
      "Iteration:  1988, Loss: 0.01067949, Accuracy: 91.30%\n",
      "Iteration:  1989, Loss: 0.01067447, Accuracy: 91.30%\n",
      "Iteration:  1990, Loss: 0.01066952, Accuracy: 91.30%\n",
      "Iteration:  1991, Loss: 0.01066449, Accuracy: 91.30%\n",
      "Iteration:  1992, Loss: 0.01065945, Accuracy: 91.30%\n",
      "Iteration:  1993, Loss: 0.01065385, Accuracy: 91.30%\n",
      "Iteration:  1994, Loss: 0.01064758, Accuracy: 91.30%\n",
      "Iteration:  1995, Loss: 0.01064258, Accuracy: 91.30%\n",
      "Iteration:  1996, Loss: 0.01063768, Accuracy: 91.30%\n",
      "Iteration:  1997, Loss: 0.01063272, Accuracy: 91.30%\n",
      "Iteration:  1998, Loss: 0.01062785, Accuracy: 91.30%\n",
      "Iteration:  1999, Loss: 0.01062291, Accuracy: 91.30%\n"
     ]
    }
   ],
   "source": [
    "n_train = 6000 # number of examples in training data (max is 60,000)\n",
    "n_test  = 1000 # number of examples in training data (max is 10,000)\n",
    "n_nodes = 400 # number of hidden nodes\n",
    "n_iters = 2000 # number of iterations\n",
    "lr      = 0.8\n",
    "\n",
    "# create a string with the parameter values\n",
    "lr_str = str(lr).replace('.', 'p')\n",
    "name = 'success_rates_train{}_test{}_nodes{}_iters{}_lr_01'.format(n_train, n_test, n_nodes, n_iters)+lr_str\n",
    "\n",
    "\n",
    "(w1, w2, sr) = train(X_train[:n_train], Y_train[:n_train], \n",
    "               X_test[:n_test], Y_test[:n_test], \n",
    "               n_hidden_nodes=n_nodes, iterations=n_iters, lr=lr)\n",
    "# store the corrent success rates in the dictionary named success rates\n",
    "success_rates[name] = sr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "f64a9f16",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(90.0, 93.0)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 700x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# here you can plot several training curves alongside each other \n",
    "# uncomment, delete, edit, copy-paste lines correspondingly\n",
    "# edit the NAME of the results to be plotted AND edit the LABEL, such that the legend is correct\n",
    "plt.figure(figsize = (7,5))\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes30_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes30_iters1000_lr0.03')\n",
    "plt.plot(success_rates['success_rates_train6000_test1000_nodes800_iters2000_lr0p8'], label = 'success_rates_train6000_test1000_nodes100_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes300_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes300_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes784_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes784_iters1000_lr0.03')\n",
    "#plt.plot(success_rates['success_rates_train6000_test1000_nodes1000_iters1000_lr0p03'], label = 'success_rates_train6000_test1000_nodes1000_iters1000_lr0.03')\n",
    "plt.legend()\n",
    "plt.xlabel('iterations')\n",
    "plt.ylabel('success rate in %')\n",
    "plt.grid()\n",
    "plt.ylim([90,93]) # uncomment line to restrict the y-range for more detailed view on the late training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3a41eca",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0218b98d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "9baf391e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exercises\n",
    "\n",
    "# explore the hyperparameters\n",
    "# - number of nodes\n",
    "# - learning rate\n",
    "# - size of traing data set\n",
    "#\n",
    "# keep size of test data set constant, e.g. 1000\n",
    "# \n",
    "# a strong increase in number of iterations beyond 1000 is  \n",
    "# recommended only AFTER exploring other prameters to some extent\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20a484ef",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2979e292",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
