{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1dbf095",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gzip\n",
    "import struct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9feafe58",
   "metadata": {},
   "source": [
    "# Daten lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "90534178",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_images(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        _, n_images, cols, rows = struct.unpack('>IIII', f.read(16))\n",
    "        all_pixels = np.frombuffer(f.read(), dtype=np.uint8)\n",
    "        return all_pixels.reshape(n_images, cols*rows)\n",
    "\n",
    "def prepend_bias(X):\n",
    "    return np.insert(X, 0, 1, axis=1)\n",
    "\n",
    "def load_labels(filename):\n",
    "    with gzip.open(filename, 'rb') as f:\n",
    "        f.read(8)\n",
    "        all_labels = f.read()\n",
    "        return np.frombuffer(all_labels, dtype=np.uint8).reshape(-1,1)\n",
    "\n",
    "def encode_fives(Y):\n",
    "    return (Y==5).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a5d9966",
   "metadata": {},
   "source": [
    "Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "333b6df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "\n",
    "X_train = prepend_bias(load_images('../data/mnist/train-images-idx3-ubyte.gz'))\n",
    "X_test = prepend_bias(load_images('../data/mnist/t10k-images-idx3-ubyte.gz'))\n",
    "\n",
    "Y_train = encode_fives(load_labels('../data/mnist/train-labels-idx1-ubyte.gz'))\n",
    "Y_test = encode_fives(load_labels('../data/mnist/t10k-labels-idx1-ubyte.gz'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f47c22e8",
   "metadata": {},
   "source": [
    "# Explore the data\n",
    "\n",
    "X_train is training data for the input varibles X. Here, X consists of images of handwritten digits\n",
    "\n",
    "\n",
    "\n",
    "Explore X the data. For example, find out\n",
    "- how many images are there in X_train\n",
    "- how large is each image\n",
    "- plot an image as a vector, plt.plot(a). What does it look like? Explain!\n",
    "- reshape the 'images' into a 2D array (hint: its a square)\n",
    "- plot images\n",
    "\n",
    "Explore the Y data. For example, find out\n",
    "- what type of vales are stored in Y\n",
    "- what's the min and max value\n",
    "- what do the ones in Y correspond to in X\n",
    "- what's the shape of the array? Why?\n",
    "\n",
    "\n",
    "Possible tools\n",
    "- len(a) returns the legnth of an array or a list\n",
    "- a.shape (alternatively numpy.shape(a)) returns the shape of an array \n",
    "- a[0] returns the first element of a 1d array or the first line of an array (equivalent to a[0,:])\n",
    "- a.reshape(n,m) returns a reshaped array as a n-by-m array (if possible)\n",
    "- plt.imshow(a)\n",
    "- max, min\n",
    "- iteration: for i in range(start, stop): ... loops over i = start, start+1, ..., stop-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2379c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "c178853e",
   "metadata": {},
   "source": [
    "# Machine learning bib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "480db7cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def forward(X, w):\n",
    "    weighted_sum = np.matmul(X, w)\n",
    "    return sigmoid(weighted_sum)\n",
    "\n",
    "def classify(X, w):\n",
    "    return np.round(forward(X,w))\n",
    "\n",
    "def loss(X, Y, w):\n",
    "    y = forward(X,w)\n",
    "    return -np.average(Y*np.log(y) + (1-Y)*np.log(1-y))\n",
    "\n",
    "def gradient(X, Y, w):\n",
    "    return np.matmul(X.T, (forward(X,w)-Y))/X.shape[0]\n",
    "\n",
    "def train(X, Y, iterations, lr):\n",
    "    losses = np.zeros(iterations)\n",
    "    w = np.zeros((X.shape[1], 1))\n",
    "    for i in range(iterations):\n",
    "        losses[i] = loss(X,Y,w)\n",
    "        print('iteration {} => Loss: {}'.format(i, loss(X, Y, w)))\n",
    "        w -= gradient(X, Y, w) * lr\n",
    "    return [w, losses]\n",
    "\n",
    "def test(X, Y, x):\n",
    "    tot_examples = X.shape[0]\n",
    "    correct_results = np.sum(classify(X,w) == Y)\n",
    "    success_rate = correct_results * 100 / tot_examples\n",
    "    print('\\nSuccess: {}/{} ({})'.format(correct_results, tot_examples, success_rate))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5539edf",
   "metadata": {},
   "source": [
    "# Train and test a classifyer for 5 or not-5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8ecb029e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a subset for training and testing\n",
    "\n",
    "first_ind = 0\n",
    "last_ind = len(X_train) # exclusive\n",
    "\n",
    "X_train_select = X_train[first_ind:last_ind]\n",
    "Y_train_select = Y_train[first_ind:last_ind]\n",
    "X_test_select = X_test[first_ind:last_ind]\n",
    "Y_test_select = Y_test[first_ind:last_ind]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "62bf7861",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 => Loss: 0.6931471805599453\n",
      "iteration 1 => Loss: 0.8004253025949019\n",
      "iteration 2 => Loss: 0.6037018000801918\n",
      "iteration 3 => Loss: 0.41561114405207117\n",
      "iteration 4 => Loss: 0.264885394555525\n",
      "iteration 5 => Loss: 0.2197496658646801\n",
      "iteration 6 => Loss: 0.2116971966744956\n",
      "iteration 7 => Loss: 0.2047508247871004\n",
      "iteration 8 => Loss: 0.19886878484015674\n",
      "iteration 9 => Loss: 0.19372162858641284\n",
      "iteration 10 => Loss: 0.18918194048765818\n",
      "iteration 11 => Loss: 0.18512959704505969\n",
      "iteration 12 => Loss: 0.1814815604080142\n",
      "iteration 13 => Loss: 0.17817479685293547\n",
      "iteration 14 => Loss: 0.17515915471115795\n",
      "iteration 15 => Loss: 0.17239535061657862\n",
      "iteration 16 => Loss: 0.16985095072606282\n",
      "iteration 17 => Loss: 0.16749932830345124\n",
      "iteration 18 => Loss: 0.1653180724308886\n",
      "iteration 19 => Loss: 0.16328826613520675\n",
      "iteration 20 => Loss: 0.16139375633494252\n",
      "iteration 21 => Loss: 0.15962067801752663\n",
      "iteration 22 => Loss: 0.15795704638735247\n",
      "iteration 23 => Loss: 0.15639244690731702\n",
      "iteration 24 => Loss: 0.15491778067148085\n",
      "iteration 25 => Loss: 0.15352506006042058\n",
      "iteration 26 => Loss: 0.15220724105167438\n",
      "iteration 27 => Loss: 0.15095808554361248\n",
      "iteration 28 => Loss: 0.14977204728419308\n",
      "iteration 29 => Loss: 0.14864417688682038\n",
      "iteration 30 => Loss: 0.14757004225490838\n",
      "iteration 31 => Loss: 0.14654566156511656\n",
      "iteration 32 => Loss: 0.14556744653604964\n",
      "iteration 33 => Loss: 0.1446321541722572\n",
      "iteration 34 => Loss: 0.14373684552914012\n",
      "iteration 35 => Loss: 0.1428788503242973\n",
      "iteration 36 => Loss: 0.14205573644132233\n",
      "iteration 37 => Loss: 0.1412652835469923\n",
      "iteration 38 => Loss: 0.14050546018235405\n",
      "iteration 39 => Loss: 0.1397744038001722\n",
      "iteration 40 => Loss: 0.1390704033114991\n",
      "iteration 41 => Loss: 0.13839188377732237\n",
      "iteration 42 => Loss: 0.13773739294086704\n",
      "iteration 43 => Loss: 0.13710558934492648\n",
      "iteration 44 => Loss: 0.13649523181871495\n",
      "iteration 45 => Loss: 0.13590517015186068\n",
      "iteration 46 => Loss: 0.1353343368006255\n",
      "iteration 47 => Loss: 0.13478173949430844\n",
      "iteration 48 => Loss: 0.1342464546289012\n",
      "iteration 49 => Loss: 0.13372762135109859\n",
      "iteration 50 => Loss: 0.13322443624926733\n",
      "iteration 51 => Loss: 0.13273614857938065\n",
      "iteration 52 => Loss: 0.13226205596359317\n",
      "iteration 53 => Loss: 0.13180150050735345\n",
      "iteration 54 => Loss: 0.1313538652879622\n",
      "iteration 55 => Loss: 0.13091857117348435\n",
      "iteration 56 => Loss: 0.13049507393607074\n",
      "iteration 57 => Loss: 0.13008286162817387\n",
      "iteration 58 => Loss: 0.12968145219396188\n",
      "iteration 59 => Loss: 0.12929039129154007\n",
      "iteration 60 => Loss: 0.12890925030445236\n",
      "iteration 61 => Loss: 0.12853762452342626\n",
      "iteration 62 => Loss: 0.1281751314814919\n",
      "iteration 63 => Loss: 0.12782140942750006\n",
      "iteration 64 => Loss: 0.12747611592471905\n",
      "iteration 65 => Loss: 0.12713892656264417\n",
      "iteration 66 => Loss: 0.12680953377142654\n",
      "iteration 67 => Loss: 0.12648764572945337\n",
      "iteration 68 => Loss: 0.1261729853555999\n",
      "iteration 69 => Loss: 0.12586528937854832\n",
      "iteration 70 => Loss: 0.1255643074763439\n",
      "iteration 71 => Loss: 0.12526980148004305\n",
      "iteration 72 => Loss: 0.12498154463591929\n",
      "iteration 73 => Loss: 0.12469932092123305\n",
      "iteration 74 => Loss: 0.12442292440905582\n",
      "iteration 75 => Loss: 0.12415215867806853\n",
      "iteration 76 => Loss: 0.1238868362636398\n",
      "iteration 77 => Loss: 0.12362677814683307\n",
      "iteration 78 => Loss: 0.12337181327830136\n",
      "iteration 79 => Loss: 0.12312177813430428\n",
      "iteration 80 => Loss: 0.12287651630233092\n",
      "iteration 81 => Loss: 0.12263587809403648\n",
      "iteration 82 => Loss: 0.12239972018340117\n",
      "iteration 83 => Loss: 0.12216790526820255\n",
      "iteration 84 => Loss: 0.121940301753056\n",
      "iteration 85 => Loss: 0.12171678345242701\n",
      "iteration 86 => Loss: 0.12149722931215241\n",
      "iteration 87 => Loss: 0.12128152314813025\n",
      "iteration 88 => Loss: 0.12106955340094785\n",
      "iteration 89 => Loss: 0.12086121290531791\n",
      "iteration 90 => Loss: 0.12065639867328369\n",
      "iteration 91 => Loss: 0.12045501169023712\n",
      "iteration 92 => Loss: 0.12025695672286942\n",
      "iteration 93 => Loss: 0.12006214213824219\n",
      "iteration 94 => Loss: 0.11987047973323024\n",
      "iteration 95 => Loss: 0.11968188457364447\n",
      "iteration 96 => Loss: 0.11949627484239574\n",
      "iteration 97 => Loss: 0.11931357169610883\n",
      "iteration 98 => Loss: 0.11913369912963911\n",
      "iteration 99 => Loss: 0.11895658384798544\n"
     ]
    }
   ],
   "source": [
    "[w, losses] = train(X_train_select, Y_train_select, iterations=100, lr=1e-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "cb741593",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'losses' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[10], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# plot losses over iterations\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mlosses\u001b[49m)\n\u001b[0;32m      3\u001b[0m plt\u001b[38;5;241m.\u001b[39mgrid()\n",
      "\u001b[1;31mNameError\u001b[0m: name 'losses' is not defined"
     ]
    }
   ],
   "source": [
    "# plot losses over iterations\n",
    "plt.plot(losses)\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1ad002b4",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'w' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# test accuracy\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m test(X_test, Y_test, \u001b[43mw\u001b[49m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'w' is not defined"
     ]
    }
   ],
   "source": [
    "# test accuracy\n",
    "test(X_test, Y_test, w)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b7c3450",
   "metadata": {},
   "source": [
    "# Exercises\n",
    "\n",
    "Experiment with different hyperparameters (hyperparameters are parameters for the training - not model parameters)\n",
    "- size of the train and test data sets\n",
    "- number of iterations\n",
    "- lerning rate\n",
    "\n",
    "Find out which fives were classified corretly and which were not\n",
    "- find them (i.e. there indices in the dataset)\n",
    "- visualize them\n",
    "\n",
    "Maybe (I did not try!) you can find systematic errors compared to our human classification. For example, select a very small training data set in which all fives are \"similar\" to humans. Then check if during testing it finds mostly those \"similar\" to the trained ones. \n",
    "\n",
    "A useful strategy is called batching. \n",
    "- Divide your training data set in batches (e.g. 6 batches of 10000 each)\n",
    "- train on first batch and get weights\n",
    "- START training of second batch with the weights returned by the first batch\n",
    "- and so on\n",
    "Compare the success rate to training on the full set. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad39dafa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
