{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2dc40453",
   "metadata": {},
   "source": [
    "# Code from previous chapters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "829d6861",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "\n",
    "def predict(x, w, b):\n",
    "    \"\"\"\n",
    "    Computes the prediction from the input x, weight w and bias b as y_hat = w*x + b.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    X: array\n",
    "        the input (\"feature\") for the prediction, i.e. the nuber of reservations\n",
    "        can be a single number or an array of numbers\n",
    "    w: number \n",
    "        the \"weight\" for this feature in the linear model\n",
    "    b: number\n",
    "        the bias of the linear model\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    y_hat: array \n",
    "        the predictions, i.e. the predicted numbers of pizzas that  will be ordered\n",
    "        the dimensions of y are those of x\n",
    "    \"\"\"\n",
    "    y_hat = w*x + b\n",
    "    return y_hat\n",
    "\n",
    "\n",
    "def loss(y, y_hat):\n",
    "    \"\"\"\n",
    "    Computes the loss as the mean squared difference between predicted and true values. \n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    y: array \n",
    "        single number or numpy array\n",
    "        the true label values in the list of examples, i.e. the true numbers of reservations\n",
    "    y_hat: array\n",
    "        single number or numpy array\n",
    "        dimensions must mach those of y\n",
    "        the predicted values \n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    loss: number\n",
    "        the squared error, i.e. the squared difference between prediction and the prediction, i.e. the predicted number of pizzas that  will be ordered. \n",
    "    \"\"\"\n",
    "    loss = np.mean( (y_hat - y)**2 )\n",
    "    return loss\n",
    "\n",
    "\n",
    "\n",
    "def train_naive(x, y, learning_rate=0.001, max_iterations=10000):\n",
    "    \"\"\"\n",
    "    Trains a linear model. The weight-bias space is explored by \n",
    "        taking steps in the w-direction and the b-direction separately. \n",
    "    If no further improvement is obtained the training stops. \n",
    "    If no stop is reached within the maximum number of iterations\n",
    "        a warning is printed and the current vales are returned.\n",
    "    \n",
    "    parameters:\n",
    "    -----------\n",
    "    x: array \n",
    "        single number or numpy array\n",
    "        the feature values in the training examples\n",
    "    y: array\n",
    "        single number or numpy array\n",
    "        dimensions must mach those of x\n",
    "        the label values in the training examples\n",
    "    learning_rate: number\n",
    "        the learning rate\n",
    "    max_iterations: integer\n",
    "        the maximum number of iterations\n",
    "    \n",
    "    returns:\n",
    "    --------\n",
    "    [w, b, current_loss, i, ws, bs, losses]\n",
    "    w: number \n",
    "        final weight\n",
    "    b: number\n",
    "        final bias\n",
    "    current_loss: number\n",
    "        final loss\n",
    "    i: integer\n",
    "        number of iterations performed\n",
    "    ws: list\n",
    "        weights for all iterations\n",
    "    bs: list\n",
    "        biases for all iterations\n",
    "    losses: list\n",
    "        loss for all iterations\n",
    "    \"\"\"\n",
    "    w = 0 # initial weight\n",
    "    b = 0 # initial bias\n",
    "    \n",
    "    ws = []\n",
    "    bs = []\n",
    "    losses = []\n",
    "    \n",
    "    for i in range(max_iterations):\n",
    "        current_loss = loss(predict(x, w, b), y)\n",
    "        #print('Iteration %4d: , w=%.3f, b=%.3f, current_loss: %.3f' %(i, w, b, current_loss))\n",
    "        \n",
    "        ws.append(w)\n",
    "        bs.append(b)\n",
    "        losses.append(current_loss)\n",
    "        \n",
    "        if loss(predict(x, w+learning_rate, b), y) < current_loss:\n",
    "            w += learning_rate\n",
    "        elif loss(predict(x, w-learning_rate, b), y) < current_loss:\n",
    "            w -= learning_rate\n",
    "        elif loss(predict(x, w, b+learning_rate), y) < current_loss:\n",
    "            b += learning_rate        \n",
    "        elif loss(predict(x, w, b-learning_rate), y) < current_loss:\n",
    "            b -= learning_rate        \n",
    "        else:\n",
    "            return [w, b, current_loss, i, ws, bs, losses]\n",
    "    print('Could not converge in ', max_iterations, 'iterations')\n",
    "    return [w, b, current_loss, i, ws, bs, losses]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# ========================================================\n",
    "# ===== AUXILIURY FUNCTIONS FOR PLOTTING \n",
    "# ========================================================\n",
    "\n",
    "def plotpath(ws, bs, losses, X=None, Y=None, step=1, xlabel='weight', ylabel='bias', figsize=(12,4)):\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    gs = fig.add_gridspec(3, 3)\n",
    "    ax1 = fig.add_subplot(gs[:, 0])\n",
    "    ax2 = fig.add_subplot(gs[0, 1])\n",
    "    ax3 = fig.add_subplot(gs[1, 1])\n",
    "    ax4 = fig.add_subplot(gs[2, 1])\n",
    "    ax5 = fig.add_subplot(gs[:, 2])\n",
    "    \n",
    "    inds = np.arange(len(ws))\n",
    "    \n",
    "    # path in 2d parameter space\n",
    "    ax1.plot(ws[::step], bs[::step], '-')\n",
    "    ax1.set_xlabel(xlabel)\n",
    "    ax1.set_ylabel(ylabel)\n",
    "    ax1.annotate('iterations: {} \\nfinal weight = {:.3f} \\nfinal bias = {:.3f}'.format(len(ws), ws[-1], bs[-1]), (0.5, 0.5), xycoords='axes fraction', va='center')\n",
    "    ax1.grid()\n",
    "    \n",
    "    # weight\n",
    "    ax2.plot(inds[::step], ws[::step])\n",
    "    ax2.set_xlabel('iteration')\n",
    "    ax2.set_ylabel(xlabel)\n",
    "    ax2.grid()\n",
    "\n",
    "    # bias\n",
    "    ax3.plot(inds[::step], bs[::step])\n",
    "    ax3.set_xlabel('iteration')\n",
    "    ax3.set_ylabel(ylabel)\n",
    "    ax3.grid()\n",
    "\n",
    "    # loss\n",
    "    ax4.semilogy(inds[::step], losses[::step])\n",
    "    ax4.set_xlabel('iteration')\n",
    "    ax4.set_ylabel('loss')\n",
    "    ax4.annotate('final loss = %.4f'%losses[-1], (0.5, 0.5), xycoords='axes fraction', va='center')\n",
    "    ax4.grid()\n",
    "\n",
    "    # data and linear fit\n",
    "    if X is not None and Y is not None:\n",
    "        Y_hat = predict(X, ws[-1], bs[-1])\n",
    "        ax5.plot(X,Y,'bo')\n",
    "        ax5.plot(X, Y_hat)\n",
    "        ax5.set_xlabel('feature: number of reservations')\n",
    "        ax5.set_ylabel('label: number of pizzas')\n",
    "        ax5.set_xlim([0, 30]) \n",
    "        ax5.set_ylim([0, 50]) \n",
    "        \n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21538ae6",
   "metadata": {},
   "source": [
    "# Daten mit drei features, vorläufig\n",
    "Hier werden die Daten mit drei Features geladen und in eine Matrix gepackt. \n",
    "* Zeilenindex i: numeriert die Beispiele\n",
    "* Spaltenindex j: numeriert die features\n",
    "\n",
    "Diese Matrix ist nur vorläufig, da wir später noch eine Spalte Einsen hinzufügen werden (siehe unten). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "44ad3578",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "(unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape (2624449147.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Cell \u001b[1;32mIn[5], line 1\u001b[1;36m\u001b[0m\n\u001b[1;33m    x1, x2, x3, y = np.loadtxt('C:\\Users\\Lenn Messelken\\Documents\\Coden\\Seminar AI\\data\\pizza_3_vars.txt', skiprows=1, unpack=True)\u001b[0m\n\u001b[1;37m                                                                                                         ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m (unicode error) 'unicodeescape' codec can't decode bytes in position 2-3: truncated \\UXXXXXXXX escape\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3, y = np.loadtxt('C:\\Users\\Lenn Messelken\\Documents\\Coden\\Seminar AI\\data\\pizza_3_vars.txt', skiprows=1, unpack=True)\n",
    "X = np.column_stack((x1, x2, x3))\n",
    "m = len(X)\n",
    "print('number of examples: m =',m)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0398a20",
   "metadata": {},
   "source": [
    "## Einzelne Einträge einer Matrix ausgeben\n",
    "\n",
    "X[0,0] gibt den Eintrag in der ersten Zeile und ersten Spalte aus (beachte das zero-based-indexing).\n",
    "\n",
    "X[3,1] gibt den Eintrag in Zeile 4 (Zeilenindex 3) und Spalte 2 (Spaltenindex 1) aus. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40498dec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cc5af547",
   "metadata": {},
   "source": [
    "## Teile einer Matrix ausgeben, slicing\n",
    "\n",
    "Der Befehl X[:4,:] druckt \n",
    "- die Zeilen 0 bis 3 (beachte: das sind die ersten VIER Zeilen)\n",
    "- alle Spalten (der Doppelpunkt ohne Zahl davor und/oder dahinter bedeutet \"alles\")\n",
    "\n",
    "Beachte: dieser Befehl ist gleichbedeutent mit X[:4]. Weiter hinten stehende nicht explizit genannte Indices werden vollständig gesliced. \n",
    "\n",
    "Probiere beides aus:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d161d0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5a5875e6",
   "metadata": {},
   "source": [
    "Der Befehl x[2:4, 1:] gibt die Zeilen mit den Indices 2 und 3 aus (also Zeilen 3 und 4), und aus diesen jeweils die Spalten mit den Indices 1 aufwaerts, hier also die zweite und dritte Spalte. \n",
    "\n",
    "Probiere es aus: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb43baf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "caabc57e",
   "metadata": {},
   "source": [
    "### Aufgaben\n",
    "\n",
    "- Gib alle Daten (die gesamte Matrix X) aus, einmal ohne und einmal mit slicing\n",
    "- Gib die Daten aus Zeilen 3 und Spalten 1 und 2 aus. \n",
    "- Gib die Daten aus Zeilen 3 bis 5 und Spalten 1 bis 3 aus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2832cd8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f9d77dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "5fd0f841",
   "metadata": {},
   "source": [
    "# Daten mit drei features, endgültig\n",
    "Hier wird, wie im Skript besprochen, eine Spalte Einsen als erste Spalte hinzugefügt. \n",
    "\n",
    "Ausserdem wird aus dem Vektor y auch eine Matrix gemacht. Dies ist nötig, damit np.matmul damit umgehen kann. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bda01e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of examples: m = 30\n"
     ]
    }
   ],
   "source": [
    "x1, x2, x3, y = np.loadtxt('../data/pizza_3_vars.txt', skiprows=1, unpack=True)\n",
    "x = np.column_stack((np.ones(x1.size), x1, x2, x3))\n",
    "y = y.reshape(-1, 1) # cast array into a matrix\n",
    "\n",
    "m = len(x)\n",
    "print('number of examples: m =',m)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a0bd56c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "15c87b04",
   "metadata": {},
   "source": [
    "# Vorhersage in Matrixschreibweise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "b799e6f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==============================================\n",
    "# ===   WRITE YOUR OWN CODE \n",
    "# ==============================================\n",
    "#\n",
    "# no help: write everything yourself, including the docstring\n",
    "#\n",
    "# Level-1-help: copy/paste the docstring from moodle\n",
    "#\n",
    "# Level-2-help: look up the code from the teachers printout\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "399e18ad",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d6bf6bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "dfe28c12",
   "metadata": {},
   "source": [
    "# Apply your function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "388d470a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# select the colums 1 to 3 (1: ones, 2: reservations, 3: temperature)\n",
    "x2 = x[:,:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "85cf1b69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute the prediction for the following weights:\n",
    "# bias = optimal value from training with one feature\n",
    "# w for reservations = optimal value from training with one feature\n",
    "# w for temperature = Zero\n",
    "# pack these into a vector called w like this: w = np.array([..., ..., ...])\n",
    "# \n",
    "# then compute the prediction as y_hat_1 = ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "ea2e7767",
   "metadata": {},
   "outputs": [],
   "source": [
    "# une nonzero values of the weight for the temperature to compute predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fbab4a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the data alongside a prediction. \n",
    "# use the number of reservations on the x-axis and the prediction on the y axis\n",
    "# for nonzero values of the temperature weight you don't get a straight line. Explain. \n",
    "\n",
    "#plt.plot(x[:,1], y, 'ob')\n",
    "#plt.plot(x[:,1], y_hat_1, '+k')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "250717fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# play with the parameters b, w_reservations and w_temperature. \n",
    "# Try to improve the agreement between data and prediction\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0619a5be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cb53e0d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
